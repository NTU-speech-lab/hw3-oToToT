{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_a2USyd4giE"
   },
   "source": [
    "# **Homework 3 - Convolutional Neural Network**\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhzdomRTOKoJ"
   },
   "outputs": [],
   "source": [
    "# !gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip # 下載資料集\n",
    "# !unzip food-11.zip # 解壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sVrKci4PUFW"
   },
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "# from apex import amp\n",
    "import time\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0i9ZCPrOVN_"
   },
   "source": [
    "#Read image\n",
    "利用 OpenCV (cv2) 讀入照片並存放在 numpy array 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf7QPifJQNUK"
   },
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to 128 x ? or ? x 128\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = 128 / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = 128, 128\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ebVIY5HQQH7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "# 分別將 training set、validation set、testing set 用 readfile 函式讀進來\n",
    "workspace_dir = './food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gq5KVMM3OHY6"
   },
   "source": [
    "# Dataset\n",
    "在 PyTorch 中，我們可以利用 torch.utils.data 的 Dataset 及 DataLoader 來\"包裝\" data，使後續的 training 及 testing 更為方便。\n",
    "\n",
    "Dataset 需要 overload 兩個函數：\\_\\_len\\_\\_ 及 \\_\\_getitem\\_\\_\n",
    "\n",
    "\\_\\_len\\_\\_ 必須要回傳 dataset 的大小，而 \\_\\_getitem\\_\\_ 則定義了當程式利用 [ ] 取值時，dataset 應該要怎麼回傳資料。\n",
    "\n",
    "實際上我們並不會直接使用到這兩個函數，但是使用 DataLoader 在 enumerate Dataset 時會使用到，沒有實做的話會在程式運行階段出現 error。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKd2abixQghI"
   },
   "outputs": [],
   "source": [
    "# training 時做 data augmentation\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective()\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomRotation(40)\n",
    "    ]),\n",
    "    transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomOrder([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective()\n",
    "        ]),\n",
    "        transforms.RandomAffine(30), # 隨機線性轉換\n",
    "        transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0)), # 隨機子圖\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(), # 隨機色溫等\n",
    "        transforms.RandomGrayscale(),\n",
    "    ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.RandomErasing(0.2),\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "# testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qz6jeMnkQl0_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9YhZo7POPYG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1c-GwrMQqMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 180]       8,847,540\n",
      "              ReLU-2                  [-1, 180]               0\n",
      "            Linear-3                  [-1, 128]          23,168\n",
      "           Dropout-4                  [-1, 128]               0\n",
      "              ReLU-5                  [-1, 128]               0\n",
      "            Linear-6                   [-1, 64]           8,256\n",
      "              ReLU-7                   [-1, 64]               0\n",
      "            Linear-8                   [-1, 30]           1,950\n",
      "             PReLU-9                   [-1, 30]               1\n",
      "           Linear-10                   [-1, 11]             341\n",
      "================================================================\n",
      "Total params: 8,881,256\n",
      "Trainable params: 8,881,256\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 33.88\n",
      "Estimated Total Size (MB): 34.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, 128, 128]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3 * 128 * 128, 180),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(180, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 30),\n",
    "            nn.PReLU(1),\n",
    "\n",
    "            nn.Linear(30, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x.view(x.size()[0], -1))\n",
    "    \n",
    "summary(Classifier().cuda(), (3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEnGbriXORN3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5x-FH2Kr_jh"
   },
   "source": [
    "使用 training set 訓練，並使用 validation set 尋找好的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHaFE-8oQtkC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:440: UserWarning: torch.gels is deprecated in favour of torch.lstsq and will be removed in the next release. Please use torch.lstsq instead.\n",
      "  res = torch.gels(B, A)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] 16.01 sec(s) Train Acc: 0.194861 Loss: 0.035040 | Val Acc: 0.245481 loss: 0.033434\n",
      "[002/300] 16.06 sec(s) Train Acc: 0.222177 Loss: 0.033793 | Val Acc: 0.244315 loss: 0.032817\n",
      "[003/300] 16.12 sec(s) Train Acc: 0.237432 Loss: 0.033336 | Val Acc: 0.259184 loss: 0.032775\n",
      "[004/300] 16.11 sec(s) Train Acc: 0.240726 Loss: 0.033005 | Val Acc: 0.252770 loss: 0.032405\n",
      "[005/300] 16.01 sec(s) Train Acc: 0.246250 Loss: 0.032743 | Val Acc: 0.269388 loss: 0.032350\n",
      "[006/300] 16.02 sec(s) Train Acc: 0.252990 Loss: 0.032561 | Val Acc: 0.274927 loss: 0.031819\n",
      "[007/300] 16.07 sec(s) Train Acc: 0.263886 Loss: 0.032269 | Val Acc: 0.275219 loss: 0.031854\n",
      "[008/300] 16.05 sec(s) Train Acc: 0.263987 Loss: 0.032265 | Val Acc: 0.295627 loss: 0.031247\n",
      "[009/300] 16.07 sec(s) Train Acc: 0.273921 Loss: 0.032020 | Val Acc: 0.284840 loss: 0.031443\n",
      "[010/300] 16.06 sec(s) Train Acc: 0.272400 Loss: 0.031982 | Val Acc: 0.288047 loss: 0.031315\n",
      "[011/300] 16.06 sec(s) Train Acc: 0.275390 Loss: 0.031781 | Val Acc: 0.278426 loss: 0.031247\n",
      "[012/300] 16.01 sec(s) Train Acc: 0.279647 Loss: 0.031696 | Val Acc: 0.300583 loss: 0.030782\n",
      "[013/300] 16.05 sec(s) Train Acc: 0.278887 Loss: 0.031498 | Val Acc: 0.287755 loss: 0.031107\n",
      "[014/300] 16.07 sec(s) Train Acc: 0.284512 Loss: 0.031582 | Val Acc: 0.290962 loss: 0.030944\n",
      "[015/300] 15.94 sec(s) Train Acc: 0.291506 Loss: 0.031373 | Val Acc: 0.310787 loss: 0.030534\n",
      "[016/300] 16.09 sec(s) Train Acc: 0.291861 Loss: 0.031261 | Val Acc: 0.309621 loss: 0.030519\n",
      "[017/300] 15.93 sec(s) Train Acc: 0.297334 Loss: 0.031067 | Val Acc: 0.320408 loss: 0.030354\n",
      "[018/300] 16.03 sec(s) Train Acc: 0.297233 Loss: 0.031056 | Val Acc: 0.304956 loss: 0.030665\n",
      "[019/300] 16.18 sec(s) Train Acc: 0.293584 Loss: 0.031031 | Val Acc: 0.318076 loss: 0.030098\n",
      "[020/300] 16.11 sec(s) Train Acc: 0.302808 Loss: 0.030930 | Val Acc: 0.312828 loss: 0.030777\n",
      "[021/300] 16.02 sec(s) Train Acc: 0.308180 Loss: 0.030796 | Val Acc: 0.326531 loss: 0.030221\n",
      "[022/300] 16.12 sec(s) Train Acc: 0.305950 Loss: 0.030775 | Val Acc: 0.320408 loss: 0.030186\n",
      "[023/300] 16.16 sec(s) Train Acc: 0.310308 Loss: 0.030656 | Val Acc: 0.324781 loss: 0.030002\n",
      "[024/300] 16.13 sec(s) Train Acc: 0.312133 Loss: 0.030704 | Val Acc: 0.323032 loss: 0.030228\n",
      "[025/300] 16.03 sec(s) Train Acc: 0.318113 Loss: 0.030447 | Val Acc: 0.327697 loss: 0.029897\n",
      "[026/300] 16.08 sec(s) Train Acc: 0.310967 Loss: 0.030530 | Val Acc: 0.331195 loss: 0.029886\n",
      "[027/300] 16.12 sec(s) Train Acc: 0.317353 Loss: 0.030413 | Val Acc: 0.326822 loss: 0.029817\n",
      "[028/300] 16.07 sec(s) Train Acc: 0.319228 Loss: 0.030362 | Val Acc: 0.334985 loss: 0.029841\n",
      "[029/300] 16.12 sec(s) Train Acc: 0.317859 Loss: 0.030263 | Val Acc: 0.340525 loss: 0.029822\n",
      "[030/300] 16.00 sec(s) Train Acc: 0.320697 Loss: 0.030160 | Val Acc: 0.346647 loss: 0.029765\n",
      "[031/300] 16.03 sec(s) Train Acc: 0.328046 Loss: 0.030039 | Val Acc: 0.325364 loss: 0.029940\n",
      "[032/300] 16.07 sec(s) Train Acc: 0.323839 Loss: 0.030130 | Val Acc: 0.343149 loss: 0.029622\n",
      "[033/300] 16.09 sec(s) Train Acc: 0.326627 Loss: 0.029978 | Val Acc: 0.343732 loss: 0.029545\n",
      "[034/300] 16.24 sec(s) Train Acc: 0.325309 Loss: 0.029922 | Val Acc: 0.338192 loss: 0.029895\n",
      "[035/300] 15.90 sec(s) Train Acc: 0.325309 Loss: 0.029895 | Val Acc: 0.324781 loss: 0.029622\n",
      "[036/300] 16.02 sec(s) Train Acc: 0.326931 Loss: 0.029833 | Val Acc: 0.349271 loss: 0.029492\n",
      "[037/300] 15.95 sec(s) Train Acc: 0.331593 Loss: 0.029785 | Val Acc: 0.339942 loss: 0.029422\n",
      "[038/300] 16.10 sec(s) Train Acc: 0.335344 Loss: 0.029700 | Val Acc: 0.352770 loss: 0.029112\n",
      "[039/300] 16.01 sec(s) Train Acc: 0.335850 Loss: 0.029603 | Val Acc: 0.352770 loss: 0.029151\n",
      "[040/300] 16.14 sec(s) Train Acc: 0.339702 Loss: 0.029519 | Val Acc: 0.335277 loss: 0.029603\n",
      "[041/300] 16.12 sec(s) Train Acc: 0.332151 Loss: 0.029546 | Val Acc: 0.348105 loss: 0.029095\n",
      "[042/300] 15.99 sec(s) Train Acc: 0.336104 Loss: 0.029507 | Val Acc: 0.351312 loss: 0.029233\n",
      "[043/300] 15.98 sec(s) Train Acc: 0.340969 Loss: 0.029433 | Val Acc: 0.351020 loss: 0.029351\n",
      "[044/300] 16.12 sec(s) Train Acc: 0.343908 Loss: 0.029384 | Val Acc: 0.338484 loss: 0.029954\n",
      "[045/300] 16.05 sec(s) Train Acc: 0.345936 Loss: 0.029228 | Val Acc: 0.350146 loss: 0.029221\n",
      "[046/300] 16.04 sec(s) Train Acc: 0.339955 Loss: 0.029315 | Val Acc: 0.356560 loss: 0.029331\n",
      "[047/300] 16.15 sec(s) Train Acc: 0.344060 Loss: 0.029228 | Val Acc: 0.356851 loss: 0.029208\n",
      "[048/300] 16.06 sec(s) Train Acc: 0.349787 Loss: 0.029169 | Val Acc: 0.359767 loss: 0.029085\n",
      "[049/300] 16.09 sec(s) Train Acc: 0.352473 Loss: 0.029015 | Val Acc: 0.355977 loss: 0.029003\n",
      "[050/300] 15.97 sec(s) Train Acc: 0.347203 Loss: 0.029045 | Val Acc: 0.356851 loss: 0.029006\n",
      "[051/300] 16.08 sec(s) Train Acc: 0.353487 Loss: 0.029091 | Val Acc: 0.343732 loss: 0.029252\n",
      "[052/300] 16.17 sec(s) Train Acc: 0.348115 Loss: 0.028920 | Val Acc: 0.358601 loss: 0.029371\n",
      "[053/300] 16.07 sec(s) Train Acc: 0.350699 Loss: 0.028882 | Val Acc: 0.352770 loss: 0.029030\n",
      "[054/300] 16.08 sec(s) Train Acc: 0.356021 Loss: 0.028855 | Val Acc: 0.348105 loss: 0.029080\n",
      "[055/300] 16.05 sec(s) Train Acc: 0.357592 Loss: 0.028817 | Val Acc: 0.352770 loss: 0.029126\n",
      "[056/300] 16.01 sec(s) Train Acc: 0.352929 Loss: 0.028918 | Val Acc: 0.354810 loss: 0.028977\n",
      "[057/300] 16.08 sec(s) Train Acc: 0.355767 Loss: 0.028753 | Val Acc: 0.362099 loss: 0.029032\n",
      "[058/300] 16.05 sec(s) Train Acc: 0.361443 Loss: 0.028591 | Val Acc: 0.351020 loss: 0.029055\n",
      "[059/300] 16.03 sec(s) Train Acc: 0.358909 Loss: 0.028616 | Val Acc: 0.362099 loss: 0.028738\n",
      "[060/300] 16.03 sec(s) Train Acc: 0.359619 Loss: 0.028650 | Val Acc: 0.351895 loss: 0.029052\n",
      "[061/300] 16.07 sec(s) Train Acc: 0.354956 Loss: 0.028551 | Val Acc: 0.355977 loss: 0.028935\n",
      "[062/300] 16.06 sec(s) Train Acc: 0.360886 Loss: 0.028475 | Val Acc: 0.354519 loss: 0.028842\n",
      "[063/300] 16.04 sec(s) Train Acc: 0.363318 Loss: 0.028502 | Val Acc: 0.358017 loss: 0.028819\n",
      "[064/300] 16.03 sec(s) Train Acc: 0.361899 Loss: 0.028478 | Val Acc: 0.360058 loss: 0.028897\n",
      "[065/300] 16.10 sec(s) Train Acc: 0.362964 Loss: 0.028407 | Val Acc: 0.357726 loss: 0.028868\n",
      "[066/300] 16.05 sec(s) Train Acc: 0.365650 Loss: 0.028354 | Val Acc: 0.360641 loss: 0.028788\n",
      "[067/300] 16.14 sec(s) Train Acc: 0.362406 Loss: 0.028591 | Val Acc: 0.355685 loss: 0.029045\n",
      "[068/300] 16.15 sec(s) Train Acc: 0.364940 Loss: 0.028351 | Val Acc: 0.354519 loss: 0.029231\n",
      "[069/300] 16.09 sec(s) Train Acc: 0.367576 Loss: 0.028299 | Val Acc: 0.370554 loss: 0.028226\n",
      "[070/300] 16.06 sec(s) Train Acc: 0.370008 Loss: 0.028250 | Val Acc: 0.362099 loss: 0.028806\n",
      "[071/300] 16.13 sec(s) Train Acc: 0.369349 Loss: 0.028205 | Val Acc: 0.367055 loss: 0.028782\n",
      "[072/300] 16.08 sec(s) Train Acc: 0.360835 Loss: 0.028337 | Val Acc: 0.351020 loss: 0.028856\n",
      "[073/300] 16.03 sec(s) Train Acc: 0.368032 Loss: 0.028106 | Val Acc: 0.358601 loss: 0.028842\n",
      "[074/300] 15.97 sec(s) Train Acc: 0.372441 Loss: 0.028118 | Val Acc: 0.365306 loss: 0.028640\n",
      "[075/300] 15.94 sec(s) Train Acc: 0.372542 Loss: 0.027960 | Val Acc: 0.360058 loss: 0.028699\n",
      "[076/300] 16.20 sec(s) Train Acc: 0.367373 Loss: 0.028202 | Val Acc: 0.361808 loss: 0.028579\n",
      "[077/300] 16.09 sec(s) Train Acc: 0.374316 Loss: 0.028117 | Val Acc: 0.365015 loss: 0.028948\n",
      "[078/300] 16.15 sec(s) Train Acc: 0.374620 Loss: 0.028005 | Val Acc: 0.376676 loss: 0.029140\n",
      "[079/300] 16.13 sec(s) Train Acc: 0.375938 Loss: 0.027974 | Val Acc: 0.379300 loss: 0.028563\n",
      "[080/300] 16.14 sec(s) Train Acc: 0.376799 Loss: 0.027978 | Val Acc: 0.367347 loss: 0.028966\n",
      "[081/300] 16.02 sec(s) Train Acc: 0.377103 Loss: 0.027808 | Val Acc: 0.362682 loss: 0.028837\n",
      "[082/300] 16.08 sec(s) Train Acc: 0.376090 Loss: 0.027857 | Val Acc: 0.373761 loss: 0.029098\n",
      "[083/300] 16.00 sec(s) Train Acc: 0.382627 Loss: 0.027802 | Val Acc: 0.370262 loss: 0.028889\n",
      "[084/300] 15.98 sec(s) Train Acc: 0.380701 Loss: 0.027778 | Val Acc: 0.360641 loss: 0.028736\n",
      "[085/300] 16.02 sec(s) Train Acc: 0.380448 Loss: 0.027763 | Val Acc: 0.375219 loss: 0.028621\n",
      "[086/300] 16.51 sec(s) Train Acc: 0.378522 Loss: 0.027814 | Val Acc: 0.367638 loss: 0.028626\n",
      "[087/300] 18.39 sec(s) Train Acc: 0.374721 Loss: 0.027843 | Val Acc: 0.380758 loss: 0.028327\n",
      "[088/300] 16.74 sec(s) Train Acc: 0.373860 Loss: 0.027815 | Val Acc: 0.371137 loss: 0.028615\n",
      "[089/300] 16.07 sec(s) Train Acc: 0.381056 Loss: 0.027778 | Val Acc: 0.366472 loss: 0.028474\n",
      "[090/300] 16.10 sec(s) Train Acc: 0.382931 Loss: 0.027663 | Val Acc: 0.360058 loss: 0.028637\n",
      "[091/300] 16.15 sec(s) Train Acc: 0.377053 Loss: 0.027742 | Val Acc: 0.363265 loss: 0.028808\n",
      "[092/300] 16.00 sec(s) Train Acc: 0.387138 Loss: 0.027437 | Val Acc: 0.356560 loss: 0.029166\n",
      "[093/300] 15.91 sec(s) Train Acc: 0.380499 Loss: 0.027667 | Val Acc: 0.363848 loss: 0.028996\n",
      "[094/300] 16.09 sec(s) Train Acc: 0.381766 Loss: 0.027635 | Val Acc: 0.368513 loss: 0.028244\n",
      "[095/300] 16.12 sec(s) Train Acc: 0.385161 Loss: 0.027596 | Val Acc: 0.369388 loss: 0.028508\n",
      "[096/300] 16.16 sec(s) Train Acc: 0.386124 Loss: 0.027566 | Val Acc: 0.357726 loss: 0.028731\n",
      "[097/300] 16.19 sec(s) Train Acc: 0.384097 Loss: 0.027566 | Val Acc: 0.371137 loss: 0.028796\n",
      "[098/300] 16.04 sec(s) Train Acc: 0.384502 Loss: 0.027566 | Val Acc: 0.364431 loss: 0.028494\n",
      "[099/300] 16.00 sec(s) Train Acc: 0.383793 Loss: 0.027453 | Val Acc: 0.373469 loss: 0.028775\n",
      "[100/300] 16.05 sec(s) Train Acc: 0.385972 Loss: 0.027656 | Val Acc: 0.353644 loss: 0.029422\n",
      "[101/300] 16.03 sec(s) Train Acc: 0.384654 Loss: 0.027549 | Val Acc: 0.378426 loss: 0.028347\n",
      "[102/300] 16.12 sec(s) Train Acc: 0.387340 Loss: 0.027399 | Val Acc: 0.378717 loss: 0.028230\n",
      "[103/300] 16.10 sec(s) Train Acc: 0.390787 Loss: 0.027309 | Val Acc: 0.374344 loss: 0.028512\n",
      "[104/300] 16.01 sec(s) Train Acc: 0.390178 Loss: 0.027239 | Val Acc: 0.379592 loss: 0.028381\n",
      "[105/300] 16.10 sec(s) Train Acc: 0.385567 Loss: 0.027373 | Val Acc: 0.370845 loss: 0.028411\n",
      "[106/300] 15.97 sec(s) Train Acc: 0.390026 Loss: 0.027299 | Val Acc: 0.390087 loss: 0.028021\n",
      "[107/300] 16.07 sec(s) Train Acc: 0.392864 Loss: 0.027356 | Val Acc: 0.370845 loss: 0.028865\n",
      "[108/300] 16.00 sec(s) Train Acc: 0.389418 Loss: 0.027345 | Val Acc: 0.373469 loss: 0.028331\n",
      "[109/300] 16.06 sec(s) Train Acc: 0.395500 Loss: 0.027118 | Val Acc: 0.362391 loss: 0.028615\n",
      "[110/300] 16.08 sec(s) Train Acc: 0.390482 Loss: 0.027204 | Val Acc: 0.374344 loss: 0.028241\n",
      "[111/300] 16.03 sec(s) Train Acc: 0.393168 Loss: 0.027193 | Val Acc: 0.377259 loss: 0.028564\n",
      "[112/300] 15.99 sec(s) Train Acc: 0.396716 Loss: 0.027157 | Val Acc: 0.370262 loss: 0.028678\n",
      "[113/300] 16.06 sec(s) Train Acc: 0.392408 Loss: 0.027215 | Val Acc: 0.373178 loss: 0.028393\n",
      "[114/300] 16.10 sec(s) Train Acc: 0.397476 Loss: 0.027021 | Val Acc: 0.369679 loss: 0.028628\n",
      "[115/300] 16.00 sec(s) Train Acc: 0.391445 Loss: 0.027221 | Val Acc: 0.367347 loss: 0.029020\n",
      "[116/300] 15.85 sec(s) Train Acc: 0.395145 Loss: 0.027208 | Val Acc: 0.367055 loss: 0.028404\n",
      "[117/300] 15.96 sec(s) Train Acc: 0.403608 Loss: 0.026955 | Val Acc: 0.369679 loss: 0.028600\n",
      "[118/300] 16.14 sec(s) Train Acc: 0.395905 Loss: 0.027005 | Val Acc: 0.374344 loss: 0.028499\n",
      "[119/300] 16.11 sec(s) Train Acc: 0.390330 Loss: 0.027123 | Val Acc: 0.368805 loss: 0.028782\n",
      "[120/300] 16.05 sec(s) Train Acc: 0.394689 Loss: 0.027011 | Val Acc: 0.372303 loss: 0.028561\n",
      "[121/300] 15.97 sec(s) Train Acc: 0.398135 Loss: 0.026899 | Val Acc: 0.380175 loss: 0.028293\n",
      "[122/300] 16.03 sec(s) Train Acc: 0.401125 Loss: 0.026835 | Val Acc: 0.358601 loss: 0.028798\n",
      "[123/300] 16.06 sec(s) Train Acc: 0.391243 Loss: 0.027102 | Val Acc: 0.382799 loss: 0.028427\n",
      "[124/300] 16.04 sec(s) Train Acc: 0.394131 Loss: 0.027185 | Val Acc: 0.376676 loss: 0.028853\n",
      "[125/300] 16.08 sec(s) Train Acc: 0.395449 Loss: 0.027091 | Val Acc: 0.362682 loss: 0.029321\n",
      "[126/300] 16.14 sec(s) Train Acc: 0.401277 Loss: 0.026796 | Val Acc: 0.360350 loss: 0.028789\n",
      "[127/300] 16.04 sec(s) Train Acc: 0.404926 Loss: 0.026858 | Val Acc: 0.379300 loss: 0.028683\n",
      "[128/300] 16.07 sec(s) Train Acc: 0.399909 Loss: 0.027010 | Val Acc: 0.365598 loss: 0.029189\n",
      "[129/300] 16.06 sec(s) Train Acc: 0.399554 Loss: 0.026928 | Val Acc: 0.380175 loss: 0.028941\n",
      "[130/300] 16.12 sec(s) Train Acc: 0.405737 Loss: 0.026776 | Val Acc: 0.377843 loss: 0.028474\n",
      "[131/300] 16.05 sec(s) Train Acc: 0.397679 Loss: 0.026951 | Val Acc: 0.386297 loss: 0.028390\n",
      "[132/300] 16.04 sec(s) Train Acc: 0.406142 Loss: 0.026609 | Val Acc: 0.378717 loss: 0.030476\n",
      "[133/300] 16.06 sec(s) Train Acc: 0.403760 Loss: 0.026723 | Val Acc: 0.378717 loss: 0.028503\n",
      "[134/300] 16.14 sec(s) Train Acc: 0.401378 Loss: 0.026844 | Val Acc: 0.380175 loss: 0.028606\n",
      "[135/300] 16.13 sec(s) Train Acc: 0.402696 Loss: 0.026877 | Val Acc: 0.381633 loss: 0.028215\n",
      "[136/300] 16.04 sec(s) Train Acc: 0.403254 Loss: 0.026660 | Val Acc: 0.377259 loss: 0.028737\n",
      "[137/300] 16.02 sec(s) Train Acc: 0.402291 Loss: 0.026759 | Val Acc: 0.373178 loss: 0.028387\n",
      "[138/300] 16.02 sec(s) Train Acc: 0.407916 Loss: 0.026673 | Val Acc: 0.376968 loss: 0.028930\n",
      "[139/300] 16.05 sec(s) Train Acc: 0.409284 Loss: 0.026631 | Val Acc: 0.373178 loss: 0.028364\n",
      "[140/300] 16.07 sec(s) Train Acc: 0.408524 Loss: 0.026532 | Val Acc: 0.380175 loss: 0.028409\n",
      "[141/300] 16.08 sec(s) Train Acc: 0.407865 Loss: 0.026647 | Val Acc: 0.386297 loss: 0.028687\n",
      "[142/300] 15.96 sec(s) Train Acc: 0.409031 Loss: 0.026689 | Val Acc: 0.380175 loss: 0.028135\n",
      "[143/300] 16.08 sec(s) Train Acc: 0.403000 Loss: 0.026723 | Val Acc: 0.375219 loss: 0.028416\n",
      "[144/300] 16.18 sec(s) Train Acc: 0.409791 Loss: 0.026576 | Val Acc: 0.379300 loss: 0.028906\n",
      "[145/300] 16.02 sec(s) Train Acc: 0.407105 Loss: 0.026487 | Val Acc: 0.378426 loss: 0.028829\n",
      "[146/300] 16.08 sec(s) Train Acc: 0.409791 Loss: 0.026669 | Val Acc: 0.381633 loss: 0.028337\n",
      "[147/300] 16.06 sec(s) Train Acc: 0.406953 Loss: 0.026674 | Val Acc: 0.384257 loss: 0.028156\n",
      "[148/300] 16.10 sec(s) Train Acc: 0.408980 Loss: 0.026709 | Val Acc: 0.375510 loss: 0.028587\n",
      "[149/300] 15.98 sec(s) Train Acc: 0.412021 Loss: 0.026345 | Val Acc: 0.376676 loss: 0.028084\n",
      "[150/300] 16.15 sec(s) Train Acc: 0.404115 Loss: 0.026464 | Val Acc: 0.381633 loss: 0.028142\n",
      "[151/300] 16.13 sec(s) Train Acc: 0.404723 Loss: 0.026784 | Val Acc: 0.372303 loss: 0.028395\n",
      "[152/300] 16.06 sec(s) Train Acc: 0.407967 Loss: 0.026578 | Val Acc: 0.390671 loss: 0.028857\n",
      "[153/300] 16.02 sec(s) Train Acc: 0.410349 Loss: 0.026501 | Val Acc: 0.377843 loss: 0.028641\n",
      "[154/300] 16.03 sec(s) Train Acc: 0.410551 Loss: 0.026487 | Val Acc: 0.381050 loss: 0.028323\n",
      "[155/300] 15.97 sec(s) Train Acc: 0.407055 Loss: 0.026510 | Val Acc: 0.376968 loss: 0.028307\n",
      "[156/300] 16.14 sec(s) Train Acc: 0.413085 Loss: 0.026443 | Val Acc: 0.388338 loss: 0.028078\n",
      "[157/300] 16.08 sec(s) Train Acc: 0.411464 Loss: 0.026470 | Val Acc: 0.384840 loss: 0.028053\n",
      "[158/300] 15.92 sec(s) Train Acc: 0.414960 Loss: 0.026416 | Val Acc: 0.383673 loss: 0.028037\n",
      "[159/300] 16.03 sec(s) Train Acc: 0.414099 Loss: 0.026452 | Val Acc: 0.386297 loss: 0.027959\n",
      "[160/300] 16.06 sec(s) Train Acc: 0.417494 Loss: 0.026326 | Val Acc: 0.382507 loss: 0.028293\n",
      "[161/300] 16.10 sec(s) Train Acc: 0.415417 Loss: 0.026301 | Val Acc: 0.385714 loss: 0.028068\n",
      "[162/300] 16.09 sec(s) Train Acc: 0.411768 Loss: 0.026417 | Val Acc: 0.376093 loss: 0.028170\n",
      "[163/300] 16.00 sec(s) Train Acc: 0.414808 Loss: 0.026341 | Val Acc: 0.381924 loss: 0.028338\n",
      "[164/300] 15.92 sec(s) Train Acc: 0.415619 Loss: 0.026173 | Val Acc: 0.378717 loss: 0.028266\n",
      "[165/300] 16.02 sec(s) Train Acc: 0.412832 Loss: 0.026293 | Val Acc: 0.376385 loss: 0.028237\n",
      "[166/300] 16.05 sec(s) Train Acc: 0.415467 Loss: 0.026490 | Val Acc: 0.377551 loss: 0.028601\n",
      "[167/300] 16.04 sec(s) Train Acc: 0.405636 Loss: 0.026700 | Val Acc: 0.379009 loss: 0.028445\n",
      "[168/300] 16.14 sec(s) Train Acc: 0.412984 Loss: 0.026235 | Val Acc: 0.374927 loss: 0.028183\n",
      "[169/300] 16.09 sec(s) Train Acc: 0.414808 Loss: 0.026254 | Val Acc: 0.383673 loss: 0.028157\n",
      "[170/300] 16.03 sec(s) Train Acc: 0.416278 Loss: 0.026289 | Val Acc: 0.386297 loss: 0.028327\n",
      "[171/300] 16.11 sec(s) Train Acc: 0.406548 Loss: 0.026383 | Val Acc: 0.378717 loss: 0.028196\n",
      "[172/300] 15.97 sec(s) Train Acc: 0.417798 Loss: 0.026013 | Val Acc: 0.390379 loss: 0.029304\n",
      "[173/300] 16.00 sec(s) Train Acc: 0.419978 Loss: 0.026230 | Val Acc: 0.383965 loss: 0.028672\n",
      "[174/300] 16.01 sec(s) Train Acc: 0.414251 Loss: 0.026312 | Val Acc: 0.378717 loss: 0.028024\n",
      "[175/300] 16.03 sec(s) Train Acc: 0.417494 Loss: 0.026100 | Val Acc: 0.381341 loss: 0.028258\n",
      "[176/300] 16.01 sec(s) Train Acc: 0.415265 Loss: 0.026085 | Val Acc: 0.379592 loss: 0.028136\n",
      "[177/300] 16.10 sec(s) Train Acc: 0.418508 Loss: 0.026113 | Val Acc: 0.386297 loss: 0.027855\n",
      "[178/300] 16.00 sec(s) Train Acc: 0.413136 Loss: 0.026176 | Val Acc: 0.377843 loss: 0.028146\n",
      "[179/300] 16.03 sec(s) Train Acc: 0.419572 Loss: 0.026114 | Val Acc: 0.382507 loss: 0.028169\n",
      "[180/300] 16.05 sec(s) Train Acc: 0.418407 Loss: 0.026126 | Val Acc: 0.389213 loss: 0.028120\n",
      "[181/300] 16.01 sec(s) Train Acc: 0.413947 Loss: 0.026271 | Val Acc: 0.379300 loss: 0.028112\n",
      "[182/300] 16.46 sec(s) Train Acc: 0.414454 Loss: 0.026271 | Val Acc: 0.374927 loss: 0.028372\n",
      "[183/300] 16.91 sec(s) Train Acc: 0.411160 Loss: 0.026448 | Val Acc: 0.386880 loss: 0.028376\n",
      "[184/300] 16.15 sec(s) Train Acc: 0.421397 Loss: 0.025970 | Val Acc: 0.383382 loss: 0.028222\n",
      "[185/300] 16.53 sec(s) Train Acc: 0.422512 Loss: 0.026003 | Val Acc: 0.384840 loss: 0.028036\n",
      "[186/300] 16.38 sec(s) Train Acc: 0.419978 Loss: 0.026004 | Val Acc: 0.399125 loss: 0.028061\n",
      "[187/300] 16.18 sec(s) Train Acc: 0.416734 Loss: 0.026133 | Val Acc: 0.387172 loss: 0.028849\n",
      "[188/300] 16.11 sec(s) Train Acc: 0.426769 Loss: 0.025906 | Val Acc: 0.382216 loss: 0.029112\n",
      "[189/300] 16.10 sec(s) Train Acc: 0.417494 Loss: 0.026323 | Val Acc: 0.381050 loss: 0.028851\n",
      "[190/300] 16.49 sec(s) Train Acc: 0.423981 Loss: 0.025935 | Val Acc: 0.388047 loss: 0.028392\n",
      "[191/300] 16.24 sec(s) Train Acc: 0.424336 Loss: 0.026047 | Val Acc: 0.388921 loss: 0.028569\n",
      "[192/300] 16.19 sec(s) Train Acc: 0.414150 Loss: 0.026209 | Val Acc: 0.380175 loss: 0.028230\n",
      "[193/300] 16.08 sec(s) Train Acc: 0.423981 Loss: 0.026044 | Val Acc: 0.379883 loss: 0.028060\n",
      "[194/300] 16.08 sec(s) Train Acc: 0.425198 Loss: 0.025817 | Val Acc: 0.377551 loss: 0.028236\n",
      "[195/300] 16.06 sec(s) Train Acc: 0.425147 Loss: 0.025886 | Val Acc: 0.383382 loss: 0.028581\n",
      "[196/300] 16.22 sec(s) Train Acc: 0.426718 Loss: 0.025836 | Val Acc: 0.379592 loss: 0.028913\n",
      "[197/300] 16.53 sec(s) Train Acc: 0.425603 Loss: 0.025875 | Val Acc: 0.382799 loss: 0.028701\n",
      "[198/300] 16.65 sec(s) Train Acc: 0.427377 Loss: 0.025827 | Val Acc: 0.379883 loss: 0.028205\n",
      "[199/300] 16.68 sec(s) Train Acc: 0.419724 Loss: 0.025873 | Val Acc: 0.384257 loss: 0.028022\n",
      "[200/300] 16.43 sec(s) Train Acc: 0.422562 Loss: 0.025928 | Val Acc: 0.387755 loss: 0.028445\n",
      "[201/300] 16.17 sec(s) Train Acc: 0.426769 Loss: 0.025759 | Val Acc: 0.382216 loss: 0.028233\n",
      "[202/300] 16.17 sec(s) Train Acc: 0.433509 Loss: 0.025737 | Val Acc: 0.377551 loss: 0.029826\n",
      "[203/300] 16.11 sec(s) Train Acc: 0.421194 Loss: 0.025985 | Val Acc: 0.382507 loss: 0.028645\n",
      "[204/300] 16.04 sec(s) Train Acc: 0.419218 Loss: 0.026032 | Val Acc: 0.386297 loss: 0.028149\n",
      "[205/300] 16.16 sec(s) Train Acc: 0.428086 Loss: 0.025832 | Val Acc: 0.384548 loss: 0.028111\n",
      "[206/300] 16.01 sec(s) Train Acc: 0.428492 Loss: 0.025893 | Val Acc: 0.380466 loss: 0.028381\n",
      "[207/300] 16.03 sec(s) Train Acc: 0.432495 Loss: 0.025647 | Val Acc: 0.379592 loss: 0.028274\n",
      "[208/300] 16.01 sec(s) Train Acc: 0.425198 Loss: 0.025974 | Val Acc: 0.386006 loss: 0.028071\n",
      "[209/300] 16.05 sec(s) Train Acc: 0.427884 Loss: 0.025713 | Val Acc: 0.382507 loss: 0.028465\n",
      "[210/300] 16.12 sec(s) Train Acc: 0.424742 Loss: 0.025787 | Val Acc: 0.387464 loss: 0.027875\n",
      "[211/300] 16.11 sec(s) Train Acc: 0.427123 Loss: 0.025835 | Val Acc: 0.371137 loss: 0.029395\n",
      "[212/300] 16.02 sec(s) Train Acc: 0.429708 Loss: 0.025731 | Val Acc: 0.383965 loss: 0.028293\n",
      "[213/300] 16.09 sec(s) Train Acc: 0.428289 Loss: 0.025594 | Val Acc: 0.386006 loss: 0.027990\n",
      "[214/300] 16.10 sec(s) Train Acc: 0.430772 Loss: 0.025646 | Val Acc: 0.386880 loss: 0.027949\n",
      "[215/300] 16.15 sec(s) Train Acc: 0.424032 Loss: 0.025756 | Val Acc: 0.386589 loss: 0.028069\n",
      "[216/300] 15.96 sec(s) Train Acc: 0.424133 Loss: 0.025811 | Val Acc: 0.391837 loss: 0.027747\n",
      "[217/300] 16.08 sec(s) Train Acc: 0.426769 Loss: 0.025821 | Val Acc: 0.372886 loss: 0.029763\n",
      "[218/300] 16.13 sec(s) Train Acc: 0.424437 Loss: 0.025937 | Val Acc: 0.384840 loss: 0.029012\n",
      "[219/300] 16.34 sec(s) Train Acc: 0.422866 Loss: 0.025770 | Val Acc: 0.381924 loss: 0.029961\n",
      "[220/300] 16.24 sec(s) Train Acc: 0.414150 Loss: 0.026309 | Val Acc: 0.375219 loss: 0.028460\n",
      "[221/300] 16.21 sec(s) Train Acc: 0.426617 Loss: 0.025959 | Val Acc: 0.376968 loss: 0.028230\n",
      "[222/300] 16.17 sec(s) Train Acc: 0.428897 Loss: 0.025778 | Val Acc: 0.390962 loss: 0.027863\n",
      "[223/300] 16.12 sec(s) Train Acc: 0.426617 Loss: 0.025859 | Val Acc: 0.378134 loss: 0.028157\n",
      "[224/300] 16.09 sec(s) Train Acc: 0.433762 Loss: 0.025487 | Val Acc: 0.389504 loss: 0.028982\n",
      "[225/300] 16.11 sec(s) Train Acc: 0.431330 Loss: 0.025715 | Val Acc: 0.389504 loss: 0.028152\n",
      "[226/300] 16.06 sec(s) Train Acc: 0.427884 Loss: 0.025615 | Val Acc: 0.377551 loss: 0.028204\n",
      "[227/300] 16.02 sec(s) Train Acc: 0.435942 Loss: 0.025453 | Val Acc: 0.388921 loss: 0.028783\n",
      "[228/300] 16.18 sec(s) Train Acc: 0.435638 Loss: 0.025422 | Val Acc: 0.382507 loss: 0.028514\n",
      "[229/300] 16.26 sec(s) Train Acc: 0.428188 Loss: 0.025652 | Val Acc: 0.375219 loss: 0.028530\n",
      "[230/300] 16.17 sec(s) Train Acc: 0.438678 Loss: 0.025466 | Val Acc: 0.382216 loss: 0.028232\n",
      "[231/300] 16.21 sec(s) Train Acc: 0.429505 Loss: 0.025498 | Val Acc: 0.385714 loss: 0.028569\n",
      "[232/300] 16.25 sec(s) Train Acc: 0.432597 Loss: 0.025756 | Val Acc: 0.370262 loss: 0.028304\n",
      "[233/300] 16.28 sec(s) Train Acc: 0.428593 Loss: 0.025818 | Val Acc: 0.389213 loss: 0.027886\n",
      "[234/300] 16.58 sec(s) Train Acc: 0.433864 Loss: 0.025464 | Val Acc: 0.386297 loss: 0.028215\n",
      "[235/300] 16.36 sec(s) Train Acc: 0.436499 Loss: 0.025329 | Val Acc: 0.382799 loss: 0.028375\n",
      "[236/300] 16.24 sec(s) Train Acc: 0.433712 Loss: 0.025514 | Val Acc: 0.384840 loss: 0.028462\n",
      "[237/300] 16.55 sec(s) Train Acc: 0.436448 Loss: 0.025689 | Val Acc: 0.383965 loss: 0.028258\n",
      "[238/300] 16.44 sec(s) Train Acc: 0.435891 Loss: 0.025521 | Val Acc: 0.381633 loss: 0.028193\n",
      "[239/300] 16.72 sec(s) Train Acc: 0.436094 Loss: 0.025475 | Val Acc: 0.386589 loss: 0.028080\n",
      "[240/300] 16.35 sec(s) Train Acc: 0.437513 Loss: 0.025464 | Val Acc: 0.380466 loss: 0.029042\n",
      "[241/300] 16.45 sec(s) Train Acc: 0.430519 Loss: 0.025516 | Val Acc: 0.386006 loss: 0.028075\n",
      "[242/300] 16.24 sec(s) Train Acc: 0.435435 Loss: 0.025526 | Val Acc: 0.383965 loss: 0.028328\n",
      "[243/300] 16.11 sec(s) Train Acc: 0.431735 Loss: 0.025675 | Val Acc: 0.386589 loss: 0.028137\n",
      "[244/300] 16.07 sec(s) Train Acc: 0.436752 Loss: 0.025514 | Val Acc: 0.377551 loss: 0.028912\n",
      "[245/300] 15.99 sec(s) Train Acc: 0.436600 Loss: 0.025380 | Val Acc: 0.388630 loss: 0.029079\n",
      "[246/300] 16.14 sec(s) Train Acc: 0.441719 Loss: 0.025262 | Val Acc: 0.390962 loss: 0.028565\n",
      "[247/300] 15.99 sec(s) Train Acc: 0.431380 Loss: 0.025453 | Val Acc: 0.382799 loss: 0.029179\n",
      "[248/300] 16.13 sec(s) Train Acc: 0.432698 Loss: 0.025550 | Val Acc: 0.384840 loss: 0.028389\n",
      "[249/300] 15.92 sec(s) Train Acc: 0.435283 Loss: 0.025679 | Val Acc: 0.376676 loss: 0.028877\n",
      "[250/300] 15.91 sec(s) Train Acc: 0.424437 Loss: 0.025658 | Val Acc: 0.369971 loss: 0.029488\n",
      "[251/300] 15.98 sec(s) Train Acc: 0.429809 Loss: 0.025650 | Val Acc: 0.383090 loss: 0.028079\n",
      "[252/300] 16.14 sec(s) Train Acc: 0.429809 Loss: 0.025627 | Val Acc: 0.383382 loss: 0.028260\n",
      "[253/300] 16.06 sec(s) Train Acc: 0.439388 Loss: 0.025200 | Val Acc: 0.388047 loss: 0.028266\n",
      "[254/300] 16.14 sec(s) Train Acc: 0.438425 Loss: 0.025239 | Val Acc: 0.378426 loss: 0.029108\n",
      "[255/300] 16.17 sec(s) Train Acc: 0.439540 Loss: 0.025307 | Val Acc: 0.390671 loss: 0.028317\n",
      "[256/300] 16.05 sec(s) Train Acc: 0.436448 Loss: 0.025364 | Val Acc: 0.384257 loss: 0.027879\n",
      "[257/300] 16.11 sec(s) Train Acc: 0.437107 Loss: 0.025404 | Val Acc: 0.376968 loss: 0.028109\n",
      "[258/300] 16.21 sec(s) Train Acc: 0.431887 Loss: 0.025618 | Val Acc: 0.387464 loss: 0.028628\n",
      "[259/300] 16.00 sec(s) Train Acc: 0.437006 Loss: 0.025316 | Val Acc: 0.390379 loss: 0.028152\n",
      "[260/300] 16.09 sec(s) Train Acc: 0.436955 Loss: 0.025369 | Val Acc: 0.385131 loss: 0.029337\n",
      "[261/300] 16.23 sec(s) Train Acc: 0.437969 Loss: 0.025426 | Val Acc: 0.380466 loss: 0.028518\n",
      "[262/300] 16.13 sec(s) Train Acc: 0.440047 Loss: 0.025238 | Val Acc: 0.391254 loss: 0.028069\n",
      "[263/300] 15.95 sec(s) Train Acc: 0.442581 Loss: 0.025372 | Val Acc: 0.364723 loss: 0.028783\n",
      "[264/300] 16.03 sec(s) Train Acc: 0.426769 Loss: 0.025820 | Val Acc: 0.389213 loss: 0.029207\n",
      "[265/300] 16.04 sec(s) Train Acc: 0.440756 Loss: 0.025331 | Val Acc: 0.378717 loss: 0.029106\n",
      "[266/300] 16.07 sec(s) Train Acc: 0.438121 Loss: 0.025341 | Val Acc: 0.387172 loss: 0.027874\n",
      "[267/300] 16.10 sec(s) Train Acc: 0.440351 Loss: 0.025188 | Val Acc: 0.385423 loss: 0.029040\n",
      "[268/300] 16.13 sec(s) Train Acc: 0.437411 Loss: 0.025390 | Val Acc: 0.389796 loss: 0.027982\n",
      "[269/300] 16.10 sec(s) Train Acc: 0.439438 Loss: 0.025367 | Val Acc: 0.387172 loss: 0.028150\n",
      "[270/300] 16.12 sec(s) Train Acc: 0.438019 Loss: 0.025501 | Val Acc: 0.378134 loss: 0.029347\n",
      "[271/300] 16.10 sec(s) Train Acc: 0.439945 Loss: 0.025235 | Val Acc: 0.393586 loss: 0.027685\n",
      "[272/300] 16.09 sec(s) Train Acc: 0.442226 Loss: 0.025054 | Val Acc: 0.380175 loss: 0.028441\n",
      "[273/300] 16.08 sec(s) Train Acc: 0.440604 Loss: 0.025263 | Val Acc: 0.380466 loss: 0.028319\n",
      "[274/300] 16.03 sec(s) Train Acc: 0.441415 Loss: 0.025302 | Val Acc: 0.381924 loss: 0.027967\n",
      "[275/300] 15.97 sec(s) Train Acc: 0.445115 Loss: 0.025039 | Val Acc: 0.382799 loss: 0.028087\n",
      "[276/300] 16.17 sec(s) Train Acc: 0.444405 Loss: 0.025163 | Val Acc: 0.376385 loss: 0.028080\n",
      "[277/300] 16.02 sec(s) Train Acc: 0.438729 Loss: 0.025282 | Val Acc: 0.383673 loss: 0.028418\n",
      "[278/300] 16.01 sec(s) Train Acc: 0.444861 Loss: 0.025160 | Val Acc: 0.389504 loss: 0.028054\n",
      "[279/300] 16.08 sec(s) Train Acc: 0.442429 Loss: 0.025236 | Val Acc: 0.375802 loss: 0.028557\n",
      "[280/300] 16.13 sec(s) Train Acc: 0.444354 Loss: 0.025241 | Val Acc: 0.383090 loss: 0.028099\n",
      "[281/300] 16.11 sec(s) Train Acc: 0.447091 Loss: 0.025060 | Val Acc: 0.384257 loss: 0.028743\n",
      "[282/300] 16.08 sec(s) Train Acc: 0.438830 Loss: 0.025257 | Val Acc: 0.385714 loss: 0.028255\n",
      "[283/300] 16.00 sec(s) Train Acc: 0.439337 Loss: 0.025561 | Val Acc: 0.379009 loss: 0.028190\n",
      "[284/300] 16.00 sec(s) Train Acc: 0.438425 Loss: 0.025414 | Val Acc: 0.377843 loss: 0.028349\n",
      "[285/300] 16.03 sec(s) Train Acc: 0.438932 Loss: 0.025162 | Val Acc: 0.384548 loss: 0.028289\n",
      "[286/300] 15.95 sec(s) Train Acc: 0.445976 Loss: 0.025167 | Val Acc: 0.383673 loss: 0.028137\n",
      "[287/300] 16.10 sec(s) Train Acc: 0.445419 Loss: 0.024984 | Val Acc: 0.381924 loss: 0.028983\n",
      "[288/300] 15.93 sec(s) Train Acc: 0.448155 Loss: 0.025012 | Val Acc: 0.381924 loss: 0.029610\n",
      "[289/300] 16.02 sec(s) Train Acc: 0.442378 Loss: 0.025148 | Val Acc: 0.385714 loss: 0.028096\n",
      "[290/300] 15.91 sec(s) Train Acc: 0.447243 Loss: 0.025122 | Val Acc: 0.386297 loss: 0.029248\n",
      "[291/300] 16.05 sec(s) Train Acc: 0.446382 Loss: 0.025048 | Val Acc: 0.386006 loss: 0.028040\n",
      "[292/300] 15.98 sec(s) Train Acc: 0.448662 Loss: 0.024966 | Val Acc: 0.384548 loss: 0.028563\n",
      "[293/300] 16.02 sec(s) Train Acc: 0.449929 Loss: 0.024904 | Val Acc: 0.378426 loss: 0.028835\n",
      "[294/300] 16.05 sec(s) Train Acc: 0.452818 Loss: 0.025002 | Val Acc: 0.382216 loss: 0.028750\n",
      "[295/300] 16.02 sec(s) Train Acc: 0.442175 Loss: 0.025100 | Val Acc: 0.384257 loss: 0.028128\n",
      "[296/300] 15.97 sec(s) Train Acc: 0.447496 Loss: 0.025156 | Val Acc: 0.387464 loss: 0.028393\n",
      "[297/300] 16.05 sec(s) Train Acc: 0.451906 Loss: 0.024898 | Val Acc: 0.390671 loss: 0.028106\n",
      "[298/300] 16.10 sec(s) Train Acc: 0.443138 Loss: 0.025128 | Val Acc: 0.379009 loss: 0.028232\n",
      "[299/300] 16.04 sec(s) Train Acc: 0.448763 Loss: 0.025095 | Val Acc: 0.382799 loss: 0.028254\n",
      "[300/300] 16.04 sec(s) Train Acc: 0.441871 Loss: 0.025368 | Val Acc: 0.383965 loss: 0.028196\n"
     ]
    }
   ],
   "source": [
    "model = Classifier().cuda()\n",
    "# model = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 300\n",
    "\n",
    "# # use apex to optimize\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "#         train_pred = model(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "#             val_pred = model(data[0].cpu())\n",
    "#             batch_loss = loss(val_pred, data[1].cpu())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-ssSxXlsI_T"
   },
   "source": [
    "得到好的參數後，我們使用 training set 和 validation set 共同訓練（資料量變多，模型效果較好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKoUxLun8lFG",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.89311144813877, 102.3587941606983, 126.59376063616554]\n",
      "[72.80305392379675, 75.35438507973123, 79.31408066842762]\n"
     ]
    }
   ],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ConcatDataset([\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform1),\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform2),\n",
    "])\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print([train_val_x[:,:,:,0].mean(), train_val_x[:,:,:,1].mean(), train_val_x[:,:,:,2].mean()])\n",
    "print([train_val_x[:,:,:,0].std(), train_val_x[:,:,:,1].std(), train_val_x[:,:,:,2].std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoAS5TtRsfOo",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/350] 20.08 sec(s) Train Acc: 0.194081 Loss: 0.034855\n",
      "[002/350] 20.22 sec(s) Train Acc: 0.224992 Loss: 0.033612\n",
      "[003/350] 20.31 sec(s) Train Acc: 0.240147 Loss: 0.033137\n",
      "[004/350] 20.26 sec(s) Train Acc: 0.249624 Loss: 0.032779\n",
      "[005/350] 20.33 sec(s) Train Acc: 0.255641 Loss: 0.032487\n",
      "[006/350] 20.16 sec(s) Train Acc: 0.263425 Loss: 0.032251\n",
      "[007/350] 20.11 sec(s) Train Acc: 0.268539 Loss: 0.032128\n",
      "[008/350] 20.28 sec(s) Train Acc: 0.272977 Loss: 0.031961\n",
      "[009/350] 20.12 sec(s) Train Acc: 0.274819 Loss: 0.031859\n",
      "[010/350] 20.31 sec(s) Train Acc: 0.280498 Loss: 0.031701\n",
      "[011/350] 20.27 sec(s) Train Acc: 0.281476 Loss: 0.031674\n",
      "[012/350] 20.29 sec(s) Train Acc: 0.287981 Loss: 0.031452\n",
      "[013/350] 20.20 sec(s) Train Acc: 0.294713 Loss: 0.031241\n",
      "[014/350] 20.31 sec(s) Train Acc: 0.292419 Loss: 0.031158\n",
      "[015/350] 20.29 sec(s) Train Acc: 0.295578 Loss: 0.031067\n",
      "[016/350] 20.12 sec(s) Train Acc: 0.301068 Loss: 0.031025\n",
      "[017/350] 20.19 sec(s) Train Acc: 0.302422 Loss: 0.030885\n",
      "[018/350] 20.12 sec(s) Train Acc: 0.305017 Loss: 0.030755\n",
      "[019/350] 20.21 sec(s) Train Acc: 0.308551 Loss: 0.030676\n",
      "[020/350] 20.36 sec(s) Train Acc: 0.307235 Loss: 0.030669\n",
      "[021/350] 20.31 sec(s) Train Acc: 0.313628 Loss: 0.030483\n",
      "[022/350] 20.36 sec(s) Train Acc: 0.319833 Loss: 0.030260\n",
      "[023/350] 20.31 sec(s) Train Acc: 0.318404 Loss: 0.030327\n",
      "[024/350] 20.28 sec(s) Train Acc: 0.325286 Loss: 0.030189\n",
      "[025/350] 20.26 sec(s) Train Acc: 0.322691 Loss: 0.030100\n",
      "[026/350] 20.15 sec(s) Train Acc: 0.328257 Loss: 0.030017\n",
      "[027/350] 20.30 sec(s) Train Acc: 0.323706 Loss: 0.029972\n",
      "[028/350] 20.28 sec(s) Train Acc: 0.329460 Loss: 0.029802\n",
      "[029/350] 20.24 sec(s) Train Acc: 0.327617 Loss: 0.029965\n",
      "[030/350] 20.32 sec(s) Train Acc: 0.335063 Loss: 0.029706\n",
      "[031/350] 20.17 sec(s) Train Acc: 0.336906 Loss: 0.029637\n",
      "[032/350] 20.18 sec(s) Train Acc: 0.332581 Loss: 0.029635\n",
      "[033/350] 20.19 sec(s) Train Acc: 0.335289 Loss: 0.029547\n",
      "[034/350] 20.37 sec(s) Train Acc: 0.337432 Loss: 0.029508\n",
      "[035/350] 20.26 sec(s) Train Acc: 0.339313 Loss: 0.029387\n",
      "[036/350] 20.25 sec(s) Train Acc: 0.338786 Loss: 0.029544\n",
      "[037/350] 20.36 sec(s) Train Acc: 0.345329 Loss: 0.029353\n",
      "[038/350] 20.18 sec(s) Train Acc: 0.346646 Loss: 0.029218\n",
      "[039/350] 20.33 sec(s) Train Acc: 0.346646 Loss: 0.029280\n",
      "[040/350] 20.29 sec(s) Train Acc: 0.349692 Loss: 0.029098\n",
      "[041/350] 20.26 sec(s) Train Acc: 0.345292 Loss: 0.029140\n",
      "[042/350] 20.34 sec(s) Train Acc: 0.349729 Loss: 0.029018\n",
      "[043/350] 20.17 sec(s) Train Acc: 0.354731 Loss: 0.028940\n",
      "[044/350] 20.17 sec(s) Train Acc: 0.353678 Loss: 0.028926\n",
      "[045/350] 20.21 sec(s) Train Acc: 0.355445 Loss: 0.028822\n",
      "[046/350] 20.07 sec(s) Train Acc: 0.354768 Loss: 0.028891\n",
      "[047/350] 20.13 sec(s) Train Acc: 0.356009 Loss: 0.028794\n",
      "[048/350] 20.35 sec(s) Train Acc: 0.355746 Loss: 0.028685\n",
      "[049/350] 20.28 sec(s) Train Acc: 0.356423 Loss: 0.028739\n",
      "[050/350] 20.26 sec(s) Train Acc: 0.353452 Loss: 0.028680\n",
      "[051/350] 20.20 sec(s) Train Acc: 0.361387 Loss: 0.028505\n",
      "[052/350] 20.29 sec(s) Train Acc: 0.353791 Loss: 0.028630\n",
      "[053/350] 20.14 sec(s) Train Acc: 0.361124 Loss: 0.028515\n",
      "[054/350] 20.22 sec(s) Train Acc: 0.367629 Loss: 0.028421\n",
      "[055/350] 20.25 sec(s) Train Acc: 0.362590 Loss: 0.028570\n",
      "[056/350] 20.24 sec(s) Train Acc: 0.366012 Loss: 0.028471\n",
      "[057/350] 20.03 sec(s) Train Acc: 0.363681 Loss: 0.028434\n",
      "[058/350] 20.13 sec(s) Train Acc: 0.365110 Loss: 0.028371\n",
      "[059/350] 20.41 sec(s) Train Acc: 0.366727 Loss: 0.028299\n",
      "[060/350] 20.24 sec(s) Train Acc: 0.363869 Loss: 0.028240\n",
      "[061/350] 20.28 sec(s) Train Acc: 0.369698 Loss: 0.028177\n",
      "[062/350] 20.30 sec(s) Train Acc: 0.366877 Loss: 0.028230\n",
      "[063/350] 20.22 sec(s) Train Acc: 0.370563 Loss: 0.028197\n",
      "[064/350] 20.20 sec(s) Train Acc: 0.368607 Loss: 0.028239\n",
      "[065/350] 20.31 sec(s) Train Acc: 0.369322 Loss: 0.028228\n",
      "[066/350] 20.21 sec(s) Train Acc: 0.371879 Loss: 0.028057\n",
      "[067/350] 20.32 sec(s) Train Acc: 0.369923 Loss: 0.028048\n",
      "[068/350] 20.13 sec(s) Train Acc: 0.371766 Loss: 0.028146\n",
      "[069/350] 20.30 sec(s) Train Acc: 0.369585 Loss: 0.028110\n",
      "[070/350] 20.20 sec(s) Train Acc: 0.372969 Loss: 0.027959\n",
      "[071/350] 20.22 sec(s) Train Acc: 0.379137 Loss: 0.027780\n",
      "[072/350] 20.22 sec(s) Train Acc: 0.379663 Loss: 0.027878\n",
      "[073/350] 20.14 sec(s) Train Acc: 0.379024 Loss: 0.027887\n",
      "[074/350] 20.18 sec(s) Train Acc: 0.378196 Loss: 0.027800\n",
      "[075/350] 20.35 sec(s) Train Acc: 0.377332 Loss: 0.027830\n",
      "[076/350] 20.21 sec(s) Train Acc: 0.374812 Loss: 0.027810\n",
      "[077/350] 20.30 sec(s) Train Acc: 0.380829 Loss: 0.027656\n",
      "[078/350] 20.32 sec(s) Train Acc: 0.380566 Loss: 0.027593\n",
      "[079/350] 20.27 sec(s) Train Acc: 0.384176 Loss: 0.027505\n",
      "[080/350] 20.33 sec(s) Train Acc: 0.382220 Loss: 0.027695\n",
      "[081/350] 20.26 sec(s) Train Acc: 0.380415 Loss: 0.027660\n",
      "[082/350] 20.24 sec(s) Train Acc: 0.383649 Loss: 0.027514\n",
      "[083/350] 20.27 sec(s) Train Acc: 0.388801 Loss: 0.027554\n",
      "[084/350] 20.33 sec(s) Train Acc: 0.384740 Loss: 0.027485\n",
      "[085/350] 20.22 sec(s) Train Acc: 0.384627 Loss: 0.027488\n",
      "[086/350] 20.18 sec(s) Train Acc: 0.387372 Loss: 0.027379\n",
      "[087/350] 20.29 sec(s) Train Acc: 0.386206 Loss: 0.027457\n",
      "[088/350] 20.26 sec(s) Train Acc: 0.388500 Loss: 0.027358\n",
      "[089/350] 20.29 sec(s) Train Acc: 0.383950 Loss: 0.027377\n",
      "[090/350] 20.22 sec(s) Train Acc: 0.385793 Loss: 0.027411\n",
      "[091/350] 20.30 sec(s) Train Acc: 0.385003 Loss: 0.027400\n",
      "[092/350] 20.24 sec(s) Train Acc: 0.388839 Loss: 0.027341\n",
      "[093/350] 20.31 sec(s) Train Acc: 0.394592 Loss: 0.027212\n",
      "[094/350] 20.29 sec(s) Train Acc: 0.392411 Loss: 0.027255\n",
      "[095/350] 20.12 sec(s) Train Acc: 0.389929 Loss: 0.027215\n",
      "[096/350] 20.30 sec(s) Train Acc: 0.393314 Loss: 0.027258\n",
      "[097/350] 20.22 sec(s) Train Acc: 0.391283 Loss: 0.027301\n",
      "[098/350] 20.26 sec(s) Train Acc: 0.392449 Loss: 0.027225\n",
      "[099/350] 20.27 sec(s) Train Acc: 0.394141 Loss: 0.027146\n",
      "[100/350] 20.27 sec(s) Train Acc: 0.395984 Loss: 0.027013\n",
      "[101/350] 20.31 sec(s) Train Acc: 0.395532 Loss: 0.027060\n",
      "[102/350] 20.26 sec(s) Train Acc: 0.396322 Loss: 0.027108\n",
      "[103/350] 20.22 sec(s) Train Acc: 0.394592 Loss: 0.027208\n",
      "[104/350] 20.18 sec(s) Train Acc: 0.398315 Loss: 0.027026\n",
      "[105/350] 20.22 sec(s) Train Acc: 0.390907 Loss: 0.027277\n",
      "[106/350] 20.28 sec(s) Train Acc: 0.395570 Loss: 0.027069\n",
      "[107/350] 20.43 sec(s) Train Acc: 0.396661 Loss: 0.027039\n",
      "[108/350] 20.42 sec(s) Train Acc: 0.400196 Loss: 0.026951\n",
      "[109/350] 20.29 sec(s) Train Acc: 0.394893 Loss: 0.027080\n",
      "[110/350] 20.52 sec(s) Train Acc: 0.399255 Loss: 0.026930\n",
      "[111/350] 20.32 sec(s) Train Acc: 0.397150 Loss: 0.027034\n",
      "[112/350] 20.23 sec(s) Train Acc: 0.397037 Loss: 0.026991\n",
      "[113/350] 20.04 sec(s) Train Acc: 0.402414 Loss: 0.026873\n",
      "[114/350] 20.28 sec(s) Train Acc: 0.400647 Loss: 0.026860\n",
      "[115/350] 20.19 sec(s) Train Acc: 0.399293 Loss: 0.026856\n",
      "[116/350] 20.22 sec(s) Train Acc: 0.397826 Loss: 0.026979\n",
      "[117/350] 20.24 sec(s) Train Acc: 0.401474 Loss: 0.026884\n",
      "[118/350] 20.24 sec(s) Train Acc: 0.399180 Loss: 0.026805\n",
      "[119/350] 20.15 sec(s) Train Acc: 0.398353 Loss: 0.027066\n",
      "[120/350] 20.21 sec(s) Train Acc: 0.401587 Loss: 0.026748\n",
      "[121/350] 20.39 sec(s) Train Acc: 0.401286 Loss: 0.026675\n",
      "[122/350] 20.30 sec(s) Train Acc: 0.401549 Loss: 0.026896\n",
      "[123/350] 20.19 sec(s) Train Acc: 0.404595 Loss: 0.026591\n",
      "[124/350] 20.19 sec(s) Train Acc: 0.405648 Loss: 0.026629\n",
      "[125/350] 20.21 sec(s) Train Acc: 0.406701 Loss: 0.026616\n",
      "[126/350] 20.24 sec(s) Train Acc: 0.401963 Loss: 0.026791\n",
      "[127/350] 20.17 sec(s) Train Acc: 0.404671 Loss: 0.026591\n",
      "[128/350] 20.29 sec(s) Train Acc: 0.403467 Loss: 0.026706\n",
      "[129/350] 20.36 sec(s) Train Acc: 0.405535 Loss: 0.026557\n",
      "[130/350] 20.25 sec(s) Train Acc: 0.396811 Loss: 0.026948\n",
      "[131/350] 20.08 sec(s) Train Acc: 0.406288 Loss: 0.026680\n",
      "[132/350] 20.23 sec(s) Train Acc: 0.407604 Loss: 0.026689\n",
      "[133/350] 20.21 sec(s) Train Acc: 0.407416 Loss: 0.026589\n",
      "[134/350] 20.38 sec(s) Train Acc: 0.406212 Loss: 0.026585\n",
      "[135/350] 20.29 sec(s) Train Acc: 0.404595 Loss: 0.026591\n",
      "[136/350] 20.32 sec(s) Train Acc: 0.411063 Loss: 0.026420\n",
      "[137/350] 20.33 sec(s) Train Acc: 0.407077 Loss: 0.026428\n",
      "[138/350] 20.33 sec(s) Train Acc: 0.406062 Loss: 0.026619\n",
      "[139/350] 20.26 sec(s) Train Acc: 0.406363 Loss: 0.026842\n",
      "[140/350] 20.17 sec(s) Train Acc: 0.408318 Loss: 0.026539\n",
      "[141/350] 20.26 sec(s) Train Acc: 0.407341 Loss: 0.026535\n",
      "[142/350] 20.16 sec(s) Train Acc: 0.410988 Loss: 0.026386\n",
      "[143/350] 20.21 sec(s) Train Acc: 0.405724 Loss: 0.026627\n",
      "[144/350] 20.57 sec(s) Train Acc: 0.410387 Loss: 0.026411\n",
      "[145/350] 20.07 sec(s) Train Acc: 0.409258 Loss: 0.026292\n",
      "[146/350] 20.15 sec(s) Train Acc: 0.410913 Loss: 0.026468\n",
      "[147/350] 20.12 sec(s) Train Acc: 0.410161 Loss: 0.026325\n",
      "[148/350] 20.29 sec(s) Train Acc: 0.410349 Loss: 0.026554\n",
      "[149/350] 20.23 sec(s) Train Acc: 0.411289 Loss: 0.026460\n",
      "[150/350] 20.18 sec(s) Train Acc: 0.411289 Loss: 0.026321\n",
      "[151/350] 20.14 sec(s) Train Acc: 0.408882 Loss: 0.026423\n",
      "[152/350] 20.33 sec(s) Train Acc: 0.408995 Loss: 0.026440\n",
      "[153/350] 20.35 sec(s) Train Acc: 0.409785 Loss: 0.026502\n",
      "[154/350] 20.19 sec(s) Train Acc: 0.414373 Loss: 0.026360\n",
      "[155/350] 20.32 sec(s) Train Acc: 0.408619 Loss: 0.026410\n",
      "[156/350] 20.30 sec(s) Train Acc: 0.410725 Loss: 0.026455\n",
      "[157/350] 20.32 sec(s) Train Acc: 0.415463 Loss: 0.026273\n",
      "[158/350] 20.25 sec(s) Train Acc: 0.415426 Loss: 0.026290\n",
      "[159/350] 20.29 sec(s) Train Acc: 0.413245 Loss: 0.026270\n",
      "[160/350] 20.19 sec(s) Train Acc: 0.414937 Loss: 0.026263\n",
      "[161/350] 20.22 sec(s) Train Acc: 0.418284 Loss: 0.026249\n",
      "[162/350] 20.16 sec(s) Train Acc: 0.416855 Loss: 0.026059\n",
      "[163/350] 20.32 sec(s) Train Acc: 0.413809 Loss: 0.026286\n",
      "[164/350] 20.22 sec(s) Train Acc: 0.419299 Loss: 0.026144\n",
      "[165/350] 20.18 sec(s) Train Acc: 0.417306 Loss: 0.026221\n",
      "[166/350] 20.19 sec(s) Train Acc: 0.418885 Loss: 0.026149\n",
      "[167/350] 20.23 sec(s) Train Acc: 0.416516 Loss: 0.026148\n",
      "[168/350] 20.21 sec(s) Train Acc: 0.415012 Loss: 0.026398\n",
      "[169/350] 20.16 sec(s) Train Acc: 0.419186 Loss: 0.026107\n",
      "[170/350] 20.36 sec(s) Train Acc: 0.423360 Loss: 0.026035\n",
      "[171/350] 20.26 sec(s) Train Acc: 0.419073 Loss: 0.026139\n",
      "[172/350] 20.25 sec(s) Train Acc: 0.419261 Loss: 0.026100\n",
      "[173/350] 20.09 sec(s) Train Acc: 0.417607 Loss: 0.026157\n",
      "[174/350] 20.20 sec(s) Train Acc: 0.417983 Loss: 0.026097\n",
      "[175/350] 20.20 sec(s) Train Acc: 0.421819 Loss: 0.026056\n",
      "[176/350] 20.31 sec(s) Train Acc: 0.412906 Loss: 0.026233\n",
      "[177/350] 20.22 sec(s) Train Acc: 0.417456 Loss: 0.026120\n",
      "[178/350] 20.27 sec(s) Train Acc: 0.415012 Loss: 0.026321\n",
      "[179/350] 20.20 sec(s) Train Acc: 0.413997 Loss: 0.026338\n",
      "[180/350] 20.33 sec(s) Train Acc: 0.417494 Loss: 0.026123\n",
      "[181/350] 20.33 sec(s) Train Acc: 0.417156 Loss: 0.026295\n",
      "[182/350] 20.11 sec(s) Train Acc: 0.417607 Loss: 0.026214\n",
      "[183/350] 20.30 sec(s) Train Acc: 0.417118 Loss: 0.026070\n",
      "[184/350] 20.23 sec(s) Train Acc: 0.420916 Loss: 0.025990\n",
      "[185/350] 20.14 sec(s) Train Acc: 0.420051 Loss: 0.026015\n",
      "[186/350] 20.17 sec(s) Train Acc: 0.417644 Loss: 0.025941\n",
      "[187/350] 20.32 sec(s) Train Acc: 0.420540 Loss: 0.025909\n",
      "[188/350] 20.42 sec(s) Train Acc: 0.420314 Loss: 0.026143\n",
      "[189/350] 20.27 sec(s) Train Acc: 0.418096 Loss: 0.026238\n",
      "[190/350] 20.29 sec(s) Train Acc: 0.420126 Loss: 0.026060\n",
      "[191/350] 20.28 sec(s) Train Acc: 0.425316 Loss: 0.025905\n",
      "[192/350] 20.24 sec(s) Train Acc: 0.423172 Loss: 0.025943\n",
      "[193/350] 20.27 sec(s) Train Acc: 0.419637 Loss: 0.025948\n",
      "[194/350] 20.26 sec(s) Train Acc: 0.421706 Loss: 0.025948\n",
      "[195/350] 20.35 sec(s) Train Acc: 0.418998 Loss: 0.025980\n",
      "[196/350] 20.30 sec(s) Train Acc: 0.422157 Loss: 0.025946\n",
      "[197/350] 20.25 sec(s) Train Acc: 0.419261 Loss: 0.026018\n",
      "[198/350] 20.33 sec(s) Train Acc: 0.422007 Loss: 0.025882\n",
      "[199/350] 20.30 sec(s) Train Acc: 0.420051 Loss: 0.026097\n",
      "[200/350] 20.27 sec(s) Train Acc: 0.423022 Loss: 0.025952\n",
      "[201/350] 20.11 sec(s) Train Acc: 0.421969 Loss: 0.025943\n",
      "[202/350] 20.27 sec(s) Train Acc: 0.421819 Loss: 0.025900\n",
      "[203/350] 20.24 sec(s) Train Acc: 0.424338 Loss: 0.025902\n",
      "[204/350] 20.29 sec(s) Train Acc: 0.423172 Loss: 0.025695\n",
      "[205/350] 20.27 sec(s) Train Acc: 0.428249 Loss: 0.025783\n",
      "[206/350] 20.21 sec(s) Train Acc: 0.421179 Loss: 0.026000\n",
      "[207/350] 20.31 sec(s) Train Acc: 0.421029 Loss: 0.025806\n",
      "[208/350] 20.21 sec(s) Train Acc: 0.423548 Loss: 0.025907\n",
      "[209/350] 20.18 sec(s) Train Acc: 0.426030 Loss: 0.025732\n",
      "[210/350] 20.34 sec(s) Train Acc: 0.422571 Loss: 0.025838\n",
      "[211/350] 20.39 sec(s) Train Acc: 0.428362 Loss: 0.025765\n",
      "[212/350] 20.31 sec(s) Train Acc: 0.428700 Loss: 0.025729\n",
      "[213/350] 20.22 sec(s) Train Acc: 0.429490 Loss: 0.025805\n",
      "[214/350] 20.21 sec(s) Train Acc: 0.427497 Loss: 0.025738\n",
      "[215/350] 20.20 sec(s) Train Acc: 0.431746 Loss: 0.025711\n",
      "[216/350] 20.29 sec(s) Train Acc: 0.422834 Loss: 0.025852\n",
      "[217/350] 20.22 sec(s) Train Acc: 0.427347 Loss: 0.025730\n",
      "[218/350] 20.26 sec(s) Train Acc: 0.419562 Loss: 0.026128\n",
      "[219/350] 20.32 sec(s) Train Acc: 0.424752 Loss: 0.025725\n",
      "[220/350] 20.36 sec(s) Train Acc: 0.426895 Loss: 0.025764\n",
      "[221/350] 20.28 sec(s) Train Acc: 0.428888 Loss: 0.025739\n",
      "[222/350] 20.34 sec(s) Train Acc: 0.427309 Loss: 0.025809\n",
      "[223/350] 20.33 sec(s) Train Acc: 0.427647 Loss: 0.025743\n",
      "[224/350] 20.58 sec(s) Train Acc: 0.428136 Loss: 0.025715\n",
      "[225/350] 20.26 sec(s) Train Acc: 0.422533 Loss: 0.025754\n",
      "[226/350] 20.30 sec(s) Train Acc: 0.428851 Loss: 0.025795\n",
      "[227/350] 20.33 sec(s) Train Acc: 0.425090 Loss: 0.025823\n",
      "[228/350] 20.23 sec(s) Train Acc: 0.428023 Loss: 0.025733\n",
      "[229/350] 20.23 sec(s) Train Acc: 0.431709 Loss: 0.025754\n",
      "[230/350] 20.30 sec(s) Train Acc: 0.426106 Loss: 0.025758\n",
      "[231/350] 20.26 sec(s) Train Acc: 0.433514 Loss: 0.025494\n",
      "[232/350] 20.29 sec(s) Train Acc: 0.431370 Loss: 0.025604\n",
      "[233/350] 20.38 sec(s) Train Acc: 0.435281 Loss: 0.025351\n",
      "[234/350] 20.40 sec(s) Train Acc: 0.427685 Loss: 0.025738\n",
      "[235/350] 20.35 sec(s) Train Acc: 0.428174 Loss: 0.025887\n",
      "[236/350] 20.23 sec(s) Train Acc: 0.428174 Loss: 0.025666\n",
      "[237/350] 20.30 sec(s) Train Acc: 0.434153 Loss: 0.025512\n",
      "[238/350] 20.13 sec(s) Train Acc: 0.428700 Loss: 0.025716\n",
      "[239/350] 20.29 sec(s) Train Acc: 0.436071 Loss: 0.025409\n",
      "[240/350] 20.26 sec(s) Train Acc: 0.433927 Loss: 0.025628\n",
      "[241/350] 20.21 sec(s) Train Acc: 0.432235 Loss: 0.025495\n",
      "[242/350] 20.27 sec(s) Train Acc: 0.435883 Loss: 0.025404\n",
      "[243/350] 20.23 sec(s) Train Acc: 0.436522 Loss: 0.025559\n",
      "[244/350] 20.15 sec(s) Train Acc: 0.428174 Loss: 0.025801\n",
      "[245/350] 20.27 sec(s) Train Acc: 0.433627 Loss: 0.025536\n",
      "[246/350] 20.29 sec(s) Train Acc: 0.433589 Loss: 0.025521\n",
      "[247/350] 20.21 sec(s) Train Acc: 0.435845 Loss: 0.025418\n",
      "[248/350] 20.18 sec(s) Train Acc: 0.438666 Loss: 0.025304\n",
      "[249/350] 20.35 sec(s) Train Acc: 0.433852 Loss: 0.025560\n",
      "[250/350] 20.28 sec(s) Train Acc: 0.427986 Loss: 0.025501\n",
      "[251/350] 20.33 sec(s) Train Acc: 0.434341 Loss: 0.025471\n",
      "[252/350] 20.39 sec(s) Train Acc: 0.424865 Loss: 0.025906\n",
      "[253/350] 20.21 sec(s) Train Acc: 0.430806 Loss: 0.025530\n",
      "[254/350] 20.43 sec(s) Train Acc: 0.435206 Loss: 0.025394\n",
      "[255/350] 20.36 sec(s) Train Acc: 0.436447 Loss: 0.025405\n",
      "[256/350] 21.01 sec(s) Train Acc: 0.435281 Loss: 0.025487\n",
      "[257/350] 20.36 sec(s) Train Acc: 0.440433 Loss: 0.025470\n",
      "[258/350] 20.19 sec(s) Train Acc: 0.436597 Loss: 0.025431\n",
      "[259/350] 20.28 sec(s) Train Acc: 0.436447 Loss: 0.025370\n",
      "[260/350] 20.32 sec(s) Train Acc: 0.435996 Loss: 0.025464\n",
      "[261/350] 20.20 sec(s) Train Acc: 0.439568 Loss: 0.025381\n",
      "[262/350] 20.42 sec(s) Train Acc: 0.436861 Loss: 0.025287\n",
      "[263/350] 20.29 sec(s) Train Acc: 0.439681 Loss: 0.025340\n",
      "[264/350] 20.21 sec(s) Train Acc: 0.430092 Loss: 0.025516\n",
      "[265/350] 20.27 sec(s) Train Acc: 0.434830 Loss: 0.025489\n",
      "[266/350] 20.28 sec(s) Train Acc: 0.434830 Loss: 0.025504\n",
      "[267/350] 20.31 sec(s) Train Acc: 0.440057 Loss: 0.025356\n",
      "[268/350] 20.25 sec(s) Train Acc: 0.433326 Loss: 0.025574\n",
      "[269/350] 20.28 sec(s) Train Acc: 0.437838 Loss: 0.025347\n",
      "[270/350] 20.33 sec(s) Train Acc: 0.436936 Loss: 0.025377\n",
      "[271/350] 20.35 sec(s) Train Acc: 0.435168 Loss: 0.025349\n",
      "[272/350] 20.36 sec(s) Train Acc: 0.441035 Loss: 0.025344\n",
      "[273/350] 20.28 sec(s) Train Acc: 0.434792 Loss: 0.025374\n",
      "[274/350] 20.25 sec(s) Train Acc: 0.437914 Loss: 0.025311\n",
      "[275/350] 20.29 sec(s) Train Acc: 0.437011 Loss: 0.025392\n",
      "[276/350] 20.32 sec(s) Train Acc: 0.437387 Loss: 0.025418\n",
      "[277/350] 20.31 sec(s) Train Acc: 0.437011 Loss: 0.025457\n",
      "[278/350] 20.28 sec(s) Train Acc: 0.436597 Loss: 0.025484\n",
      "[279/350] 20.32 sec(s) Train Acc: 0.440809 Loss: 0.025138\n",
      "[280/350] 20.39 sec(s) Train Acc: 0.435845 Loss: 0.025393\n",
      "[281/350] 20.34 sec(s) Train Acc: 0.442050 Loss: 0.025218\n",
      "[282/350] 20.26 sec(s) Train Acc: 0.438215 Loss: 0.025151\n",
      "[283/350] 20.31 sec(s) Train Acc: 0.435056 Loss: 0.025324\n",
      "[284/350] 20.29 sec(s) Train Acc: 0.441411 Loss: 0.025111\n",
      "[285/350] 20.29 sec(s) Train Acc: 0.437951 Loss: 0.025211\n",
      "[286/350] 20.18 sec(s) Train Acc: 0.439644 Loss: 0.025351\n",
      "[287/350] 20.18 sec(s) Train Acc: 0.439531 Loss: 0.025264\n",
      "[288/350] 20.29 sec(s) Train Acc: 0.436823 Loss: 0.025386\n",
      "[289/350] 20.36 sec(s) Train Acc: 0.434717 Loss: 0.025446\n",
      "[290/350] 20.30 sec(s) Train Acc: 0.432687 Loss: 0.025422\n",
      "[291/350] 20.27 sec(s) Train Acc: 0.429114 Loss: 0.025670\n",
      "[292/350] 20.24 sec(s) Train Acc: 0.440997 Loss: 0.025222\n",
      "[293/350] 20.33 sec(s) Train Acc: 0.440847 Loss: 0.025124\n",
      "[294/350] 20.25 sec(s) Train Acc: 0.443178 Loss: 0.025224\n",
      "[295/350] 20.30 sec(s) Train Acc: 0.440884 Loss: 0.025251\n",
      "[296/350] 20.24 sec(s) Train Acc: 0.442276 Loss: 0.025056\n",
      "[297/350] 20.33 sec(s) Train Acc: 0.440696 Loss: 0.025316\n",
      "[298/350] 20.32 sec(s) Train Acc: 0.440320 Loss: 0.025234\n",
      "[299/350] 20.33 sec(s) Train Acc: 0.438967 Loss: 0.025309\n",
      "[300/350] 20.22 sec(s) Train Acc: 0.443366 Loss: 0.025120\n",
      "[301/350] 20.29 sec(s) Train Acc: 0.446074 Loss: 0.025048\n",
      "[302/350] 20.24 sec(s) Train Acc: 0.442840 Loss: 0.025219\n",
      "[303/350] 20.30 sec(s) Train Acc: 0.443968 Loss: 0.025077\n",
      "[304/350] 20.27 sec(s) Train Acc: 0.445698 Loss: 0.025063\n",
      "[305/350] 20.13 sec(s) Train Acc: 0.442088 Loss: 0.025134\n",
      "[306/350] 20.26 sec(s) Train Acc: 0.439305 Loss: 0.025392\n",
      "[307/350] 20.32 sec(s) Train Acc: 0.443066 Loss: 0.025264\n",
      "[308/350] 20.17 sec(s) Train Acc: 0.444758 Loss: 0.025060\n",
      "[309/350] 20.26 sec(s) Train Acc: 0.446638 Loss: 0.025103\n",
      "[310/350] 20.09 sec(s) Train Acc: 0.447578 Loss: 0.024959\n",
      "[311/350] 20.36 sec(s) Train Acc: 0.439493 Loss: 0.025226\n",
      "[312/350] 20.21 sec(s) Train Acc: 0.443554 Loss: 0.025122\n",
      "[313/350] 20.24 sec(s) Train Acc: 0.439907 Loss: 0.025261\n",
      "[314/350] 20.30 sec(s) Train Acc: 0.444946 Loss: 0.025153\n",
      "[315/350] 20.14 sec(s) Train Acc: 0.445585 Loss: 0.025120\n",
      "[316/350] 20.31 sec(s) Train Acc: 0.445961 Loss: 0.025014\n",
      "[317/350] 20.30 sec(s) Train Acc: 0.443404 Loss: 0.025196\n",
      "[318/350] 20.32 sec(s) Train Acc: 0.441110 Loss: 0.025199\n",
      "[319/350] 20.31 sec(s) Train Acc: 0.444983 Loss: 0.025174\n",
      "[320/350] 20.38 sec(s) Train Acc: 0.442313 Loss: 0.025115\n",
      "[321/350] 20.69 sec(s) Train Acc: 0.441749 Loss: 0.025176\n",
      "[322/350] 20.26 sec(s) Train Acc: 0.442313 Loss: 0.025192\n",
      "[323/350] 20.26 sec(s) Train Acc: 0.445736 Loss: 0.024990\n",
      "[324/350] 20.31 sec(s) Train Acc: 0.445021 Loss: 0.025059\n",
      "[325/350] 20.25 sec(s) Train Acc: 0.445698 Loss: 0.025088\n",
      "[326/350] 20.24 sec(s) Train Acc: 0.451715 Loss: 0.024926\n",
      "[327/350] 20.28 sec(s) Train Acc: 0.447052 Loss: 0.025006\n",
      "[328/350] 20.27 sec(s) Train Acc: 0.447879 Loss: 0.024979\n",
      "[329/350] 20.26 sec(s) Train Acc: 0.443554 Loss: 0.025156\n",
      "[330/350] 20.27 sec(s) Train Acc: 0.447653 Loss: 0.024946\n",
      "[331/350] 20.31 sec(s) Train Acc: 0.440170 Loss: 0.025132\n",
      "[332/350] 20.31 sec(s) Train Acc: 0.444457 Loss: 0.025034\n",
      "[333/350] 20.36 sec(s) Train Acc: 0.451828 Loss: 0.024912\n",
      "[334/350] 20.32 sec(s) Train Acc: 0.443893 Loss: 0.025153\n",
      "[335/350] 20.24 sec(s) Train Acc: 0.445510 Loss: 0.025023\n",
      "[336/350] 20.29 sec(s) Train Acc: 0.447503 Loss: 0.024862\n",
      "[337/350] 20.29 sec(s) Train Acc: 0.445134 Loss: 0.024982\n",
      "[338/350] 20.36 sec(s) Train Acc: 0.443630 Loss: 0.025086\n",
      "[339/350] 20.30 sec(s) Train Acc: 0.444720 Loss: 0.024895\n",
      "[340/350] 20.21 sec(s) Train Acc: 0.443216 Loss: 0.025073\n",
      "[341/350] 20.29 sec(s) Train Acc: 0.446224 Loss: 0.024943\n",
      "[342/350] 20.29 sec(s) Train Acc: 0.445773 Loss: 0.024899\n",
      "[343/350] 20.35 sec(s) Train Acc: 0.449007 Loss: 0.024927\n",
      "[344/350] 20.27 sec(s) Train Acc: 0.452053 Loss: 0.024801\n",
      "[345/350] 20.31 sec(s) Train Acc: 0.448142 Loss: 0.024933\n",
      "[346/350] 20.27 sec(s) Train Acc: 0.445961 Loss: 0.024996\n",
      "[347/350] 20.33 sec(s) Train Acc: 0.448857 Loss: 0.024789\n",
      "[348/350] 20.24 sec(s) Train Acc: 0.447992 Loss: 0.024803\n",
      "[349/350] 20.43 sec(s) Train Acc: 0.449120 Loss: 0.024867\n",
      "[350/350] 20.23 sec(s) Train Acc: 0.444871 Loss: 0.024961\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "# model_best = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 350\n",
    "\n",
    "# use apex to optimize\n",
    "# model_best, optimizer = amp.initialize(model_best, optimizer, opt_level=\"O3\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "#         train_pred = model_best(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model_best, 'model_dnn.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2o1oCMXy61_3"
   },
   "source": [
    "# Testing\n",
    "利用剛剛 train 好的 model 進行 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAR6sn8U661G"
   },
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HznI9_-ocrq"
   },
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "#         test_pred = model_best(data.cpu())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t2q2Th85ZUE"
   },
   "outputs": [],
   "source": [
    "#將結果寫入 csv 檔\n",
    "with open(\"predict_dnn.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 釋放記憶體\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
