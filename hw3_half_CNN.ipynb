{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_a2USyd4giE"
   },
   "source": [
    "# **Homework 3 - Convolutional Neural Network**\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhzdomRTOKoJ"
   },
   "outputs": [],
   "source": [
    "# !gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip # 下載資料集\n",
    "# !unzip food-11.zip # 解壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sVrKci4PUFW"
   },
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "# from apex import amp\n",
    "import time\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0i9ZCPrOVN_"
   },
   "source": [
    "#Read image\n",
    "利用 OpenCV (cv2) 讀入照片並存放在 numpy array 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf7QPifJQNUK"
   },
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to 128 x ? or ? x 128\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = 128 / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = 128, 128\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ebVIY5HQQH7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "# 分別將 training set、validation set、testing set 用 readfile 函式讀進來\n",
    "workspace_dir = './food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gq5KVMM3OHY6"
   },
   "source": [
    "# Dataset\n",
    "在 PyTorch 中，我們可以利用 torch.utils.data 的 Dataset 及 DataLoader 來\"包裝\" data，使後續的 training 及 testing 更為方便。\n",
    "\n",
    "Dataset 需要 overload 兩個函數：\\_\\_len\\_\\_ 及 \\_\\_getitem\\_\\_\n",
    "\n",
    "\\_\\_len\\_\\_ 必須要回傳 dataset 的大小，而 \\_\\_getitem\\_\\_ 則定義了當程式利用 [ ] 取值時，dataset 應該要怎麼回傳資料。\n",
    "\n",
    "實際上我們並不會直接使用到這兩個函數，但是使用 DataLoader 在 enumerate Dataset 時會使用到，沒有實做的話會在程式運行階段出現 error。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKd2abixQghI"
   },
   "outputs": [],
   "source": [
    "# training 時做 data augmentation\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective()\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomRotation(40)\n",
    "    ]),\n",
    "    transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomOrder([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective()\n",
    "        ]),\n",
    "        transforms.RandomAffine(30), # 隨機線性轉換\n",
    "        transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0)), # 隨機子圖\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(), # 隨機色溫等\n",
    "        transforms.RandomGrayscale(),\n",
    "    ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.RandomErasing(0.2),\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "# testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qz6jeMnkQl0_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9YhZo7POPYG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1c-GwrMQqMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 128, 128]           3,584\n",
      "       BatchNorm2d-2        [-1, 128, 128, 128]             256\n",
      "              ReLU-3        [-1, 128, 128, 128]               0\n",
      "         MaxPool2d-4          [-1, 128, 64, 64]               0\n",
      "         Dropout2d-5          [-1, 128, 64, 64]               0\n",
      "            Conv2d-6          [-1, 256, 64, 64]         295,168\n",
      "       BatchNorm2d-7          [-1, 256, 64, 64]             512\n",
      "              ReLU-8          [-1, 256, 64, 64]               0\n",
      "         MaxPool2d-9          [-1, 256, 16, 16]               0\n",
      "           Conv2d-10          [-1, 512, 16, 16]       1,180,160\n",
      "      BatchNorm2d-11          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-12          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-13            [-1, 512, 4, 4]               0\n",
      "           Linear-14                  [-1, 800]       6,554,400\n",
      "          Dropout-15                  [-1, 800]               0\n",
      "             ReLU-16                  [-1, 800]               0\n",
      "           Linear-17                  [-1, 400]         320,400\n",
      "            PReLU-18                  [-1, 400]               1\n",
      "           Linear-19                  [-1, 256]         102,656\n",
      "             ReLU-20                  [-1, 256]               0\n",
      "           Linear-21                  [-1, 128]          32,896\n",
      "          Dropout-22                  [-1, 128]               0\n",
      "             ReLU-23                  [-1, 128]               0\n",
      "           Linear-24                   [-1, 50]           6,450\n",
      "             ReLU-25                   [-1, 50]               0\n",
      "           Linear-26                   [-1, 30]           1,530\n",
      "            PReLU-27                   [-1, 30]               1\n",
      "           Linear-28                   [-1, 11]             341\n",
      "================================================================\n",
      "Total params: 8,499,379\n",
      "Trainable params: 8,499,379\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 83.60\n",
      "Params size (MB): 32.42\n",
      "Estimated Total Size (MB): 116.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(3, 128, 5, 1, 3),  # [3, 128, 128]\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(3, 128, 3, 1, 1),  # [3, 128, 128]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 64, 64]\n",
    "            \n",
    "#             nn.Dropout2d(0.5),\n",
    "\n",
    "#             nn.Conv2d(128, 128, 3, 1, 1), # [128, 64, 64]\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "#             nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.PReLU(1),\n",
    "#             nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "            \n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4, 0),       # [256, 16, 16]\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [256, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4, 0),       # [512, 4, 4]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 800),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(800, 400),\n",
    "            nn.PReLU(1),\n",
    "            \n",
    "            nn.Linear(400, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 50),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(50, 30),\n",
    "            nn.PReLU(1),\n",
    "\n",
    "            nn.Linear(30, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)\n",
    "    \n",
    "summary(Classifier().cuda(), (3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEnGbriXORN3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5x-FH2Kr_jh"
   },
   "source": [
    "使用 training set 訓練，並使用 validation set 尋找好的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHaFE-8oQtkC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:440: UserWarning: torch.gels is deprecated in favour of torch.lstsq and will be removed in the next release. Please use torch.lstsq instead.\n",
      "  res = torch.gels(B, A)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] 94.76 sec(s) Train Acc: 0.204845 Loss: 0.034483 | Val Acc: 0.282216 loss: 0.032155\n",
      "[002/300] 94.98 sec(s) Train Acc: 0.270424 Loss: 0.031960 | Val Acc: 0.295627 loss: 0.030098\n",
      "[003/300] 95.02 sec(s) Train Acc: 0.299615 Loss: 0.030790 | Val Acc: 0.283965 loss: 0.030754\n",
      "[004/300] 94.91 sec(s) Train Acc: 0.320799 Loss: 0.029864 | Val Acc: 0.353936 loss: 0.028387\n",
      "[005/300] 94.79 sec(s) Train Acc: 0.346037 Loss: 0.028797 | Val Acc: 0.404665 loss: 0.026205\n",
      "[006/300] 95.06 sec(s) Train Acc: 0.372339 Loss: 0.027919 | Val Acc: 0.447522 loss: 0.025200\n",
      "[007/300] 94.95 sec(s) Train Acc: 0.397476 Loss: 0.027050 | Val Acc: 0.451895 loss: 0.024711\n",
      "[008/300] 95.00 sec(s) Train Acc: 0.417038 Loss: 0.026173 | Val Acc: 0.488338 loss: 0.023269\n",
      "[009/300] 94.80 sec(s) Train Acc: 0.433408 Loss: 0.025566 | Val Acc: 0.502624 loss: 0.023251\n",
      "[010/300] 94.94 sec(s) Train Acc: 0.445723 Loss: 0.025080 | Val Acc: 0.502915 loss: 0.022876\n",
      "[011/300] 94.93 sec(s) Train Acc: 0.466501 Loss: 0.024275 | Val Acc: 0.517493 loss: 0.022568\n",
      "[012/300] 94.90 sec(s) Train Acc: 0.470860 Loss: 0.024023 | Val Acc: 0.520408 loss: 0.022143\n",
      "[013/300] 95.02 sec(s) Train Acc: 0.480387 Loss: 0.023485 | Val Acc: 0.547813 loss: 0.021887\n",
      "[014/300] 94.86 sec(s) Train Acc: 0.496554 Loss: 0.022954 | Val Acc: 0.528571 loss: 0.022332\n",
      "[015/300] 94.98 sec(s) Train Acc: 0.504510 Loss: 0.022776 | Val Acc: 0.566181 loss: 0.020963\n",
      "[016/300] 94.96 sec(s) Train Acc: 0.521741 Loss: 0.022033 | Val Acc: 0.588338 loss: 0.020323\n",
      "[017/300] 94.98 sec(s) Train Acc: 0.525390 Loss: 0.021740 | Val Acc: 0.582799 loss: 0.019772\n",
      "[018/300] 94.92 sec(s) Train Acc: 0.533347 Loss: 0.021297 | Val Acc: 0.599417 loss: 0.019406\n",
      "[019/300] 94.79 sec(s) Train Acc: 0.544344 Loss: 0.021033 | Val Acc: 0.563557 loss: 0.020133\n",
      "[020/300] 94.81 sec(s) Train Acc: 0.555139 Loss: 0.020661 | Val Acc: 0.605539 loss: 0.018730\n",
      "[021/300] 94.86 sec(s) Train Acc: 0.562741 Loss: 0.020242 | Val Acc: 0.608746 loss: 0.019009\n",
      "[022/300] 94.96 sec(s) Train Acc: 0.567454 Loss: 0.020045 | Val Acc: 0.609038 loss: 0.018833\n",
      "[023/300] 94.92 sec(s) Train Acc: 0.573181 Loss: 0.019779 | Val Acc: 0.615160 loss: 0.018194\n",
      "[024/300] 94.99 sec(s) Train Acc: 0.579161 Loss: 0.019493 | Val Acc: 0.613703 loss: 0.018279\n",
      "[025/300] 94.97 sec(s) Train Acc: 0.586965 Loss: 0.019251 | Val Acc: 0.589213 loss: 0.018821\n",
      "[026/300] 94.92 sec(s) Train Acc: 0.592287 Loss: 0.018881 | Val Acc: 0.648980 loss: 0.017016\n",
      "[027/300] 95.08 sec(s) Train Acc: 0.602372 Loss: 0.018608 | Val Acc: 0.634402 loss: 0.017421\n",
      "[028/300] 94.85 sec(s) Train Acc: 0.605210 Loss: 0.018227 | Val Acc: 0.629446 loss: 0.017748\n",
      "[029/300] 94.92 sec(s) Train Acc: 0.609213 Loss: 0.018050 | Val Acc: 0.649563 loss: 0.017274\n",
      "[030/300] 94.81 sec(s) Train Acc: 0.620616 Loss: 0.017797 | Val Acc: 0.645190 loss: 0.016908\n",
      "[031/300] 94.84 sec(s) Train Acc: 0.625329 Loss: 0.017557 | Val Acc: 0.650729 loss: 0.016924\n",
      "[032/300] 94.75 sec(s) Train Acc: 0.622086 Loss: 0.017492 | Val Acc: 0.652187 loss: 0.016818\n",
      "[033/300] 94.89 sec(s) Train Acc: 0.624519 Loss: 0.017362 | Val Acc: 0.650437 loss: 0.016616\n",
      "[034/300] 94.83 sec(s) Train Acc: 0.634046 Loss: 0.017047 | Val Acc: 0.671137 loss: 0.016026\n",
      "[035/300] 94.72 sec(s) Train Acc: 0.640787 Loss: 0.016652 | Val Acc: 0.658017 loss: 0.016409\n",
      "[036/300] 94.90 sec(s) Train Acc: 0.643929 Loss: 0.016680 | Val Acc: 0.656560 loss: 0.016911\n",
      "[037/300] 94.92 sec(s) Train Acc: 0.653152 Loss: 0.016306 | Val Acc: 0.634402 loss: 0.016886\n",
      "[038/300] 94.85 sec(s) Train Acc: 0.654571 Loss: 0.016065 | Val Acc: 0.674927 loss: 0.015644\n",
      "[039/300] 94.86 sec(s) Train Acc: 0.655483 Loss: 0.015915 | Val Acc: 0.675219 loss: 0.015508\n",
      "[040/300] 94.88 sec(s) Train Acc: 0.663085 Loss: 0.015783 | Val Acc: 0.640233 loss: 0.016811\n",
      "[041/300] 94.91 sec(s) Train Acc: 0.673170 Loss: 0.015462 | Val Acc: 0.673761 loss: 0.016086\n",
      "[042/300] 94.82 sec(s) Train Acc: 0.679151 Loss: 0.015171 | Val Acc: 0.659184 loss: 0.016585\n",
      "[043/300] 94.83 sec(s) Train Acc: 0.671599 Loss: 0.015234 | Val Acc: 0.673469 loss: 0.015965\n",
      "[044/300] 94.82 sec(s) Train Acc: 0.680570 Loss: 0.015005 | Val Acc: 0.683673 loss: 0.015203\n",
      "[045/300] 94.81 sec(s) Train Acc: 0.683864 Loss: 0.014810 | Val Acc: 0.688047 loss: 0.015374\n",
      "[046/300] 94.84 sec(s) Train Acc: 0.687259 Loss: 0.014817 | Val Acc: 0.694169 loss: 0.015046\n",
      "[047/300] 94.81 sec(s) Train Acc: 0.692277 Loss: 0.014378 | Val Acc: 0.703207 loss: 0.014846\n",
      "[048/300] 94.93 sec(s) Train Acc: 0.696179 Loss: 0.014393 | Val Acc: 0.670262 loss: 0.015783\n",
      "[049/300] 94.88 sec(s) Train Acc: 0.703122 Loss: 0.013997 | Val Acc: 0.691837 loss: 0.015203\n",
      "[050/300] 94.94 sec(s) Train Acc: 0.707075 Loss: 0.013924 | Val Acc: 0.700000 loss: 0.014512\n",
      "[051/300] 94.75 sec(s) Train Acc: 0.708849 Loss: 0.013751 | Val Acc: 0.708455 loss: 0.014119\n",
      "[052/300] 94.91 sec(s) Train Acc: 0.710673 Loss: 0.013606 | Val Acc: 0.713703 loss: 0.014290\n",
      "[053/300] 95.00 sec(s) Train Acc: 0.720099 Loss: 0.013433 | Val Acc: 0.682216 loss: 0.015132\n",
      "[054/300] 94.92 sec(s) Train Acc: 0.720555 Loss: 0.013293 | Val Acc: 0.717493 loss: 0.014143\n",
      "[055/300] 95.00 sec(s) Train Acc: 0.722380 Loss: 0.013048 | Val Acc: 0.720700 loss: 0.014099\n",
      "[056/300] 95.01 sec(s) Train Acc: 0.723951 Loss: 0.013213 | Val Acc: 0.719534 loss: 0.013609\n",
      "[057/300] 94.91 sec(s) Train Acc: 0.725826 Loss: 0.013039 | Val Acc: 0.702624 loss: 0.014791\n",
      "[058/300] 95.03 sec(s) Train Acc: 0.735303 Loss: 0.012627 | Val Acc: 0.716327 loss: 0.014161\n",
      "[059/300] 94.81 sec(s) Train Acc: 0.736114 Loss: 0.012663 | Val Acc: 0.720408 loss: 0.013949\n",
      "[060/300] 94.93 sec(s) Train Acc: 0.739357 Loss: 0.012413 | Val Acc: 0.726531 loss: 0.013599\n",
      "[061/300] 94.93 sec(s) Train Acc: 0.743108 Loss: 0.012319 | Val Acc: 0.710496 loss: 0.014362\n",
      "[062/300] 95.00 sec(s) Train Acc: 0.748885 Loss: 0.012169 | Val Acc: 0.711370 loss: 0.014011\n",
      "[063/300] 94.98 sec(s) Train Acc: 0.750963 Loss: 0.012142 | Val Acc: 0.731487 loss: 0.013170\n",
      "[064/300] 95.10 sec(s) Train Acc: 0.748429 Loss: 0.012093 | Val Acc: 0.713411 loss: 0.014204\n",
      "[065/300] 94.85 sec(s) Train Acc: 0.752281 Loss: 0.011959 | Val Acc: 0.732070 loss: 0.013229\n",
      "[066/300] 94.87 sec(s) Train Acc: 0.762214 Loss: 0.011686 | Val Acc: 0.734985 loss: 0.013240\n",
      "[067/300] 94.91 sec(s) Train Acc: 0.763177 Loss: 0.011403 | Val Acc: 0.709621 loss: 0.013875\n",
      "[068/300] 94.86 sec(s) Train Acc: 0.765001 Loss: 0.011381 | Val Acc: 0.685423 loss: 0.015160\n",
      "[069/300] 94.95 sec(s) Train Acc: 0.768701 Loss: 0.011296 | Val Acc: 0.743732 loss: 0.012899\n",
      "[070/300] 94.96 sec(s) Train Acc: 0.767383 Loss: 0.011212 | Val Acc: 0.739067 loss: 0.013226\n",
      "[071/300] 94.94 sec(s) Train Acc: 0.771184 Loss: 0.011160 | Val Acc: 0.732362 loss: 0.013217\n",
      "[072/300] 94.89 sec(s) Train Acc: 0.772299 Loss: 0.011072 | Val Acc: 0.738776 loss: 0.013238\n",
      "[073/300] 94.92 sec(s) Train Acc: 0.774326 Loss: 0.010830 | Val Acc: 0.718659 loss: 0.013854\n",
      "[074/300] 94.92 sec(s) Train Acc: 0.779495 Loss: 0.010726 | Val Acc: 0.729738 loss: 0.013388\n",
      "[075/300] 94.92 sec(s) Train Acc: 0.783499 Loss: 0.010594 | Val Acc: 0.747522 loss: 0.012685\n",
      "[076/300] 94.77 sec(s) Train Acc: 0.784360 Loss: 0.010606 | Val Acc: 0.739359 loss: 0.012897\n",
      "[077/300] 94.87 sec(s) Train Acc: 0.785171 Loss: 0.010521 | Val Acc: 0.749854 loss: 0.012691\n",
      "[078/300] 94.86 sec(s) Train Acc: 0.786438 Loss: 0.010416 | Val Acc: 0.720991 loss: 0.013916\n",
      "[079/300] 94.85 sec(s) Train Acc: 0.786844 Loss: 0.010283 | Val Acc: 0.730029 loss: 0.013195\n",
      "[080/300] 94.94 sec(s) Train Acc: 0.785222 Loss: 0.010427 | Val Acc: 0.746647 loss: 0.013018\n",
      "[081/300] 94.97 sec(s) Train Acc: 0.793584 Loss: 0.010105 | Val Acc: 0.726822 loss: 0.013600\n",
      "[082/300] 95.01 sec(s) Train Acc: 0.796169 Loss: 0.009912 | Val Acc: 0.744606 loss: 0.013089\n",
      "[083/300] 94.88 sec(s) Train Acc: 0.801693 Loss: 0.009717 | Val Acc: 0.745773 loss: 0.013012\n",
      "[084/300] 94.95 sec(s) Train Acc: 0.801693 Loss: 0.009763 | Val Acc: 0.750146 loss: 0.012687\n",
      "[085/300] 94.91 sec(s) Train Acc: 0.802504 Loss: 0.009746 | Val Acc: 0.748105 loss: 0.012808\n",
      "[086/300] 94.89 sec(s) Train Acc: 0.802352 Loss: 0.009591 | Val Acc: 0.731487 loss: 0.013568\n",
      "[087/300] 94.86 sec(s) Train Acc: 0.806051 Loss: 0.009499 | Val Acc: 0.747230 loss: 0.012528\n",
      "[088/300] 94.86 sec(s) Train Acc: 0.805848 Loss: 0.009456 | Val Acc: 0.742274 loss: 0.012917\n",
      "[089/300] 94.81 sec(s) Train Acc: 0.809497 Loss: 0.009317 | Val Acc: 0.757143 loss: 0.012449\n",
      "[090/300] 94.85 sec(s) Train Acc: 0.811423 Loss: 0.009241 | Val Acc: 0.735860 loss: 0.012984\n",
      "[091/300] 94.80 sec(s) Train Acc: 0.814210 Loss: 0.009049 | Val Acc: 0.748688 loss: 0.012791\n",
      "[092/300] 94.83 sec(s) Train Acc: 0.814261 Loss: 0.009107 | Val Acc: 0.748688 loss: 0.012581\n",
      "[093/300] 94.87 sec(s) Train Acc: 0.815528 Loss: 0.008916 | Val Acc: 0.739942 loss: 0.012898\n",
      "[094/300] 94.84 sec(s) Train Acc: 0.813197 Loss: 0.009158 | Val Acc: 0.758601 loss: 0.012289\n",
      "[095/300] 94.77 sec(s) Train Acc: 0.815427 Loss: 0.008954 | Val Acc: 0.751895 loss: 0.012583\n",
      "[096/300] 94.84 sec(s) Train Acc: 0.822015 Loss: 0.008760 | Val Acc: 0.750729 loss: 0.012477\n",
      "[097/300] 94.79 sec(s) Train Acc: 0.823991 Loss: 0.008681 | Val Acc: 0.738192 loss: 0.013357\n",
      "[098/300] 94.93 sec(s) Train Acc: 0.825664 Loss: 0.008637 | Val Acc: 0.742857 loss: 0.012968\n",
      "[099/300] 94.82 sec(s) Train Acc: 0.822978 Loss: 0.008659 | Val Acc: 0.764723 loss: 0.011931\n",
      "[100/300] 94.95 sec(s) Train Acc: 0.827944 Loss: 0.008395 | Val Acc: 0.745481 loss: 0.012531\n",
      "[101/300] 94.87 sec(s) Train Acc: 0.831948 Loss: 0.008345 | Val Acc: 0.762099 loss: 0.012399\n",
      "[102/300] 94.95 sec(s) Train Acc: 0.831948 Loss: 0.008432 | Val Acc: 0.757434 loss: 0.012416\n",
      "[103/300] 94.81 sec(s) Train Acc: 0.831847 Loss: 0.008261 | Val Acc: 0.750437 loss: 0.012879\n",
      "[104/300] 94.84 sec(s) Train Acc: 0.831593 Loss: 0.008327 | Val Acc: 0.757143 loss: 0.012287\n",
      "[105/300] 94.86 sec(s) Train Acc: 0.833215 Loss: 0.008193 | Val Acc: 0.765889 loss: 0.011872\n",
      "[106/300] 94.86 sec(s) Train Acc: 0.834127 Loss: 0.008077 | Val Acc: 0.756560 loss: 0.012350\n",
      "[107/300] 94.80 sec(s) Train Acc: 0.837928 Loss: 0.007994 | Val Acc: 0.771720 loss: 0.011489\n",
      "[108/300] 94.82 sec(s) Train Acc: 0.838182 Loss: 0.008025 | Val Acc: 0.762099 loss: 0.011955\n",
      "[109/300] 94.90 sec(s) Train Acc: 0.838790 Loss: 0.007913 | Val Acc: 0.763848 loss: 0.012279\n",
      "[110/300] 94.69 sec(s) Train Acc: 0.839753 Loss: 0.007943 | Val Acc: 0.754810 loss: 0.012077\n",
      "[111/300] 94.84 sec(s) Train Acc: 0.838840 Loss: 0.007870 | Val Acc: 0.761516 loss: 0.011840\n",
      "[112/300] 94.88 sec(s) Train Acc: 0.845885 Loss: 0.007528 | Val Acc: 0.732070 loss: 0.013202\n",
      "[113/300] 94.88 sec(s) Train Acc: 0.842033 Loss: 0.007744 | Val Acc: 0.753644 loss: 0.012371\n",
      "[114/300] 94.87 sec(s) Train Acc: 0.851105 Loss: 0.007508 | Val Acc: 0.759475 loss: 0.012179\n",
      "[115/300] 94.81 sec(s) Train Acc: 0.849889 Loss: 0.007496 | Val Acc: 0.771429 loss: 0.011701\n",
      "[116/300] 94.95 sec(s) Train Acc: 0.846290 Loss: 0.007529 | Val Acc: 0.762391 loss: 0.011841\n",
      "[117/300] 94.98 sec(s) Train Acc: 0.851155 Loss: 0.007432 | Val Acc: 0.755102 loss: 0.012327\n",
      "[118/300] 94.93 sec(s) Train Acc: 0.850547 Loss: 0.007355 | Val Acc: 0.748397 loss: 0.012910\n",
      "[119/300] 94.95 sec(s) Train Acc: 0.854044 Loss: 0.007254 | Val Acc: 0.747230 loss: 0.012849\n",
      "[120/300] 95.01 sec(s) Train Acc: 0.854247 Loss: 0.007247 | Val Acc: 0.764431 loss: 0.011720\n",
      "[121/300] 94.91 sec(s) Train Acc: 0.850345 Loss: 0.007397 | Val Acc: 0.758892 loss: 0.011882\n",
      "[122/300] 94.88 sec(s) Train Acc: 0.854247 Loss: 0.007238 | Val Acc: 0.765889 loss: 0.011749\n",
      "[123/300] 94.86 sec(s) Train Acc: 0.854196 Loss: 0.007188 | Val Acc: 0.765598 loss: 0.012023\n",
      "[124/300] 94.87 sec(s) Train Acc: 0.856325 Loss: 0.007172 | Val Acc: 0.761224 loss: 0.012151\n",
      "[125/300] 94.78 sec(s) Train Acc: 0.859619 Loss: 0.006966 | Val Acc: 0.772886 loss: 0.012249\n",
      "[126/300] 94.84 sec(s) Train Acc: 0.862710 Loss: 0.006904 | Val Acc: 0.772303 loss: 0.011832\n",
      "[127/300] 94.84 sec(s) Train Acc: 0.858200 Loss: 0.007039 | Val Acc: 0.764431 loss: 0.012033\n",
      "[128/300] 94.87 sec(s) Train Acc: 0.863977 Loss: 0.006821 | Val Acc: 0.761808 loss: 0.012394\n",
      "[129/300] 94.84 sec(s) Train Acc: 0.865852 Loss: 0.006717 | Val Acc: 0.769096 loss: 0.011685\n",
      "[130/300] 94.85 sec(s) Train Acc: 0.864940 Loss: 0.006728 | Val Acc: 0.767055 loss: 0.011875\n",
      "[131/300] 94.87 sec(s) Train Acc: 0.861089 Loss: 0.006960 | Val Acc: 0.766472 loss: 0.012230\n",
      "[132/300] 94.92 sec(s) Train Acc: 0.864433 Loss: 0.006782 | Val Acc: 0.774344 loss: 0.011932\n",
      "[133/300] 94.97 sec(s) Train Acc: 0.866461 Loss: 0.006729 | Val Acc: 0.765598 loss: 0.012019\n",
      "[134/300] 94.85 sec(s) Train Acc: 0.865194 Loss: 0.006675 | Val Acc: 0.757143 loss: 0.012516\n",
      "[135/300] 94.92 sec(s) Train Acc: 0.867119 Loss: 0.006547 | Val Acc: 0.767347 loss: 0.012066\n",
      "[136/300] 94.81 sec(s) Train Acc: 0.866815 Loss: 0.006585 | Val Acc: 0.774636 loss: 0.011474\n",
      "[137/300] 94.79 sec(s) Train Acc: 0.870008 Loss: 0.006522 | Val Acc: 0.773178 loss: 0.011477\n",
      "[138/300] 94.80 sec(s) Train Acc: 0.874569 Loss: 0.006326 | Val Acc: 0.775219 loss: 0.011901\n",
      "[139/300] 94.87 sec(s) Train Acc: 0.869501 Loss: 0.006592 | Val Acc: 0.771137 loss: 0.011656\n",
      "[140/300] 94.99 sec(s) Train Acc: 0.874113 Loss: 0.006235 | Val Acc: 0.774927 loss: 0.011707\n",
      "[141/300] 94.94 sec(s) Train Acc: 0.872593 Loss: 0.006397 | Val Acc: 0.790379 loss: 0.011248\n",
      "[142/300] 94.90 sec(s) Train Acc: 0.876900 Loss: 0.006131 | Val Acc: 0.766181 loss: 0.012645\n",
      "[143/300] 94.86 sec(s) Train Acc: 0.875684 Loss: 0.006237 | Val Acc: 0.773469 loss: 0.011860\n",
      "[144/300] 94.95 sec(s) Train Acc: 0.880549 Loss: 0.006036 | Val Acc: 0.773761 loss: 0.012147\n",
      "[145/300] 94.89 sec(s) Train Acc: 0.875076 Loss: 0.006344 | Val Acc: 0.774052 loss: 0.011948\n",
      "[146/300] 94.90 sec(s) Train Acc: 0.880397 Loss: 0.006116 | Val Acc: 0.772595 loss: 0.012142\n",
      "[147/300] 94.89 sec(s) Train Acc: 0.880499 Loss: 0.006041 | Val Acc: 0.774636 loss: 0.011886\n",
      "[148/300] 94.90 sec(s) Train Acc: 0.881208 Loss: 0.006024 | Val Acc: 0.746647 loss: 0.013017\n",
      "[149/300] 94.91 sec(s) Train Acc: 0.880701 Loss: 0.006084 | Val Acc: 0.769096 loss: 0.012005\n",
      "[150/300] 94.92 sec(s) Train Acc: 0.878725 Loss: 0.006022 | Val Acc: 0.769096 loss: 0.012014\n",
      "[151/300] 94.75 sec(s) Train Acc: 0.882323 Loss: 0.005885 | Val Acc: 0.778134 loss: 0.011780\n",
      "[152/300] 94.82 sec(s) Train Acc: 0.884502 Loss: 0.005815 | Val Acc: 0.772886 loss: 0.012887\n",
      "[153/300] 94.85 sec(s) Train Acc: 0.884350 Loss: 0.005861 | Val Acc: 0.772012 loss: 0.012542\n",
      "[154/300] 94.85 sec(s) Train Acc: 0.884097 Loss: 0.005911 | Val Acc: 0.775219 loss: 0.011662\n",
      "[155/300] 94.88 sec(s) Train Acc: 0.884097 Loss: 0.005943 | Val Acc: 0.776968 loss: 0.011650\n",
      "[156/300] 94.81 sec(s) Train Acc: 0.885313 Loss: 0.005844 | Val Acc: 0.771429 loss: 0.012125\n",
      "[157/300] 94.81 sec(s) Train Acc: 0.885871 Loss: 0.005701 | Val Acc: 0.767055 loss: 0.012177\n",
      "[158/300] 94.88 sec(s) Train Acc: 0.887847 Loss: 0.005703 | Val Acc: 0.763848 loss: 0.012732\n",
      "[159/300] 94.82 sec(s) Train Acc: 0.886175 Loss: 0.005811 | Val Acc: 0.770845 loss: 0.012124\n",
      "[160/300] 94.94 sec(s) Train Acc: 0.885263 Loss: 0.005751 | Val Acc: 0.773761 loss: 0.012193\n",
      "[161/300] 94.75 sec(s) Train Acc: 0.889266 Loss: 0.005751 | Val Acc: 0.760933 loss: 0.012145\n",
      "[162/300] 94.85 sec(s) Train Acc: 0.884806 Loss: 0.005724 | Val Acc: 0.752770 loss: 0.013260\n",
      "[163/300] 94.92 sec(s) Train Acc: 0.891749 Loss: 0.005568 | Val Acc: 0.772012 loss: 0.011951\n",
      "[164/300] 94.89 sec(s) Train Acc: 0.892712 Loss: 0.005466 | Val Acc: 0.774927 loss: 0.011753\n",
      "[165/300] 94.77 sec(s) Train Acc: 0.890685 Loss: 0.005468 | Val Acc: 0.764723 loss: 0.012478\n",
      "[166/300] 94.86 sec(s) Train Acc: 0.889418 Loss: 0.005599 | Val Acc: 0.771137 loss: 0.011868\n",
      "[167/300] 94.87 sec(s) Train Acc: 0.892459 Loss: 0.005433 | Val Acc: 0.770845 loss: 0.012478\n",
      "[168/300] 94.86 sec(s) Train Acc: 0.890837 Loss: 0.005408 | Val Acc: 0.773469 loss: 0.011738\n",
      "[169/300] 94.76 sec(s) Train Acc: 0.894283 Loss: 0.005393 | Val Acc: 0.755394 loss: 0.013101\n",
      "[170/300] 94.85 sec(s) Train Acc: 0.895145 Loss: 0.005255 | Val Acc: 0.779300 loss: 0.012223\n",
      "[171/300] 94.86 sec(s) Train Acc: 0.889621 Loss: 0.005508 | Val Acc: 0.768222 loss: 0.012289\n",
      "[172/300] 94.79 sec(s) Train Acc: 0.894892 Loss: 0.005277 | Val Acc: 0.769679 loss: 0.012675\n",
      "[173/300] 94.82 sec(s) Train Acc: 0.895601 Loss: 0.005229 | Val Acc: 0.768222 loss: 0.012942\n",
      "[174/300] 94.88 sec(s) Train Acc: 0.893675 Loss: 0.005292 | Val Acc: 0.776676 loss: 0.012143\n",
      "[175/300] 94.81 sec(s) Train Acc: 0.896412 Loss: 0.005250 | Val Acc: 0.776676 loss: 0.011984\n",
      "[176/300] 94.80 sec(s) Train Acc: 0.894030 Loss: 0.005349 | Val Acc: 0.773761 loss: 0.012093\n",
      "[177/300] 94.80 sec(s) Train Acc: 0.898287 Loss: 0.005227 | Val Acc: 0.760058 loss: 0.012713\n",
      "[178/300] 94.83 sec(s) Train Acc: 0.893067 Loss: 0.005364 | Val Acc: 0.777843 loss: 0.011807\n",
      "[179/300] 94.80 sec(s) Train Acc: 0.899301 Loss: 0.005065 | Val Acc: 0.768513 loss: 0.012695\n",
      "[180/300] 94.81 sec(s) Train Acc: 0.897273 Loss: 0.005195 | Val Acc: 0.777843 loss: 0.012188\n",
      "[181/300] 94.98 sec(s) Train Acc: 0.901632 Loss: 0.004988 | Val Acc: 0.785131 loss: 0.011560\n",
      "[182/300] 94.75 sec(s) Train Acc: 0.896716 Loss: 0.005134 | Val Acc: 0.760058 loss: 0.012808\n",
      "[183/300] 94.87 sec(s) Train Acc: 0.897020 Loss: 0.005223 | Val Acc: 0.754519 loss: 0.013330\n",
      "[184/300] 94.87 sec(s) Train Acc: 0.899199 Loss: 0.005058 | Val Acc: 0.779883 loss: 0.011199\n",
      "[185/300] 94.83 sec(s) Train Acc: 0.899098 Loss: 0.005062 | Val Acc: 0.784548 loss: 0.011311\n",
      "[186/300] 94.84 sec(s) Train Acc: 0.905483 Loss: 0.004783 | Val Acc: 0.788047 loss: 0.011840\n",
      "[187/300] 94.79 sec(s) Train Acc: 0.903811 Loss: 0.004950 | Val Acc: 0.783673 loss: 0.011942\n",
      "[188/300] 94.91 sec(s) Train Acc: 0.901683 Loss: 0.004933 | Val Acc: 0.785423 loss: 0.011689\n",
      "[189/300] 94.88 sec(s) Train Acc: 0.901835 Loss: 0.004901 | Val Acc: 0.777551 loss: 0.012245\n",
      "[190/300] 94.90 sec(s) Train Acc: 0.902139 Loss: 0.004953 | Val Acc: 0.776385 loss: 0.012009\n",
      "[191/300] 94.92 sec(s) Train Acc: 0.904723 Loss: 0.004794 | Val Acc: 0.777259 loss: 0.011886\n",
      "[192/300] 94.86 sec(s) Train Acc: 0.903608 Loss: 0.004934 | Val Acc: 0.774636 loss: 0.012409\n",
      "[193/300] 94.90 sec(s) Train Acc: 0.907004 Loss: 0.004784 | Val Acc: 0.777843 loss: 0.012251\n",
      "[194/300] 94.84 sec(s) Train Acc: 0.905078 Loss: 0.004703 | Val Acc: 0.747230 loss: 0.013797\n",
      "[195/300] 94.88 sec(s) Train Acc: 0.904419 Loss: 0.004904 | Val Acc: 0.779883 loss: 0.012213\n",
      "[196/300] 94.86 sec(s) Train Acc: 0.904267 Loss: 0.004875 | Val Acc: 0.767930 loss: 0.012260\n",
      "[197/300] 94.77 sec(s) Train Acc: 0.902595 Loss: 0.004902 | Val Acc: 0.774636 loss: 0.012168\n",
      "[198/300] 94.85 sec(s) Train Acc: 0.906852 Loss: 0.004748 | Val Acc: 0.770845 loss: 0.012720\n",
      "[199/300] 94.92 sec(s) Train Acc: 0.905585 Loss: 0.004788 | Val Acc: 0.784257 loss: 0.012209\n",
      "[200/300] 94.94 sec(s) Train Acc: 0.905737 Loss: 0.004815 | Val Acc: 0.774052 loss: 0.012222\n",
      "[201/300] 94.92 sec(s) Train Acc: 0.906446 Loss: 0.004710 | Val Acc: 0.771720 loss: 0.012327\n",
      "[202/300] 94.84 sec(s) Train Acc: 0.904166 Loss: 0.004875 | Val Acc: 0.780175 loss: 0.012605\n",
      "[203/300] 94.97 sec(s) Train Acc: 0.908524 Loss: 0.004640 | Val Acc: 0.778134 loss: 0.012287\n",
      "[204/300] 94.92 sec(s) Train Acc: 0.908068 Loss: 0.004691 | Val Acc: 0.772886 loss: 0.012912\n",
      "[205/300] 94.88 sec(s) Train Acc: 0.909082 Loss: 0.004672 | Val Acc: 0.778134 loss: 0.012817\n",
      "[206/300] 94.92 sec(s) Train Acc: 0.908169 Loss: 0.004646 | Val Acc: 0.781924 loss: 0.011692\n",
      "[207/300] 94.96 sec(s) Train Acc: 0.909335 Loss: 0.004586 | Val Acc: 0.774052 loss: 0.012341\n",
      "[208/300] 94.86 sec(s) Train Acc: 0.907257 Loss: 0.004750 | Val Acc: 0.786880 loss: 0.012091\n",
      "[209/300] 94.90 sec(s) Train Acc: 0.909386 Loss: 0.004681 | Val Acc: 0.768805 loss: 0.012952\n",
      "[210/300] 94.80 sec(s) Train Acc: 0.909893 Loss: 0.004666 | Val Acc: 0.771429 loss: 0.012940\n",
      "[211/300] 94.91 sec(s) Train Acc: 0.910197 Loss: 0.004561 | Val Acc: 0.781341 loss: 0.011905\n",
      "[212/300] 94.87 sec(s) Train Acc: 0.910095 Loss: 0.004582 | Val Acc: 0.780175 loss: 0.012539\n",
      "[213/300] 94.93 sec(s) Train Acc: 0.909893 Loss: 0.004562 | Val Acc: 0.774344 loss: 0.012570\n",
      "[214/300] 94.94 sec(s) Train Acc: 0.910045 Loss: 0.004532 | Val Acc: 0.785423 loss: 0.011681\n",
      "[215/300] 94.84 sec(s) Train Acc: 0.909893 Loss: 0.004538 | Val Acc: 0.773178 loss: 0.012706\n",
      "[216/300] 94.84 sec(s) Train Acc: 0.912325 Loss: 0.004502 | Val Acc: 0.779300 loss: 0.012356\n",
      "[217/300] 94.72 sec(s) Train Acc: 0.916329 Loss: 0.004389 | Val Acc: 0.770554 loss: 0.012292\n",
      "[218/300] 94.80 sec(s) Train Acc: 0.908828 Loss: 0.004602 | Val Acc: 0.781633 loss: 0.011837\n",
      "[219/300] 94.91 sec(s) Train Acc: 0.908879 Loss: 0.004535 | Val Acc: 0.778717 loss: 0.012239\n",
      "[220/300] 94.83 sec(s) Train Acc: 0.913491 Loss: 0.004432 | Val Acc: 0.769096 loss: 0.013266\n",
      "[221/300] 94.87 sec(s) Train Acc: 0.913085 Loss: 0.004384 | Val Acc: 0.789213 loss: 0.011651\n",
      "[222/300] 94.95 sec(s) Train Acc: 0.913998 Loss: 0.004241 | Val Acc: 0.769096 loss: 0.013344\n",
      "[223/300] 94.97 sec(s) Train Acc: 0.914859 Loss: 0.004398 | Val Acc: 0.781341 loss: 0.011927\n",
      "[224/300] 94.94 sec(s) Train Acc: 0.913136 Loss: 0.004377 | Val Acc: 0.782799 loss: 0.012501\n",
      "[225/300] 94.82 sec(s) Train Acc: 0.917494 Loss: 0.004199 | Val Acc: 0.769971 loss: 0.012296\n",
      "[226/300] 94.81 sec(s) Train Acc: 0.911109 Loss: 0.004483 | Val Acc: 0.766181 loss: 0.012778\n",
      "[227/300] 94.87 sec(s) Train Acc: 0.915771 Loss: 0.004362 | Val Acc: 0.785131 loss: 0.011783\n",
      "[228/300] 94.87 sec(s) Train Acc: 0.915366 Loss: 0.004312 | Val Acc: 0.786006 loss: 0.011598\n",
      "[229/300] 94.87 sec(s) Train Acc: 0.913491 Loss: 0.004516 | Val Acc: 0.776385 loss: 0.012167\n",
      "[230/300] 94.88 sec(s) Train Acc: 0.917545 Loss: 0.004271 | Val Acc: 0.766181 loss: 0.012303\n",
      "[231/300] 94.82 sec(s) Train Acc: 0.916025 Loss: 0.004353 | Val Acc: 0.776968 loss: 0.012331\n",
      "[232/300] 94.92 sec(s) Train Acc: 0.915315 Loss: 0.004362 | Val Acc: 0.764723 loss: 0.013110\n",
      "[233/300] 94.94 sec(s) Train Acc: 0.916177 Loss: 0.004207 | Val Acc: 0.783673 loss: 0.012082\n",
      "[234/300] 94.89 sec(s) Train Acc: 0.913541 Loss: 0.004419 | Val Acc: 0.778426 loss: 0.013045\n",
      "[235/300] 94.85 sec(s) Train Acc: 0.918761 Loss: 0.004196 | Val Acc: 0.776385 loss: 0.012731\n",
      "[236/300] 94.85 sec(s) Train Acc: 0.914251 Loss: 0.004259 | Val Acc: 0.778426 loss: 0.011945\n",
      "[237/300] 94.85 sec(s) Train Acc: 0.916278 Loss: 0.004214 | Val Acc: 0.793003 loss: 0.011637\n",
      "[238/300] 94.89 sec(s) Train Acc: 0.921853 Loss: 0.004054 | Val Acc: 0.767055 loss: 0.013425\n",
      "[239/300] 94.87 sec(s) Train Acc: 0.916988 Loss: 0.004323 | Val Acc: 0.764140 loss: 0.012955\n",
      "[240/300] 94.88 sec(s) Train Acc: 0.922613 Loss: 0.004032 | Val Acc: 0.774636 loss: 0.012050\n",
      "[241/300] 94.92 sec(s) Train Acc: 0.920231 Loss: 0.004084 | Val Acc: 0.774344 loss: 0.012411\n",
      "[242/300] 94.79 sec(s) Train Acc: 0.921549 Loss: 0.004011 | Val Acc: 0.780758 loss: 0.012785\n",
      "[243/300] 94.99 sec(s) Train Acc: 0.919420 Loss: 0.004087 | Val Acc: 0.781341 loss: 0.012380\n",
      "[244/300] 94.87 sec(s) Train Acc: 0.923069 Loss: 0.004001 | Val Acc: 0.779009 loss: 0.012583\n",
      "[245/300] 94.81 sec(s) Train Acc: 0.921802 Loss: 0.004072 | Val Acc: 0.765598 loss: 0.012477\n",
      "[246/300] 94.85 sec(s) Train Acc: 0.921751 Loss: 0.003959 | Val Acc: 0.779300 loss: 0.012651\n",
      "[247/300] 94.85 sec(s) Train Acc: 0.919775 Loss: 0.003992 | Val Acc: 0.773178 loss: 0.012234\n",
      "[248/300] 94.87 sec(s) Train Acc: 0.922917 Loss: 0.004037 | Val Acc: 0.780466 loss: 0.012073\n",
      "[249/300] 94.75 sec(s) Train Acc: 0.915923 Loss: 0.004150 | Val Acc: 0.778717 loss: 0.012785\n",
      "[250/300] 94.81 sec(s) Train Acc: 0.923627 Loss: 0.003842 | Val Acc: 0.776676 loss: 0.013259\n",
      "[251/300] 94.80 sec(s) Train Acc: 0.917748 Loss: 0.004216 | Val Acc: 0.783090 loss: 0.011919\n",
      "[252/300] 94.80 sec(s) Train Acc: 0.923677 Loss: 0.003992 | Val Acc: 0.787755 loss: 0.011797\n",
      "[253/300] 94.82 sec(s) Train Acc: 0.920789 Loss: 0.004007 | Val Acc: 0.775802 loss: 0.012585\n",
      "[254/300] 94.97 sec(s) Train Acc: 0.918103 Loss: 0.004201 | Val Acc: 0.760933 loss: 0.013181\n",
      "[255/300] 94.87 sec(s) Train Acc: 0.921245 Loss: 0.003974 | Val Acc: 0.774052 loss: 0.012748\n",
      "[256/300] 94.81 sec(s) Train Acc: 0.926819 Loss: 0.003801 | Val Acc: 0.779300 loss: 0.012519\n",
      "[257/300] 94.71 sec(s) Train Acc: 0.923221 Loss: 0.004011 | Val Acc: 0.767638 loss: 0.013049\n",
      "[258/300] 94.76 sec(s) Train Acc: 0.922309 Loss: 0.003950 | Val Acc: 0.781341 loss: 0.012546\n",
      "[259/300] 94.80 sec(s) Train Acc: 0.923221 Loss: 0.003939 | Val Acc: 0.727988 loss: 0.015034\n",
      "[260/300] 94.81 sec(s) Train Acc: 0.923272 Loss: 0.003955 | Val Acc: 0.774344 loss: 0.012672\n",
      "[261/300] 94.87 sec(s) Train Acc: 0.924691 Loss: 0.003859 | Val Acc: 0.769096 loss: 0.012858\n",
      "[262/300] 94.93 sec(s) Train Acc: 0.922512 Loss: 0.003977 | Val Acc: 0.782216 loss: 0.012352\n",
      "[263/300] 94.89 sec(s) Train Acc: 0.925400 Loss: 0.003830 | Val Acc: 0.776093 loss: 0.012371\n",
      "[264/300] 94.87 sec(s) Train Acc: 0.922309 Loss: 0.004037 | Val Acc: 0.774052 loss: 0.012649\n",
      "[265/300] 94.95 sec(s) Train Acc: 0.921802 Loss: 0.004069 | Val Acc: 0.780175 loss: 0.011986\n",
      "[266/300] 94.85 sec(s) Train Acc: 0.923931 Loss: 0.003841 | Val Acc: 0.783382 loss: 0.012240\n",
      "[267/300] 94.87 sec(s) Train Acc: 0.920535 Loss: 0.003991 | Val Acc: 0.769388 loss: 0.013372\n",
      "[268/300] 94.82 sec(s) Train Acc: 0.925198 Loss: 0.003824 | Val Acc: 0.772595 loss: 0.013521\n",
      "[269/300] 94.92 sec(s) Train Acc: 0.923525 Loss: 0.003934 | Val Acc: 0.781341 loss: 0.012853\n",
      "[270/300] 94.93 sec(s) Train Acc: 0.925400 Loss: 0.003815 | Val Acc: 0.783965 loss: 0.012233\n",
      "[271/300] 94.92 sec(s) Train Acc: 0.922410 Loss: 0.003897 | Val Acc: 0.767055 loss: 0.012586\n",
      "[272/300] 94.97 sec(s) Train Acc: 0.928745 Loss: 0.003641 | Val Acc: 0.780466 loss: 0.012481\n",
      "[273/300] 94.84 sec(s) Train Acc: 0.924032 Loss: 0.003838 | Val Acc: 0.788338 loss: 0.011929\n",
      "[274/300] 94.88 sec(s) Train Acc: 0.920890 Loss: 0.004143 | Val Acc: 0.778426 loss: 0.012292\n",
      "[275/300] 94.86 sec(s) Train Acc: 0.928796 Loss: 0.003727 | Val Acc: 0.786297 loss: 0.011886\n",
      "[276/300] 94.80 sec(s) Train Acc: 0.930570 Loss: 0.003678 | Val Acc: 0.776968 loss: 0.012286\n",
      "[277/300] 94.85 sec(s) Train Acc: 0.926718 Loss: 0.003772 | Val Acc: 0.783382 loss: 0.012335\n",
      "[278/300] 94.84 sec(s) Train Acc: 0.929759 Loss: 0.003615 | Val Acc: 0.778134 loss: 0.012867\n",
      "[279/300] 94.83 sec(s) Train Acc: 0.929303 Loss: 0.003663 | Val Acc: 0.791837 loss: 0.012255\n",
      "[280/300] 94.88 sec(s) Train Acc: 0.927225 Loss: 0.003677 | Val Acc: 0.782799 loss: 0.012952\n",
      "[281/300] 94.87 sec(s) Train Acc: 0.926313 Loss: 0.003795 | Val Acc: 0.779883 loss: 0.012683\n",
      "[282/300] 94.78 sec(s) Train Acc: 0.926667 Loss: 0.003699 | Val Acc: 0.781633 loss: 0.012135\n",
      "[283/300] 94.87 sec(s) Train Acc: 0.929860 Loss: 0.003587 | Val Acc: 0.775510 loss: 0.013057\n",
      "[284/300] 94.83 sec(s) Train Acc: 0.928492 Loss: 0.003705 | Val Acc: 0.778717 loss: 0.012344\n",
      "[285/300] 94.87 sec(s) Train Acc: 0.927529 Loss: 0.003748 | Val Acc: 0.765306 loss: 0.013384\n",
      "[286/300] 94.80 sec(s) Train Acc: 0.926262 Loss: 0.003697 | Val Acc: 0.775510 loss: 0.013349\n",
      "[287/300] 94.80 sec(s) Train Acc: 0.926971 Loss: 0.003732 | Val Acc: 0.772012 loss: 0.012414\n",
      "[288/300] 94.81 sec(s) Train Acc: 0.928644 Loss: 0.003662 | Val Acc: 0.781050 loss: 0.012570\n",
      "[289/300] 94.91 sec(s) Train Acc: 0.927884 Loss: 0.003702 | Val Acc: 0.786297 loss: 0.012286\n",
      "[290/300] 94.74 sec(s) Train Acc: 0.928086 Loss: 0.003717 | Val Acc: 0.779883 loss: 0.012546\n",
      "[291/300] 94.88 sec(s) Train Acc: 0.925704 Loss: 0.003697 | Val Acc: 0.784548 loss: 0.012329\n",
      "[292/300] 94.90 sec(s) Train Acc: 0.927580 Loss: 0.003692 | Val Acc: 0.774344 loss: 0.012685\n",
      "[293/300] 94.88 sec(s) Train Acc: 0.926363 Loss: 0.003779 | Val Acc: 0.778426 loss: 0.013125\n",
      "[294/300] 94.87 sec(s) Train Acc: 0.930671 Loss: 0.003569 | Val Acc: 0.783382 loss: 0.012558\n",
      "[295/300] 94.98 sec(s) Train Acc: 0.929252 Loss: 0.003554 | Val Acc: 0.783965 loss: 0.012845\n",
      "[296/300] 94.87 sec(s) Train Acc: 0.927580 Loss: 0.003696 | Val Acc: 0.778134 loss: 0.013565\n",
      "[297/300] 94.87 sec(s) Train Acc: 0.929657 Loss: 0.003542 | Val Acc: 0.775802 loss: 0.012742\n",
      "[298/300] 94.98 sec(s) Train Acc: 0.933306 Loss: 0.003382 | Val Acc: 0.791837 loss: 0.011947\n",
      "[299/300] 94.91 sec(s) Train Acc: 0.933509 Loss: 0.003481 | Val Acc: 0.779300 loss: 0.012978\n",
      "[300/300] 94.98 sec(s) Train Acc: 0.926313 Loss: 0.003657 | Val Acc: 0.784548 loss: 0.012210\n"
     ]
    }
   ],
   "source": [
    "model = Classifier().cuda()\n",
    "# model = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 300\n",
    "\n",
    "# # use apex to optimize\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "#         train_pred = model(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "#             val_pred = model(data[0].cpu())\n",
    "#             batch_loss = loss(val_pred, data[1].cpu())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-ssSxXlsI_T"
   },
   "source": [
    "得到好的參數後，我們使用 training set 和 validation set 共同訓練（資料量變多，模型效果較好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKoUxLun8lFG",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.89311144813877, 102.3587941606983, 126.59376063616554]\n",
      "[72.80305392379675, 75.35438507973123, 79.31408066842762]\n"
     ]
    }
   ],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ConcatDataset([\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform1),\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform2),\n",
    "])\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print([train_val_x[:,:,:,0].mean(), train_val_x[:,:,:,1].mean(), train_val_x[:,:,:,2].mean()])\n",
    "print([train_val_x[:,:,:,0].std(), train_val_x[:,:,:,1].std(), train_val_x[:,:,:,2].std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoAS5TtRsfOo",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] 119.79 sec(s) Train Acc: 0.240862 Loss: 0.032975\n",
      "[002/300] 119.81 sec(s) Train Acc: 0.304227 Loss: 0.030493\n",
      "[003/300] 120.00 sec(s) Train Acc: 0.338560 Loss: 0.028927\n",
      "[004/300] 119.81 sec(s) Train Acc: 0.373270 Loss: 0.027659\n",
      "[005/300] 119.95 sec(s) Train Acc: 0.403730 Loss: 0.026673\n",
      "[006/300] 119.83 sec(s) Train Acc: 0.419224 Loss: 0.026093\n",
      "[007/300] 119.86 sec(s) Train Acc: 0.442502 Loss: 0.025263\n",
      "[008/300] 119.85 sec(s) Train Acc: 0.468938 Loss: 0.024263\n",
      "[009/300] 119.82 sec(s) Train Acc: 0.482137 Loss: 0.023685\n",
      "[010/300] 119.88 sec(s) Train Acc: 0.496691 Loss: 0.023057\n",
      "[011/300] 120.00 sec(s) Train Acc: 0.512222 Loss: 0.022562\n",
      "[012/300] 119.81 sec(s) Train Acc: 0.525083 Loss: 0.021962\n",
      "[013/300] 119.84 sec(s) Train Acc: 0.544186 Loss: 0.021387\n",
      "[014/300] 119.66 sec(s) Train Acc: 0.546443 Loss: 0.020972\n",
      "[015/300] 119.90 sec(s) Train Acc: 0.560845 Loss: 0.020557\n",
      "[016/300] 119.74 sec(s) Train Acc: 0.564004 Loss: 0.020187\n",
      "[017/300] 119.82 sec(s) Train Acc: 0.576752 Loss: 0.019767\n",
      "[018/300] 119.82 sec(s) Train Acc: 0.581002 Loss: 0.019535\n",
      "[019/300] 119.74 sec(s) Train Acc: 0.588523 Loss: 0.019179\n",
      "[020/300] 119.81 sec(s) Train Acc: 0.592697 Loss: 0.018880\n",
      "[021/300] 119.73 sec(s) Train Acc: 0.606310 Loss: 0.018514\n",
      "[022/300] 119.77 sec(s) Train Acc: 0.608792 Loss: 0.018303\n",
      "[023/300] 119.76 sec(s) Train Acc: 0.618758 Loss: 0.017927\n",
      "[024/300] 119.79 sec(s) Train Acc: 0.627219 Loss: 0.017554\n",
      "[025/300] 119.86 sec(s) Train Acc: 0.628196 Loss: 0.017512\n",
      "[026/300] 119.90 sec(s) Train Acc: 0.639741 Loss: 0.017083\n",
      "[027/300] 119.83 sec(s) Train Acc: 0.643539 Loss: 0.016825\n",
      "[028/300] 119.73 sec(s) Train Acc: 0.647902 Loss: 0.016651\n",
      "[029/300] 119.80 sec(s) Train Acc: 0.647450 Loss: 0.016590\n",
      "[030/300] 119.95 sec(s) Train Acc: 0.663508 Loss: 0.016042\n",
      "[031/300] 119.71 sec(s) Train Acc: 0.662793 Loss: 0.015921\n",
      "[032/300] 119.85 sec(s) Train Acc: 0.667005 Loss: 0.015731\n",
      "[033/300] 119.78 sec(s) Train Acc: 0.670803 Loss: 0.015534\n",
      "[034/300] 119.71 sec(s) Train Acc: 0.676406 Loss: 0.015342\n",
      "[035/300] 119.68 sec(s) Train Acc: 0.682611 Loss: 0.015018\n",
      "[036/300] 120.10 sec(s) Train Acc: 0.684040 Loss: 0.014961\n",
      "[037/300] 119.89 sec(s) Train Acc: 0.691674 Loss: 0.014714\n",
      "[038/300] 119.97 sec(s) Train Acc: 0.698481 Loss: 0.014395\n",
      "[039/300] 119.65 sec(s) Train Acc: 0.701903 Loss: 0.014080\n",
      "[040/300] 119.83 sec(s) Train Acc: 0.707619 Loss: 0.014004\n",
      "[041/300] 119.78 sec(s) Train Acc: 0.708221 Loss: 0.013919\n",
      "[042/300] 119.82 sec(s) Train Acc: 0.714313 Loss: 0.013740\n",
      "[043/300] 119.77 sec(s) Train Acc: 0.721232 Loss: 0.013564\n",
      "[044/300] 119.79 sec(s) Train Acc: 0.723112 Loss: 0.013265\n",
      "[045/300] 119.87 sec(s) Train Acc: 0.723714 Loss: 0.013153\n",
      "[046/300] 119.65 sec(s) Train Acc: 0.736500 Loss: 0.012759\n",
      "[047/300] 119.77 sec(s) Train Acc: 0.736613 Loss: 0.012634\n",
      "[048/300] 119.68 sec(s) Train Acc: 0.736763 Loss: 0.012629\n",
      "[049/300] 119.78 sec(s) Train Acc: 0.742291 Loss: 0.012451\n",
      "[050/300] 119.64 sec(s) Train Acc: 0.751767 Loss: 0.012114\n",
      "[051/300] 119.68 sec(s) Train Acc: 0.751655 Loss: 0.011995\n",
      "[052/300] 119.89 sec(s) Train Acc: 0.751279 Loss: 0.012003\n",
      "[053/300] 119.81 sec(s) Train Acc: 0.757521 Loss: 0.011652\n",
      "[054/300] 119.76 sec(s) Train Acc: 0.758499 Loss: 0.011747\n",
      "[055/300] 119.75 sec(s) Train Acc: 0.758950 Loss: 0.011539\n",
      "[056/300] 119.66 sec(s) Train Acc: 0.767486 Loss: 0.011406\n",
      "[057/300] 119.72 sec(s) Train Acc: 0.766057 Loss: 0.011229\n",
      "[058/300] 119.81 sec(s) Train Acc: 0.773202 Loss: 0.011093\n",
      "[059/300] 119.87 sec(s) Train Acc: 0.774707 Loss: 0.010925\n",
      "[060/300] 119.88 sec(s) Train Acc: 0.775346 Loss: 0.010981\n",
      "[061/300] 119.95 sec(s) Train Acc: 0.780009 Loss: 0.010679\n",
      "[062/300] 119.80 sec(s) Train Acc: 0.779708 Loss: 0.010578\n",
      "[063/300] 119.82 sec(s) Train Acc: 0.785875 Loss: 0.010450\n",
      "[064/300] 119.72 sec(s) Train Acc: 0.784823 Loss: 0.010530\n",
      "[065/300] 119.79 sec(s) Train Acc: 0.793096 Loss: 0.010158\n",
      "[066/300] 119.85 sec(s) Train Acc: 0.794111 Loss: 0.010116\n",
      "[067/300] 120.04 sec(s) Train Acc: 0.797684 Loss: 0.009945\n",
      "[068/300] 119.97 sec(s) Train Acc: 0.800090 Loss: 0.009746\n",
      "[069/300] 119.74 sec(s) Train Acc: 0.798436 Loss: 0.009845\n",
      "[070/300] 119.58 sec(s) Train Acc: 0.803400 Loss: 0.009718\n",
      "[071/300] 119.77 sec(s) Train Acc: 0.808627 Loss: 0.009565\n",
      "[072/300] 119.63 sec(s) Train Acc: 0.805881 Loss: 0.009518\n",
      "[073/300] 119.67 sec(s) Train Acc: 0.807386 Loss: 0.009285\n",
      "[074/300] 119.74 sec(s) Train Acc: 0.812462 Loss: 0.009348\n",
      "[075/300] 119.62 sec(s) Train Acc: 0.813403 Loss: 0.009134\n",
      "[076/300] 119.82 sec(s) Train Acc: 0.812500 Loss: 0.009182\n",
      "[077/300] 119.65 sec(s) Train Acc: 0.816637 Loss: 0.008997\n",
      "[078/300] 119.82 sec(s) Train Acc: 0.816674 Loss: 0.008978\n",
      "[079/300] 121.59 sec(s) Train Acc: 0.821713 Loss: 0.008798\n",
      "[080/300] 119.94 sec(s) Train Acc: 0.817765 Loss: 0.008808\n",
      "[081/300] 119.83 sec(s) Train Acc: 0.822578 Loss: 0.008664\n",
      "[082/300] 119.95 sec(s) Train Acc: 0.824647 Loss: 0.008585\n",
      "[083/300] 119.81 sec(s) Train Acc: 0.827542 Loss: 0.008531\n",
      "[084/300] 119.76 sec(s) Train Acc: 0.826828 Loss: 0.008450\n",
      "[085/300] 119.77 sec(s) Train Acc: 0.830513 Loss: 0.008274\n",
      "[086/300] 119.74 sec(s) Train Acc: 0.827016 Loss: 0.008488\n",
      "[087/300] 119.62 sec(s) Train Acc: 0.830400 Loss: 0.008464\n",
      "[088/300] 119.80 sec(s) Train Acc: 0.831716 Loss: 0.008274\n",
      "[089/300] 119.60 sec(s) Train Acc: 0.837771 Loss: 0.008079\n",
      "[090/300] 119.68 sec(s) Train Acc: 0.836530 Loss: 0.008217\n",
      "[091/300] 119.70 sec(s) Train Acc: 0.842171 Loss: 0.007935\n",
      "[092/300] 119.81 sec(s) Train Acc: 0.840478 Loss: 0.007818\n",
      "[093/300] 119.89 sec(s) Train Acc: 0.842659 Loss: 0.007787\n",
      "[094/300] 119.78 sec(s) Train Acc: 0.844916 Loss: 0.007759\n",
      "[095/300] 119.78 sec(s) Train Acc: 0.841118 Loss: 0.007820\n",
      "[096/300] 119.92 sec(s) Train Acc: 0.844502 Loss: 0.007701\n",
      "[097/300] 119.93 sec(s) Train Acc: 0.851158 Loss: 0.007364\n",
      "[098/300] 119.88 sec(s) Train Acc: 0.847435 Loss: 0.007591\n",
      "[099/300] 119.81 sec(s) Train Acc: 0.847059 Loss: 0.007544\n",
      "[100/300] 119.75 sec(s) Train Acc: 0.850406 Loss: 0.007400\n",
      "[101/300] 119.84 sec(s) Train Acc: 0.852587 Loss: 0.007278\n",
      "[102/300] 119.88 sec(s) Train Acc: 0.854054 Loss: 0.007172\n",
      "[103/300] 119.90 sec(s) Train Acc: 0.855408 Loss: 0.007231\n",
      "[104/300] 119.82 sec(s) Train Acc: 0.856122 Loss: 0.007165\n",
      "[105/300] 119.80 sec(s) Train Acc: 0.855934 Loss: 0.007228\n",
      "[106/300] 119.86 sec(s) Train Acc: 0.854204 Loss: 0.007132\n",
      "[107/300] 119.75 sec(s) Train Acc: 0.856385 Loss: 0.007055\n",
      "[108/300] 119.78 sec(s) Train Acc: 0.862741 Loss: 0.006772\n",
      "[109/300] 119.76 sec(s) Train Acc: 0.861537 Loss: 0.006866\n",
      "[110/300] 119.61 sec(s) Train Acc: 0.860409 Loss: 0.007075\n",
      "[111/300] 119.74 sec(s) Train Acc: 0.863342 Loss: 0.006849\n",
      "[112/300] 123.04 sec(s) Train Acc: 0.864207 Loss: 0.006795\n",
      "[113/300] 123.73 sec(s) Train Acc: 0.862064 Loss: 0.006809\n",
      "[114/300] 122.08 sec(s) Train Acc: 0.867554 Loss: 0.006540\n",
      "[115/300] 120.30 sec(s) Train Acc: 0.866464 Loss: 0.006684\n",
      "[116/300] 120.32 sec(s) Train Acc: 0.867479 Loss: 0.006602\n",
      "[117/300] 120.29 sec(s) Train Acc: 0.864621 Loss: 0.006780\n",
      "[118/300] 120.25 sec(s) Train Acc: 0.872142 Loss: 0.006524\n",
      "[119/300] 120.27 sec(s) Train Acc: 0.867291 Loss: 0.006635\n",
      "[120/300] 120.36 sec(s) Train Acc: 0.865411 Loss: 0.006624\n",
      "[121/300] 120.19 sec(s) Train Acc: 0.873458 Loss: 0.006339\n",
      "[122/300] 120.17 sec(s) Train Acc: 0.872706 Loss: 0.006457\n",
      "[123/300] 119.92 sec(s) Train Acc: 0.875301 Loss: 0.006431\n",
      "[124/300] 120.05 sec(s) Train Acc: 0.875338 Loss: 0.006327\n",
      "[125/300] 120.00 sec(s) Train Acc: 0.872932 Loss: 0.006351\n",
      "[126/300] 119.91 sec(s) Train Acc: 0.875376 Loss: 0.006237\n",
      "[127/300] 119.84 sec(s) Train Acc: 0.874737 Loss: 0.006177\n",
      "[128/300] 119.91 sec(s) Train Acc: 0.871540 Loss: 0.006373\n",
      "[129/300] 119.79 sec(s) Train Acc: 0.878309 Loss: 0.006138\n",
      "[130/300] 119.74 sec(s) Train Acc: 0.879738 Loss: 0.006076\n",
      "[131/300] 119.89 sec(s) Train Acc: 0.877520 Loss: 0.006105\n",
      "[132/300] 119.78 sec(s) Train Acc: 0.880641 Loss: 0.006128\n",
      "[133/300] 119.68 sec(s) Train Acc: 0.879249 Loss: 0.006086\n",
      "[134/300] 119.80 sec(s) Train Acc: 0.877820 Loss: 0.006073\n",
      "[135/300] 119.89 sec(s) Train Acc: 0.881242 Loss: 0.005994\n",
      "[136/300] 119.90 sec(s) Train Acc: 0.884777 Loss: 0.005827\n",
      "[137/300] 119.77 sec(s) Train Acc: 0.883348 Loss: 0.005825\n",
      "[138/300] 119.77 sec(s) Train Acc: 0.884326 Loss: 0.005759\n",
      "[139/300] 119.81 sec(s) Train Acc: 0.884665 Loss: 0.005966\n",
      "[140/300] 119.79 sec(s) Train Acc: 0.886582 Loss: 0.005764\n",
      "[141/300] 119.82 sec(s) Train Acc: 0.883574 Loss: 0.005813\n",
      "[142/300] 119.85 sec(s) Train Acc: 0.888199 Loss: 0.005623\n",
      "[143/300] 119.81 sec(s) Train Acc: 0.884589 Loss: 0.005791\n",
      "[144/300] 119.90 sec(s) Train Acc: 0.886018 Loss: 0.005767\n",
      "[145/300] 119.84 sec(s) Train Acc: 0.887447 Loss: 0.005582\n",
      "[146/300] 119.94 sec(s) Train Acc: 0.893201 Loss: 0.005445\n",
      "[147/300] 119.88 sec(s) Train Acc: 0.889704 Loss: 0.005508\n",
      "[148/300] 119.91 sec(s) Train Acc: 0.891283 Loss: 0.005531\n",
      "[149/300] 119.93 sec(s) Train Acc: 0.890869 Loss: 0.005437\n",
      "[150/300] 119.92 sec(s) Train Acc: 0.890230 Loss: 0.005463\n",
      "[151/300] 119.88 sec(s) Train Acc: 0.889967 Loss: 0.005443\n",
      "[152/300] 119.82 sec(s) Train Acc: 0.895194 Loss: 0.005390\n",
      "[153/300] 119.82 sec(s) Train Acc: 0.887485 Loss: 0.005611\n",
      "[154/300] 119.72 sec(s) Train Acc: 0.893953 Loss: 0.005329\n",
      "[155/300] 119.81 sec(s) Train Acc: 0.891546 Loss: 0.005517\n",
      "[156/300] 119.74 sec(s) Train Acc: 0.891057 Loss: 0.005363\n",
      "[157/300] 119.82 sec(s) Train Acc: 0.894893 Loss: 0.005305\n",
      "[158/300] 119.81 sec(s) Train Acc: 0.895796 Loss: 0.005334\n",
      "[159/300] 119.75 sec(s) Train Acc: 0.892787 Loss: 0.005309\n",
      "[160/300] 119.69 sec(s) Train Acc: 0.894329 Loss: 0.005279\n",
      "[161/300] 119.79 sec(s) Train Acc: 0.895683 Loss: 0.005241\n",
      "[162/300] 119.75 sec(s) Train Acc: 0.898390 Loss: 0.005088\n",
      "[163/300] 119.78 sec(s) Train Acc: 0.897375 Loss: 0.005222\n",
      "[164/300] 119.87 sec(s) Train Acc: 0.899481 Loss: 0.005145\n",
      "[165/300] 119.82 sec(s) Train Acc: 0.897638 Loss: 0.005210\n",
      "[166/300] 119.84 sec(s) Train Acc: 0.894893 Loss: 0.005238\n",
      "[167/300] 119.75 sec(s) Train Acc: 0.900308 Loss: 0.005054\n",
      "[168/300] 119.70 sec(s) Train Acc: 0.900008 Loss: 0.005008\n",
      "[169/300] 119.93 sec(s) Train Acc: 0.894254 Loss: 0.005298\n",
      "[170/300] 119.81 sec(s) Train Acc: 0.903843 Loss: 0.004956\n",
      "[171/300] 119.83 sec(s) Train Acc: 0.903618 Loss: 0.004972\n",
      "[172/300] 119.82 sec(s) Train Acc: 0.906212 Loss: 0.004853\n",
      "[173/300] 120.07 sec(s) Train Acc: 0.901775 Loss: 0.004942\n",
      "[174/300] 119.86 sec(s) Train Acc: 0.901700 Loss: 0.004941\n",
      "[175/300] 119.73 sec(s) Train Acc: 0.901361 Loss: 0.005116\n",
      "[176/300] 119.69 sec(s) Train Acc: 0.901888 Loss: 0.004986\n",
      "[177/300] 119.81 sec(s) Train Acc: 0.904483 Loss: 0.004799\n",
      "[178/300] 119.92 sec(s) Train Acc: 0.903279 Loss: 0.004925\n",
      "[179/300] 120.01 sec(s) Train Acc: 0.907265 Loss: 0.004691\n",
      "[180/300] 120.05 sec(s) Train Acc: 0.905761 Loss: 0.004812\n",
      "[181/300] 119.80 sec(s) Train Acc: 0.901850 Loss: 0.004885\n",
      "[182/300] 119.96 sec(s) Train Acc: 0.905347 Loss: 0.004619\n",
      "[183/300] 119.93 sec(s) Train Acc: 0.906100 Loss: 0.004838\n",
      "[184/300] 119.93 sec(s) Train Acc: 0.907792 Loss: 0.004676\n",
      "[185/300] 119.80 sec(s) Train Acc: 0.907604 Loss: 0.004617\n",
      "[186/300] 119.89 sec(s) Train Acc: 0.907378 Loss: 0.004692\n",
      "[187/300] 119.96 sec(s) Train Acc: 0.907228 Loss: 0.004667\n",
      "[188/300] 119.86 sec(s) Train Acc: 0.905836 Loss: 0.004719\n",
      "[189/300] 119.80 sec(s) Train Acc: 0.910537 Loss: 0.004607\n",
      "[190/300] 119.79 sec(s) Train Acc: 0.906024 Loss: 0.004770\n",
      "[191/300] 119.80 sec(s) Train Acc: 0.907077 Loss: 0.004735\n",
      "[192/300] 119.93 sec(s) Train Acc: 0.911364 Loss: 0.004521\n",
      "[193/300] 119.75 sec(s) Train Acc: 0.909559 Loss: 0.004533\n",
      "[194/300] 121.31 sec(s) Train Acc: 0.905611 Loss: 0.004795\n",
      "[195/300] 122.35 sec(s) Train Acc: 0.910349 Loss: 0.004544\n",
      "[196/300] 121.74 sec(s) Train Acc: 0.907378 Loss: 0.004617\n",
      "[197/300] 121.13 sec(s) Train Acc: 0.912041 Loss: 0.004433\n",
      "[198/300] 120.34 sec(s) Train Acc: 0.914335 Loss: 0.004374\n",
      "[199/300] 120.21 sec(s) Train Acc: 0.908506 Loss: 0.004625\n",
      "[200/300] 120.38 sec(s) Train Acc: 0.909183 Loss: 0.004579\n",
      "[201/300] 120.19 sec(s) Train Acc: 0.909108 Loss: 0.004547\n",
      "[202/300] 120.23 sec(s) Train Acc: 0.915651 Loss: 0.004396\n",
      "[203/300] 120.27 sec(s) Train Acc: 0.910575 Loss: 0.004457\n",
      "[204/300] 120.32 sec(s) Train Acc: 0.912906 Loss: 0.004429\n",
      "[205/300] 120.05 sec(s) Train Acc: 0.915614 Loss: 0.004362\n",
      "[206/300] 119.86 sec(s) Train Acc: 0.916779 Loss: 0.004268\n",
      "[207/300] 119.91 sec(s) Train Acc: 0.911703 Loss: 0.004457\n",
      "[208/300] 119.71 sec(s) Train Acc: 0.915990 Loss: 0.004310\n",
      "[209/300] 119.64 sec(s) Train Acc: 0.916667 Loss: 0.004298\n",
      "[210/300] 119.78 sec(s) Train Acc: 0.916554 Loss: 0.004231\n",
      "[211/300] 119.93 sec(s) Train Acc: 0.913169 Loss: 0.004368\n",
      "[212/300] 119.88 sec(s) Train Acc: 0.914786 Loss: 0.004400\n",
      "[213/300] 119.87 sec(s) Train Acc: 0.915200 Loss: 0.004253\n",
      "[214/300] 119.84 sec(s) Train Acc: 0.912342 Loss: 0.004453\n",
      "[215/300] 119.92 sec(s) Train Acc: 0.913245 Loss: 0.004293\n",
      "[216/300] 119.89 sec(s) Train Acc: 0.912718 Loss: 0.004369\n",
      "[217/300] 119.85 sec(s) Train Acc: 0.919713 Loss: 0.004168\n",
      "[218/300] 119.88 sec(s) Train Acc: 0.911740 Loss: 0.004483\n",
      "[219/300] 119.75 sec(s) Train Acc: 0.917419 Loss: 0.004205\n",
      "[220/300] 119.70 sec(s) Train Acc: 0.917268 Loss: 0.004246\n",
      "[221/300] 119.72 sec(s) Train Acc: 0.916291 Loss: 0.004352\n",
      "[222/300] 119.72 sec(s) Train Acc: 0.922082 Loss: 0.003988\n",
      "[223/300] 119.68 sec(s) Train Acc: 0.918773 Loss: 0.004110\n",
      "[224/300] 119.83 sec(s) Train Acc: 0.919073 Loss: 0.004155\n",
      "[225/300] 120.03 sec(s) Train Acc: 0.915539 Loss: 0.004241\n",
      "[226/300] 119.77 sec(s) Train Acc: 0.915952 Loss: 0.004222\n",
      "[227/300] 119.79 sec(s) Train Acc: 0.921217 Loss: 0.003994\n",
      "[228/300] 119.83 sec(s) Train Acc: 0.920540 Loss: 0.003961\n",
      "[229/300] 119.92 sec(s) Train Acc: 0.919261 Loss: 0.004030\n",
      "[230/300] 119.80 sec(s) Train Acc: 0.918133 Loss: 0.004166\n",
      "[231/300] 119.85 sec(s) Train Acc: 0.917795 Loss: 0.004138\n",
      "[232/300] 119.96 sec(s) Train Acc: 0.917983 Loss: 0.004105\n",
      "[233/300] 119.95 sec(s) Train Acc: 0.917532 Loss: 0.004158\n",
      "[234/300] 120.01 sec(s) Train Acc: 0.921894 Loss: 0.003952\n",
      "[235/300] 120.05 sec(s) Train Acc: 0.918020 Loss: 0.004083\n",
      "[236/300] 119.93 sec(s) Train Acc: 0.916291 Loss: 0.004219\n",
      "[237/300] 119.86 sec(s) Train Acc: 0.920502 Loss: 0.004134\n",
      "[238/300] 119.85 sec(s) Train Acc: 0.919299 Loss: 0.004094\n",
      "[239/300] 119.87 sec(s) Train Acc: 0.922345 Loss: 0.003965\n",
      "[240/300] 119.94 sec(s) Train Acc: 0.924977 Loss: 0.003909\n",
      "[241/300] 119.90 sec(s) Train Acc: 0.921781 Loss: 0.004059\n",
      "[242/300] 120.02 sec(s) Train Acc: 0.923586 Loss: 0.003937\n",
      "[243/300] 119.91 sec(s) Train Acc: 0.920239 Loss: 0.004103\n",
      "[244/300] 120.06 sec(s) Train Acc: 0.921819 Loss: 0.003900\n",
      "[245/300] 120.00 sec(s) Train Acc: 0.920653 Loss: 0.004054\n",
      "[246/300] 120.05 sec(s) Train Acc: 0.924902 Loss: 0.003895\n",
      "[247/300] 119.91 sec(s) Train Acc: 0.923323 Loss: 0.003940\n",
      "[248/300] 119.92 sec(s) Train Acc: 0.924301 Loss: 0.003877\n",
      "[249/300] 119.90 sec(s) Train Acc: 0.925504 Loss: 0.003871\n",
      "[250/300] 119.79 sec(s) Train Acc: 0.924902 Loss: 0.003804\n",
      "[251/300] 119.81 sec(s) Train Acc: 0.922383 Loss: 0.003963\n",
      "[252/300] 120.01 sec(s) Train Acc: 0.920502 Loss: 0.003941\n",
      "[253/300] 119.99 sec(s) Train Acc: 0.921217 Loss: 0.004051\n",
      "[254/300] 119.96 sec(s) Train Acc: 0.925391 Loss: 0.003837\n",
      "[255/300] 119.91 sec(s) Train Acc: 0.918848 Loss: 0.004074\n",
      "[256/300] 119.91 sec(s) Train Acc: 0.924037 Loss: 0.003869\n",
      "[257/300] 119.91 sec(s) Train Acc: 0.923436 Loss: 0.003840\n",
      "[258/300] 119.90 sec(s) Train Acc: 0.927121 Loss: 0.003704\n",
      "[259/300] 119.95 sec(s) Train Acc: 0.923548 Loss: 0.003851\n",
      "[260/300] 120.02 sec(s) Train Acc: 0.928136 Loss: 0.003701\n",
      "[261/300] 119.93 sec(s) Train Acc: 0.924564 Loss: 0.003838\n",
      "[262/300] 120.11 sec(s) Train Acc: 0.927459 Loss: 0.003729\n",
      "[263/300] 120.01 sec(s) Train Acc: 0.927046 Loss: 0.003750\n",
      "[264/300] 119.97 sec(s) Train Acc: 0.925278 Loss: 0.003810\n",
      "[265/300] 120.05 sec(s) Train Acc: 0.923360 Loss: 0.003878\n",
      "[266/300] 119.97 sec(s) Train Acc: 0.925391 Loss: 0.003802\n",
      "[267/300] 119.88 sec(s) Train Acc: 0.924413 Loss: 0.003864\n",
      "[268/300] 119.74 sec(s) Train Acc: 0.929152 Loss: 0.003677\n",
      "[269/300] 119.89 sec(s) Train Acc: 0.932574 Loss: 0.003620\n",
      "[270/300] 119.82 sec(s) Train Acc: 0.927572 Loss: 0.003671\n",
      "[271/300] 120.04 sec(s) Train Acc: 0.928851 Loss: 0.003609\n",
      "[272/300] 119.91 sec(s) Train Acc: 0.929227 Loss: 0.003613\n",
      "[273/300] 119.88 sec(s) Train Acc: 0.926971 Loss: 0.003738\n",
      "[274/300] 119.85 sec(s) Train Acc: 0.926482 Loss: 0.003807\n",
      "[275/300] 119.90 sec(s) Train Acc: 0.926482 Loss: 0.003703\n",
      "[276/300] 119.93 sec(s) Train Acc: 0.927873 Loss: 0.003668\n",
      "[277/300] 120.89 sec(s) Train Acc: 0.928475 Loss: 0.003651\n",
      "[278/300] 120.38 sec(s) Train Acc: 0.926971 Loss: 0.003629\n",
      "[279/300] 120.26 sec(s) Train Acc: 0.926406 Loss: 0.003716\n",
      "[280/300] 120.25 sec(s) Train Acc: 0.928136 Loss: 0.003736\n",
      "[281/300] 120.39 sec(s) Train Acc: 0.929152 Loss: 0.003578\n",
      "[282/300] 120.32 sec(s) Train Acc: 0.930919 Loss: 0.003570\n",
      "[283/300] 120.31 sec(s) Train Acc: 0.925278 Loss: 0.003827\n",
      "[284/300] 120.33 sec(s) Train Acc: 0.930280 Loss: 0.003571\n",
      "[285/300] 120.75 sec(s) Train Acc: 0.929904 Loss: 0.003470\n",
      "[286/300] 120.30 sec(s) Train Acc: 0.930242 Loss: 0.003574\n",
      "[287/300] 120.42 sec(s) Train Acc: 0.929076 Loss: 0.003658\n",
      "[288/300] 120.45 sec(s) Train Acc: 0.932574 Loss: 0.003494\n",
      "[289/300] 120.32 sec(s) Train Acc: 0.932198 Loss: 0.003507\n",
      "[290/300] 120.26 sec(s) Train Acc: 0.928362 Loss: 0.003666\n",
      "[291/300] 120.29 sec(s) Train Acc: 0.929528 Loss: 0.003575\n",
      "[292/300] 120.37 sec(s) Train Acc: 0.933288 Loss: 0.003414\n",
      "[293/300] 120.59 sec(s) Train Acc: 0.930054 Loss: 0.003623\n",
      "[294/300] 120.32 sec(s) Train Acc: 0.930806 Loss: 0.003546\n",
      "[295/300] 120.61 sec(s) Train Acc: 0.930881 Loss: 0.003507\n",
      "[296/300] 120.57 sec(s) Train Acc: 0.933777 Loss: 0.003400\n",
      "[297/300] 120.60 sec(s) Train Acc: 0.931709 Loss: 0.003508\n",
      "[298/300] 120.44 sec(s) Train Acc: 0.932273 Loss: 0.003418\n",
      "[299/300] 120.39 sec(s) Train Acc: 0.931634 Loss: 0.003472\n",
      "[300/300] 120.23 sec(s) Train Acc: 0.931822 Loss: 0.003506\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "# model_best = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 300\n",
    "\n",
    "# use apex to optimize\n",
    "# model_best, optimizer = amp.initialize(model_best, optimizer, opt_level=\"O3\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "#         train_pred = model_best(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model_best, 'model_half.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2o1oCMXy61_3"
   },
   "source": [
    "# Testing\n",
    "利用剛剛 train 好的 model 進行 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAR6sn8U661G"
   },
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HznI9_-ocrq"
   },
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "#         test_pred = model_best(data.cpu())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t2q2Th85ZUE"
   },
   "outputs": [],
   "source": [
    "#將結果寫入 csv 檔\n",
    "with open(\"predict_half.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 釋放記憶體\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
