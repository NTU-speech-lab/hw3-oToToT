{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_a2USyd4giE"
   },
   "source": [
    "# **Homework 3 - Convolutional Neural Network**\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhzdomRTOKoJ"
   },
   "outputs": [],
   "source": [
    "# !gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip # 下載資料集\n",
    "# !unzip food-11.zip # 解壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sVrKci4PUFW"
   },
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "# from apex import amp\n",
    "import time\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0i9ZCPrOVN_"
   },
   "source": [
    "#Read image\n",
    "利用 OpenCV (cv2) 讀入照片並存放在 numpy array 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf7QPifJQNUK"
   },
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to 128 x ? or ? x 128\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = 128 / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = 128, 128\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ebVIY5HQQH7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "# 分別將 training set、validation set、testing set 用 readfile 函式讀進來\n",
    "workspace_dir = './food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gq5KVMM3OHY6"
   },
   "source": [
    "# Dataset\n",
    "在 PyTorch 中，我們可以利用 torch.utils.data 的 Dataset 及 DataLoader 來\"包裝\" data，使後續的 training 及 testing 更為方便。\n",
    "\n",
    "Dataset 需要 overload 兩個函數：\\_\\_len\\_\\_ 及 \\_\\_getitem\\_\\_\n",
    "\n",
    "\\_\\_len\\_\\_ 必須要回傳 dataset 的大小，而 \\_\\_getitem\\_\\_ 則定義了當程式利用 [ ] 取值時，dataset 應該要怎麼回傳資料。\n",
    "\n",
    "實際上我們並不會直接使用到這兩個函數，但是使用 DataLoader 在 enumerate Dataset 時會使用到，沒有實做的話會在程式運行階段出現 error。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKd2abixQghI"
   },
   "outputs": [],
   "source": [
    "# training 時做 data augmentation\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective()\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomRotation(40)\n",
    "    ]),\n",
    "    transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomOrder([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective()\n",
    "        ]),\n",
    "        transforms.RandomAffine(30), # 隨機線性轉換\n",
    "        transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0)), # 隨機子圖\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(), # 隨機色溫等\n",
    "        transforms.RandomGrayscale(),\n",
    "    ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.RandomErasing(0.2),\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "# testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "        [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "    )\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qz6jeMnkQl0_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9YhZo7POPYG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1c-GwrMQqMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 128, 128]           3,584\n",
      "       BatchNorm2d-2        [-1, 128, 128, 128]             256\n",
      "              ReLU-3        [-1, 128, 128, 128]               0\n",
      "         MaxPool2d-4          [-1, 128, 64, 64]               0\n",
      "         Dropout2d-5          [-1, 128, 64, 64]               0\n",
      "            Conv2d-6          [-1, 128, 64, 64]         147,584\n",
      "       BatchNorm2d-7          [-1, 128, 64, 64]             256\n",
      "              ReLU-8          [-1, 128, 64, 64]               0\n",
      "         MaxPool2d-9          [-1, 128, 32, 32]               0\n",
      "           Conv2d-10          [-1, 256, 32, 32]         295,168\n",
      "      BatchNorm2d-11          [-1, 256, 32, 32]             512\n",
      "            PReLU-12          [-1, 256, 32, 32]               1\n",
      "        MaxPool2d-13          [-1, 256, 16, 16]               0\n",
      "        Dropout2d-14          [-1, 256, 16, 16]               0\n",
      "           Conv2d-15          [-1, 512, 16, 16]       1,180,160\n",
      "      BatchNorm2d-16          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-17          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-18            [-1, 512, 8, 8]               0\n",
      "           Conv2d-19            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-20            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-21            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-22            [-1, 512, 4, 4]               0\n",
      "           Linear-23                  [-1, 512]       4,194,816\n",
      "             ReLU-24                  [-1, 512]               0\n",
      "           Linear-25                  [-1, 256]         131,328\n",
      "             ReLU-26                  [-1, 256]               0\n",
      "           Linear-27                  [-1, 128]          32,896\n",
      "          Dropout-28                  [-1, 128]               0\n",
      "             ReLU-29                  [-1, 128]               0\n",
      "           Linear-30                  [-1, 100]          12,900\n",
      "             ReLU-31                  [-1, 100]               0\n",
      "           Linear-32                   [-1, 30]           3,030\n",
      "            PReLU-33                   [-1, 30]               1\n",
      "           Linear-34                   [-1, 11]             341\n",
      "================================================================\n",
      "Total params: 8,364,689\n",
      "Trainable params: 8,364,689\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 80.08\n",
      "Params size (MB): 31.91\n",
      "Estimated Total Size (MB): 112.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(3, 128, 5, 1, 3),  # [3, 128, 128]\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(3, 128, 3, 1, 1),  # [64, 128, 128]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "            \n",
    "            nn.Dropout2d(0.5),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(1),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "            \n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 100),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(100, 30),\n",
    "            nn.PReLU(1),\n",
    "\n",
    "            nn.Linear(30, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)\n",
    "    \n",
    "summary(Classifier().cuda(), (3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEnGbriXORN3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5x-FH2Kr_jh"
   },
   "source": [
    "使用 training set 訓練，並使用 validation set 尋找好的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHaFE-8oQtkC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:440: UserWarning: torch.gels is deprecated in favour of torch.lstsq and will be removed in the next release. Please use torch.lstsq instead.\n",
      "  res = torch.gels(B, A)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] 85.94 sec(s) Train Acc: 0.223241 Loss: 0.034019 | Val Acc: 0.279883 loss: 0.031549\n",
      "[002/300] 85.41 sec(s) Train Acc: 0.269005 Loss: 0.032253 | Val Acc: 0.308455 loss: 0.031056\n",
      "[003/300] 85.58 sec(s) Train Acc: 0.289327 Loss: 0.031283 | Val Acc: 0.312536 loss: 0.029988\n",
      "[004/300] 85.58 sec(s) Train Acc: 0.322775 Loss: 0.030057 | Val Acc: 0.387755 loss: 0.027668\n",
      "[005/300] 85.62 sec(s) Train Acc: 0.354956 Loss: 0.028806 | Val Acc: 0.407580 loss: 0.026616\n",
      "[006/300] 85.66 sec(s) Train Acc: 0.380549 Loss: 0.027677 | Val Acc: 0.423324 loss: 0.025960\n",
      "[007/300] 85.63 sec(s) Train Acc: 0.406700 Loss: 0.026774 | Val Acc: 0.469388 loss: 0.024931\n",
      "[008/300] 85.66 sec(s) Train Acc: 0.425147 Loss: 0.025925 | Val Acc: 0.476676 loss: 0.023451\n",
      "[009/300] 85.65 sec(s) Train Acc: 0.440553 Loss: 0.025393 | Val Acc: 0.479883 loss: 0.023823\n",
      "[010/300] 85.67 sec(s) Train Acc: 0.465842 Loss: 0.024475 | Val Acc: 0.519242 loss: 0.021887\n",
      "[011/300] 85.63 sec(s) Train Acc: 0.480032 Loss: 0.023552 | Val Acc: 0.543732 loss: 0.021451\n",
      "[012/300] 85.60 sec(s) Train Acc: 0.496047 Loss: 0.023136 | Val Acc: 0.537318 loss: 0.021064\n",
      "[013/300] 85.63 sec(s) Train Acc: 0.515204 Loss: 0.022333 | Val Acc: 0.560058 loss: 0.020175\n",
      "[014/300] 85.70 sec(s) Train Acc: 0.521032 Loss: 0.021934 | Val Acc: 0.542274 loss: 0.021163\n",
      "[015/300] 85.73 sec(s) Train Acc: 0.538871 Loss: 0.021258 | Val Acc: 0.572012 loss: 0.020034\n",
      "[016/300] 85.58 sec(s) Train Acc: 0.544902 Loss: 0.020850 | Val Acc: 0.584548 loss: 0.019545\n",
      "[017/300] 85.69 sec(s) Train Acc: 0.560815 Loss: 0.020187 | Val Acc: 0.596210 loss: 0.019090\n",
      "[018/300] 85.78 sec(s) Train Acc: 0.575765 Loss: 0.019682 | Val Acc: 0.596793 loss: 0.018983\n",
      "[019/300] 85.67 sec(s) Train Acc: 0.582810 Loss: 0.019304 | Val Acc: 0.611953 loss: 0.018497\n",
      "[020/300] 85.73 sec(s) Train Acc: 0.596138 Loss: 0.018680 | Val Acc: 0.615743 loss: 0.018512\n",
      "[021/300] 85.73 sec(s) Train Acc: 0.602574 Loss: 0.018373 | Val Acc: 0.626531 loss: 0.017402\n",
      "[022/300] 85.72 sec(s) Train Acc: 0.610987 Loss: 0.018028 | Val Acc: 0.624781 loss: 0.018040\n",
      "[023/300] 85.71 sec(s) Train Acc: 0.621681 Loss: 0.017370 | Val Acc: 0.634694 loss: 0.017374\n",
      "[024/300] 85.67 sec(s) Train Acc: 0.633945 Loss: 0.016992 | Val Acc: 0.658601 loss: 0.016387\n",
      "[025/300] 85.70 sec(s) Train Acc: 0.643016 Loss: 0.016666 | Val Acc: 0.644315 loss: 0.017206\n",
      "[026/300] 85.76 sec(s) Train Acc: 0.645449 Loss: 0.016307 | Val Acc: 0.655102 loss: 0.016204\n",
      "[027/300] 85.72 sec(s) Train Acc: 0.658828 Loss: 0.015940 | Val Acc: 0.672303 loss: 0.015931\n",
      "[028/300] 85.69 sec(s) Train Acc: 0.668052 Loss: 0.015512 | Val Acc: 0.658309 loss: 0.016375\n",
      "[029/300] 85.67 sec(s) Train Acc: 0.668407 Loss: 0.015494 | Val Acc: 0.654519 loss: 0.016737\n",
      "[030/300] 85.69 sec(s) Train Acc: 0.679607 Loss: 0.014878 | Val Acc: 0.674344 loss: 0.015785\n",
      "[031/300] 85.67 sec(s) Train Acc: 0.681786 Loss: 0.014717 | Val Acc: 0.691254 loss: 0.015151\n",
      "[032/300] 85.62 sec(s) Train Acc: 0.690097 Loss: 0.014480 | Val Acc: 0.675510 loss: 0.015419\n",
      "[033/300] 85.71 sec(s) Train Acc: 0.701247 Loss: 0.013910 | Val Acc: 0.690087 loss: 0.015197\n",
      "[034/300] 85.62 sec(s) Train Acc: 0.705149 Loss: 0.013751 | Val Acc: 0.693294 loss: 0.014868\n",
      "[035/300] 85.67 sec(s) Train Acc: 0.705250 Loss: 0.013579 | Val Acc: 0.689213 loss: 0.016107\n",
      "[036/300] 85.64 sec(s) Train Acc: 0.712548 Loss: 0.013370 | Val Acc: 0.707580 loss: 0.014495\n",
      "[037/300] 85.65 sec(s) Train Acc: 0.725370 Loss: 0.012870 | Val Acc: 0.686589 loss: 0.016101\n",
      "[038/300] 85.66 sec(s) Train Acc: 0.731553 Loss: 0.012692 | Val Acc: 0.699708 loss: 0.015289\n",
      "[039/300] 85.74 sec(s) Train Acc: 0.731756 Loss: 0.012566 | Val Acc: 0.690962 loss: 0.015398\n",
      "[040/300] 85.72 sec(s) Train Acc: 0.739611 Loss: 0.012105 | Val Acc: 0.705831 loss: 0.015450\n",
      "[041/300] 85.92 sec(s) Train Acc: 0.746098 Loss: 0.011912 | Val Acc: 0.713703 loss: 0.014517\n",
      "[042/300] 85.64 sec(s) Train Acc: 0.751926 Loss: 0.011680 | Val Acc: 0.697376 loss: 0.015638\n",
      "[043/300] 85.72 sec(s) Train Acc: 0.758666 Loss: 0.011440 | Val Acc: 0.697376 loss: 0.015445\n",
      "[044/300] 85.71 sec(s) Train Acc: 0.763835 Loss: 0.011157 | Val Acc: 0.721866 loss: 0.014006\n",
      "[045/300] 85.74 sec(s) Train Acc: 0.769055 Loss: 0.010901 | Val Acc: 0.696793 loss: 0.016028\n",
      "[046/300] 85.76 sec(s) Train Acc: 0.774782 Loss: 0.010588 | Val Acc: 0.700875 loss: 0.015891\n",
      "[047/300] 85.69 sec(s) Train Acc: 0.779749 Loss: 0.010483 | Val Acc: 0.714286 loss: 0.015093\n",
      "[048/300] 85.63 sec(s) Train Acc: 0.781928 Loss: 0.010274 | Val Acc: 0.721574 loss: 0.014936\n",
      "[049/300] 85.64 sec(s) Train Acc: 0.783904 Loss: 0.010020 | Val Acc: 0.716035 loss: 0.015274\n",
      "[050/300] 85.68 sec(s) Train Acc: 0.791912 Loss: 0.009832 | Val Acc: 0.713411 loss: 0.014769\n",
      "[051/300] 85.65 sec(s) Train Acc: 0.798500 Loss: 0.009660 | Val Acc: 0.724198 loss: 0.014530\n",
      "[052/300] 85.69 sec(s) Train Acc: 0.797284 Loss: 0.009491 | Val Acc: 0.724781 loss: 0.014611\n",
      "[053/300] 85.70 sec(s) Train Acc: 0.800223 Loss: 0.009501 | Val Acc: 0.718950 loss: 0.015348\n",
      "[054/300] 85.65 sec(s) Train Acc: 0.807267 Loss: 0.009170 | Val Acc: 0.730904 loss: 0.014977\n",
      "[055/300] 85.65 sec(s) Train Acc: 0.807318 Loss: 0.009097 | Val Acc: 0.734985 loss: 0.014845\n",
      "[056/300] 85.60 sec(s) Train Acc: 0.811220 Loss: 0.008953 | Val Acc: 0.731195 loss: 0.014543\n",
      "[057/300] 85.63 sec(s) Train Acc: 0.817910 Loss: 0.008559 | Val Acc: 0.734402 loss: 0.014846\n",
      "[058/300] 85.67 sec(s) Train Acc: 0.819937 Loss: 0.008513 | Val Acc: 0.726239 loss: 0.014704\n",
      "[059/300] 85.66 sec(s) Train Acc: 0.825664 Loss: 0.008336 | Val Acc: 0.726531 loss: 0.015060\n",
      "[060/300] 85.60 sec(s) Train Acc: 0.831188 Loss: 0.008124 | Val Acc: 0.725364 loss: 0.015456\n",
      "[061/300] 85.58 sec(s) Train Acc: 0.833874 Loss: 0.007858 | Val Acc: 0.705248 loss: 0.016951\n",
      "[062/300] 85.64 sec(s) Train Acc: 0.834127 Loss: 0.007916 | Val Acc: 0.719825 loss: 0.015669\n",
      "[063/300] 85.66 sec(s) Train Acc: 0.838334 Loss: 0.007736 | Val Acc: 0.730321 loss: 0.015583\n",
      "[064/300] 85.68 sec(s) Train Acc: 0.835648 Loss: 0.007823 | Val Acc: 0.731487 loss: 0.014680\n",
      "[065/300] 85.76 sec(s) Train Acc: 0.843604 Loss: 0.007483 | Val Acc: 0.736443 loss: 0.014550\n",
      "[066/300] 85.67 sec(s) Train Acc: 0.848013 Loss: 0.007205 | Val Acc: 0.731487 loss: 0.015825\n",
      "[067/300] 85.67 sec(s) Train Acc: 0.843452 Loss: 0.007378 | Val Acc: 0.739650 loss: 0.014776\n",
      "[068/300] 85.64 sec(s) Train Acc: 0.849990 Loss: 0.007225 | Val Acc: 0.731487 loss: 0.014938\n",
      "[069/300] 85.69 sec(s) Train Acc: 0.856071 Loss: 0.006989 | Val Acc: 0.735860 loss: 0.015050\n",
      "[070/300] 85.69 sec(s) Train Acc: 0.861494 Loss: 0.006804 | Val Acc: 0.725073 loss: 0.015676\n",
      "[071/300] 85.65 sec(s) Train Acc: 0.856071 Loss: 0.006995 | Val Acc: 0.735277 loss: 0.015077\n",
      "[072/300] 85.82 sec(s) Train Acc: 0.860328 Loss: 0.006798 | Val Acc: 0.736735 loss: 0.015567\n",
      "[073/300] 85.69 sec(s) Train Acc: 0.859771 Loss: 0.006650 | Val Acc: 0.732362 loss: 0.015030\n",
      "[074/300] 85.68 sec(s) Train Acc: 0.863521 Loss: 0.006635 | Val Acc: 0.743732 loss: 0.014935\n",
      "[075/300] 85.66 sec(s) Train Acc: 0.865295 Loss: 0.006514 | Val Acc: 0.732653 loss: 0.015391\n",
      "[076/300] 85.72 sec(s) Train Acc: 0.868640 Loss: 0.006400 | Val Acc: 0.728280 loss: 0.016042\n",
      "[077/300] 85.69 sec(s) Train Acc: 0.872846 Loss: 0.006215 | Val Acc: 0.745481 loss: 0.014968\n",
      "[078/300] 85.72 sec(s) Train Acc: 0.869147 Loss: 0.006326 | Val Acc: 0.742274 loss: 0.016145\n",
      "[079/300] 85.68 sec(s) Train Acc: 0.878218 Loss: 0.006017 | Val Acc: 0.746939 loss: 0.015832\n",
      "[080/300] 85.71 sec(s) Train Acc: 0.877306 Loss: 0.006005 | Val Acc: 0.746064 loss: 0.015989\n",
      "[081/300] 85.70 sec(s) Train Acc: 0.875938 Loss: 0.005977 | Val Acc: 0.741691 loss: 0.016049\n",
      "[082/300] 85.60 sec(s) Train Acc: 0.877661 Loss: 0.006007 | Val Acc: 0.741108 loss: 0.015768\n",
      "[083/300] 85.72 sec(s) Train Acc: 0.879434 Loss: 0.005801 | Val Acc: 0.756268 loss: 0.014847\n",
      "[084/300] 85.71 sec(s) Train Acc: 0.883945 Loss: 0.005768 | Val Acc: 0.752770 loss: 0.015268\n",
      "[085/300] 85.69 sec(s) Train Acc: 0.884148 Loss: 0.005678 | Val Acc: 0.755977 loss: 0.014642\n",
      "[086/300] 85.66 sec(s) Train Acc: 0.889976 Loss: 0.005436 | Val Acc: 0.747813 loss: 0.015589\n",
      "[087/300] 85.59 sec(s) Train Acc: 0.887492 Loss: 0.005458 | Val Acc: 0.742274 loss: 0.016218\n",
      "[088/300] 85.68 sec(s) Train Acc: 0.889368 Loss: 0.005363 | Val Acc: 0.749854 loss: 0.015997\n",
      "[089/300] 85.64 sec(s) Train Acc: 0.889013 Loss: 0.005493 | Val Acc: 0.734402 loss: 0.016463\n",
      "[090/300] 85.60 sec(s) Train Acc: 0.891293 Loss: 0.005321 | Val Acc: 0.737901 loss: 0.015954\n",
      "[091/300] 85.70 sec(s) Train Acc: 0.894942 Loss: 0.005244 | Val Acc: 0.738192 loss: 0.016234\n",
      "[092/300] 85.63 sec(s) Train Acc: 0.886377 Loss: 0.005514 | Val Acc: 0.743732 loss: 0.015732\n",
      "[093/300] 85.61 sec(s) Train Acc: 0.896209 Loss: 0.005045 | Val Acc: 0.729446 loss: 0.017472\n",
      "[094/300] 85.63 sec(s) Train Acc: 0.894790 Loss: 0.005086 | Val Acc: 0.760933 loss: 0.015012\n",
      "[095/300] 85.58 sec(s) Train Acc: 0.900314 Loss: 0.004888 | Val Acc: 0.733236 loss: 0.016929\n",
      "[096/300] 85.62 sec(s) Train Acc: 0.898540 Loss: 0.005015 | Val Acc: 0.736152 loss: 0.017717\n",
      "[097/300] 85.69 sec(s) Train Acc: 0.898692 Loss: 0.004930 | Val Acc: 0.748688 loss: 0.016603\n",
      "[098/300] 85.66 sec(s) Train Acc: 0.901480 Loss: 0.004742 | Val Acc: 0.753936 loss: 0.016342\n",
      "[099/300] 85.62 sec(s) Train Acc: 0.899807 Loss: 0.004883 | Val Acc: 0.751895 loss: 0.017314\n",
      "[100/300] 85.57 sec(s) Train Acc: 0.899098 Loss: 0.004817 | Val Acc: 0.735569 loss: 0.017918\n",
      "[101/300] 85.56 sec(s) Train Acc: 0.903912 Loss: 0.004695 | Val Acc: 0.754810 loss: 0.016075\n",
      "[102/300] 85.62 sec(s) Train Acc: 0.904014 Loss: 0.004766 | Val Acc: 0.753061 loss: 0.016089\n",
      "[103/300] 85.54 sec(s) Train Acc: 0.906750 Loss: 0.004537 | Val Acc: 0.747813 loss: 0.016488\n",
      "[104/300] 85.64 sec(s) Train Acc: 0.909183 Loss: 0.004612 | Val Acc: 0.754519 loss: 0.017007\n",
      "[105/300] 85.61 sec(s) Train Acc: 0.907713 Loss: 0.004427 | Val Acc: 0.750146 loss: 0.016264\n",
      "[106/300] 85.69 sec(s) Train Acc: 0.907865 Loss: 0.004485 | Val Acc: 0.753353 loss: 0.017031\n",
      "[107/300] 85.61 sec(s) Train Acc: 0.908828 Loss: 0.004411 | Val Acc: 0.743732 loss: 0.017202\n",
      "[108/300] 85.72 sec(s) Train Acc: 0.905534 Loss: 0.004539 | Val Acc: 0.756560 loss: 0.015727\n",
      "[109/300] 85.66 sec(s) Train Acc: 0.910197 Loss: 0.004409 | Val Acc: 0.759767 loss: 0.015901\n",
      "[110/300] 85.56 sec(s) Train Acc: 0.909132 Loss: 0.004465 | Val Acc: 0.769096 loss: 0.015878\n",
      "[111/300] 85.75 sec(s) Train Acc: 0.911666 Loss: 0.004445 | Val Acc: 0.746356 loss: 0.017102\n",
      "[112/300] 85.67 sec(s) Train Acc: 0.913947 Loss: 0.004273 | Val Acc: 0.763557 loss: 0.015292\n",
      "[113/300] 85.72 sec(s) Train Acc: 0.911261 Loss: 0.004332 | Val Acc: 0.739650 loss: 0.017692\n",
      "[114/300] 85.72 sec(s) Train Acc: 0.913693 Loss: 0.004288 | Val Acc: 0.758309 loss: 0.016734\n",
      "[115/300] 85.64 sec(s) Train Acc: 0.914707 Loss: 0.004151 | Val Acc: 0.749563 loss: 0.017292\n",
      "[116/300] 85.75 sec(s) Train Acc: 0.914758 Loss: 0.004230 | Val Acc: 0.747813 loss: 0.018374\n",
      "[117/300] 85.78 sec(s) Train Acc: 0.912629 Loss: 0.004360 | Val Acc: 0.744606 loss: 0.016016\n",
      "[118/300] 85.76 sec(s) Train Acc: 0.920789 Loss: 0.004014 | Val Acc: 0.751020 loss: 0.016928\n",
      "[119/300] 85.62 sec(s) Train Acc: 0.917596 Loss: 0.004080 | Val Acc: 0.753353 loss: 0.016770\n",
      "[120/300] 85.72 sec(s) Train Acc: 0.919927 Loss: 0.004029 | Val Acc: 0.763265 loss: 0.015875\n",
      "[121/300] 85.73 sec(s) Train Acc: 0.917140 Loss: 0.004092 | Val Acc: 0.763848 loss: 0.015733\n",
      "[122/300] 85.59 sec(s) Train Acc: 0.917798 Loss: 0.004027 | Val Acc: 0.750146 loss: 0.017409\n",
      "[123/300] 85.64 sec(s) Train Acc: 0.920028 Loss: 0.003959 | Val Acc: 0.754810 loss: 0.016937\n",
      "[124/300] 85.61 sec(s) Train Acc: 0.919927 Loss: 0.003978 | Val Acc: 0.757143 loss: 0.016816\n",
      "[125/300] 85.99 sec(s) Train Acc: 0.918913 Loss: 0.003891 | Val Acc: 0.753936 loss: 0.017111\n",
      "[126/300] 85.69 sec(s) Train Acc: 0.919116 Loss: 0.003929 | Val Acc: 0.764431 loss: 0.015751\n",
      "[127/300] 85.61 sec(s) Train Acc: 0.922816 Loss: 0.003909 | Val Acc: 0.756851 loss: 0.016013\n",
      "[128/300] 85.70 sec(s) Train Acc: 0.921295 Loss: 0.003881 | Val Acc: 0.752187 loss: 0.016990\n",
      "[129/300] 85.60 sec(s) Train Acc: 0.920941 Loss: 0.003973 | Val Acc: 0.757434 loss: 0.016704\n",
      "[130/300] 85.60 sec(s) Train Acc: 0.927326 Loss: 0.003650 | Val Acc: 0.753061 loss: 0.016320\n",
      "[131/300] 85.59 sec(s) Train Acc: 0.926414 Loss: 0.003668 | Val Acc: 0.769388 loss: 0.015947\n",
      "[132/300] 85.80 sec(s) Train Acc: 0.926515 Loss: 0.003578 | Val Acc: 0.767055 loss: 0.016429\n",
      "[133/300] 85.55 sec(s) Train Acc: 0.926009 Loss: 0.003606 | Val Acc: 0.763848 loss: 0.016207\n",
      "[134/300] 85.69 sec(s) Train Acc: 0.923627 Loss: 0.003787 | Val Acc: 0.759475 loss: 0.016204\n",
      "[135/300] 85.71 sec(s) Train Acc: 0.922309 Loss: 0.003899 | Val Acc: 0.761808 loss: 0.016210\n",
      "[136/300] 85.57 sec(s) Train Acc: 0.924894 Loss: 0.003718 | Val Acc: 0.755102 loss: 0.016304\n",
      "[137/300] 85.68 sec(s) Train Acc: 0.929303 Loss: 0.003578 | Val Acc: 0.764431 loss: 0.016533\n",
      "[138/300] 85.61 sec(s) Train Acc: 0.927022 Loss: 0.003622 | Val Acc: 0.760058 loss: 0.017768\n",
      "[139/300] 85.64 sec(s) Train Acc: 0.923221 Loss: 0.003687 | Val Acc: 0.761224 loss: 0.015792\n",
      "[140/300] 85.54 sec(s) Train Acc: 0.927275 Loss: 0.003546 | Val Acc: 0.765306 loss: 0.017511\n",
      "[141/300] 85.63 sec(s) Train Acc: 0.928441 Loss: 0.003531 | Val Acc: 0.750437 loss: 0.017350\n",
      "[142/300] 85.60 sec(s) Train Acc: 0.930418 Loss: 0.003513 | Val Acc: 0.769388 loss: 0.016421\n",
      "[143/300] 85.76 sec(s) Train Acc: 0.928644 Loss: 0.003483 | Val Acc: 0.757143 loss: 0.018198\n",
      "[144/300] 85.78 sec(s) Train Acc: 0.931533 Loss: 0.003344 | Val Acc: 0.770845 loss: 0.016436\n",
      "[145/300] 85.64 sec(s) Train Acc: 0.929860 Loss: 0.003547 | Val Acc: 0.772595 loss: 0.016363\n",
      "[146/300] 85.65 sec(s) Train Acc: 0.931735 Loss: 0.003438 | Val Acc: 0.776676 loss: 0.015959\n",
      "[147/300] 85.66 sec(s) Train Acc: 0.930823 Loss: 0.003519 | Val Acc: 0.765306 loss: 0.015575\n",
      "[148/300] 85.57 sec(s) Train Acc: 0.932546 Loss: 0.003355 | Val Acc: 0.773761 loss: 0.016516\n",
      "[149/300] 85.57 sec(s) Train Acc: 0.931786 Loss: 0.003351 | Val Acc: 0.758892 loss: 0.016803\n",
      "[150/300] 85.68 sec(s) Train Acc: 0.931887 Loss: 0.003350 | Val Acc: 0.759767 loss: 0.017316\n",
      "[151/300] 85.86 sec(s) Train Acc: 0.933357 Loss: 0.003322 | Val Acc: 0.761808 loss: 0.015745\n",
      "[152/300] 85.72 sec(s) Train Acc: 0.929252 Loss: 0.003458 | Val Acc: 0.766181 loss: 0.015598\n",
      "[153/300] 85.63 sec(s) Train Acc: 0.933610 Loss: 0.003250 | Val Acc: 0.769388 loss: 0.016559\n",
      "[154/300] 85.70 sec(s) Train Acc: 0.934776 Loss: 0.003197 | Val Acc: 0.763265 loss: 0.016356\n",
      "[155/300] 85.64 sec(s) Train Acc: 0.933053 Loss: 0.003216 | Val Acc: 0.769679 loss: 0.017612\n",
      "[156/300] 85.86 sec(s) Train Acc: 0.932394 Loss: 0.003331 | Val Acc: 0.753644 loss: 0.018178\n",
      "[157/300] 85.63 sec(s) Train Acc: 0.931989 Loss: 0.003382 | Val Acc: 0.767347 loss: 0.016163\n",
      "[158/300] 85.58 sec(s) Train Acc: 0.933002 Loss: 0.003320 | Val Acc: 0.772303 loss: 0.016523\n",
      "[159/300] 85.55 sec(s) Train Acc: 0.936094 Loss: 0.003113 | Val Acc: 0.769388 loss: 0.016645\n",
      "[160/300] 85.64 sec(s) Train Acc: 0.934472 Loss: 0.003246 | Val Acc: 0.771137 loss: 0.016835\n",
      "[161/300] 85.62 sec(s) Train Acc: 0.935536 Loss: 0.003348 | Val Acc: 0.768222 loss: 0.015595\n",
      "[162/300] 85.64 sec(s) Train Acc: 0.936600 Loss: 0.003071 | Val Acc: 0.762099 loss: 0.017097\n",
      "[163/300] 85.60 sec(s) Train Acc: 0.936803 Loss: 0.003166 | Val Acc: 0.766472 loss: 0.016973\n",
      "[164/300] 85.68 sec(s) Train Acc: 0.936600 Loss: 0.003208 | Val Acc: 0.767930 loss: 0.017221\n",
      "[165/300] 85.63 sec(s) Train Acc: 0.938019 Loss: 0.003075 | Val Acc: 0.765889 loss: 0.017343\n",
      "[166/300] 85.59 sec(s) Train Acc: 0.937411 Loss: 0.003087 | Val Acc: 0.773178 loss: 0.016988\n",
      "[167/300] 85.61 sec(s) Train Acc: 0.940249 Loss: 0.003060 | Val Acc: 0.772886 loss: 0.015849\n",
      "[168/300] 85.65 sec(s) Train Acc: 0.940047 Loss: 0.002961 | Val Acc: 0.758892 loss: 0.018858\n",
      "[169/300] 85.61 sec(s) Train Acc: 0.938932 Loss: 0.003024 | Val Acc: 0.767347 loss: 0.017793\n",
      "[170/300] 85.66 sec(s) Train Acc: 0.938830 Loss: 0.003053 | Val Acc: 0.764723 loss: 0.016102\n",
      "[171/300] 85.68 sec(s) Train Acc: 0.937361 Loss: 0.003084 | Val Acc: 0.765889 loss: 0.017240\n",
      "[172/300] 85.76 sec(s) Train Acc: 0.940756 Loss: 0.002852 | Val Acc: 0.764431 loss: 0.017872\n",
      "[173/300] 85.71 sec(s) Train Acc: 0.937411 Loss: 0.003154 | Val Acc: 0.761808 loss: 0.017790\n",
      "[174/300] 85.63 sec(s) Train Acc: 0.936550 Loss: 0.003082 | Val Acc: 0.773178 loss: 0.016259\n",
      "[175/300] 85.62 sec(s) Train Acc: 0.939084 Loss: 0.003043 | Val Acc: 0.753353 loss: 0.017990\n",
      "[176/300] 85.56 sec(s) Train Acc: 0.937057 Loss: 0.003141 | Val Acc: 0.758601 loss: 0.018090\n",
      "[177/300] 85.67 sec(s) Train Acc: 0.940452 Loss: 0.002983 | Val Acc: 0.765015 loss: 0.017232\n",
      "[178/300] 85.52 sec(s) Train Acc: 0.940705 Loss: 0.002841 | Val Acc: 0.762974 loss: 0.018067\n",
      "[179/300] 85.64 sec(s) Train Acc: 0.941010 Loss: 0.002954 | Val Acc: 0.748688 loss: 0.017043\n",
      "[180/300] 85.54 sec(s) Train Acc: 0.940807 Loss: 0.002956 | Val Acc: 0.762682 loss: 0.017482\n",
      "[181/300] 85.63 sec(s) Train Acc: 0.941770 Loss: 0.002934 | Val Acc: 0.755977 loss: 0.017560\n",
      "[182/300] 85.69 sec(s) Train Acc: 0.938374 Loss: 0.002969 | Val Acc: 0.760350 loss: 0.016461\n",
      "[183/300] 85.56 sec(s) Train Acc: 0.941314 Loss: 0.002869 | Val Acc: 0.770845 loss: 0.016411\n",
      "[184/300] 85.67 sec(s) Train Acc: 0.941719 Loss: 0.002926 | Val Acc: 0.756268 loss: 0.017430\n",
      "[185/300] 85.67 sec(s) Train Acc: 0.944405 Loss: 0.002710 | Val Acc: 0.758892 loss: 0.017009\n",
      "[186/300] 85.65 sec(s) Train Acc: 0.939692 Loss: 0.003044 | Val Acc: 0.764723 loss: 0.016623\n",
      "[187/300] 85.63 sec(s) Train Acc: 0.944962 Loss: 0.002706 | Val Acc: 0.756851 loss: 0.017620\n",
      "[188/300] 85.66 sec(s) Train Acc: 0.945013 Loss: 0.002724 | Val Acc: 0.758892 loss: 0.018912\n",
      "[189/300] 85.63 sec(s) Train Acc: 0.943898 Loss: 0.002698 | Val Acc: 0.768222 loss: 0.018424\n",
      "[190/300] 85.78 sec(s) Train Acc: 0.944101 Loss: 0.002767 | Val Acc: 0.761808 loss: 0.018326\n",
      "[191/300] 85.72 sec(s) Train Acc: 0.943949 Loss: 0.002782 | Val Acc: 0.759767 loss: 0.017132\n",
      "[192/300] 85.67 sec(s) Train Acc: 0.948966 Loss: 0.002590 | Val Acc: 0.765598 loss: 0.017874\n",
      "[193/300] 85.71 sec(s) Train Acc: 0.943746 Loss: 0.002802 | Val Acc: 0.774052 loss: 0.018841\n",
      "[194/300] 85.64 sec(s) Train Acc: 0.946382 Loss: 0.002694 | Val Acc: 0.754519 loss: 0.019226\n",
      "[195/300] 85.67 sec(s) Train Acc: 0.945875 Loss: 0.002697 | Val Acc: 0.760641 loss: 0.019179\n",
      "[196/300] 85.68 sec(s) Train Acc: 0.945520 Loss: 0.002681 | Val Acc: 0.756268 loss: 0.019602\n",
      "[197/300] 85.60 sec(s) Train Acc: 0.945267 Loss: 0.002591 | Val Acc: 0.771720 loss: 0.017887\n",
      "[198/300] 85.66 sec(s) Train Acc: 0.952463 Loss: 0.002415 | Val Acc: 0.762682 loss: 0.018811\n",
      "[199/300] 85.60 sec(s) Train Acc: 0.943189 Loss: 0.002827 | Val Acc: 0.768513 loss: 0.017597\n",
      "[200/300] 85.61 sec(s) Train Acc: 0.943797 Loss: 0.002779 | Val Acc: 0.765598 loss: 0.017265\n",
      "[201/300] 85.55 sec(s) Train Acc: 0.943493 Loss: 0.002782 | Val Acc: 0.769679 loss: 0.017991\n",
      "[202/300] 85.63 sec(s) Train Acc: 0.949980 Loss: 0.002498 | Val Acc: 0.764140 loss: 0.019655\n",
      "[203/300] 85.66 sec(s) Train Acc: 0.943797 Loss: 0.002725 | Val Acc: 0.766181 loss: 0.016527\n",
      "[204/300] 85.64 sec(s) Train Acc: 0.946432 Loss: 0.002678 | Val Acc: 0.773178 loss: 0.017073\n",
      "[205/300] 85.56 sec(s) Train Acc: 0.946635 Loss: 0.002618 | Val Acc: 0.765889 loss: 0.018160\n",
      "[206/300] 85.57 sec(s) Train Acc: 0.949422 Loss: 0.002616 | Val Acc: 0.772012 loss: 0.018068\n",
      "[207/300] 85.54 sec(s) Train Acc: 0.949726 Loss: 0.002550 | Val Acc: 0.761516 loss: 0.019105\n",
      "[208/300] 85.55 sec(s) Train Acc: 0.948763 Loss: 0.002519 | Val Acc: 0.768805 loss: 0.016729\n",
      "[209/300] 85.71 sec(s) Train Acc: 0.947446 Loss: 0.002650 | Val Acc: 0.770554 loss: 0.017399\n",
      "[210/300] 85.65 sec(s) Train Acc: 0.947446 Loss: 0.002641 | Val Acc: 0.762974 loss: 0.018829\n",
      "[211/300] 85.60 sec(s) Train Acc: 0.947395 Loss: 0.002598 | Val Acc: 0.771137 loss: 0.017950\n",
      "[212/300] 85.61 sec(s) Train Acc: 0.947851 Loss: 0.002568 | Val Acc: 0.754519 loss: 0.017645\n",
      "[213/300] 85.66 sec(s) Train Acc: 0.947648 Loss: 0.002556 | Val Acc: 0.767055 loss: 0.016890\n",
      "[214/300] 85.63 sec(s) Train Acc: 0.947648 Loss: 0.002546 | Val Acc: 0.772595 loss: 0.016985\n",
      "[215/300] 85.65 sec(s) Train Acc: 0.950081 Loss: 0.002516 | Val Acc: 0.761808 loss: 0.017884\n",
      "[216/300] 85.66 sec(s) Train Acc: 0.947598 Loss: 0.002543 | Val Acc: 0.759475 loss: 0.018879\n",
      "[217/300] 85.62 sec(s) Train Acc: 0.949473 Loss: 0.002548 | Val Acc: 0.762391 loss: 0.019289\n",
      "[218/300] 85.65 sec(s) Train Acc: 0.952666 Loss: 0.002457 | Val Acc: 0.763848 loss: 0.017524\n",
      "[219/300] 85.72 sec(s) Train Acc: 0.950233 Loss: 0.002497 | Val Acc: 0.766472 loss: 0.018333\n",
      "[220/300] 85.62 sec(s) Train Acc: 0.950081 Loss: 0.002563 | Val Acc: 0.751020 loss: 0.019431\n",
      "[221/300] 85.60 sec(s) Train Acc: 0.950791 Loss: 0.002392 | Val Acc: 0.762682 loss: 0.018633\n",
      "[222/300] 85.64 sec(s) Train Acc: 0.950182 Loss: 0.002492 | Val Acc: 0.766181 loss: 0.018425\n",
      "[223/300] 85.71 sec(s) Train Acc: 0.950791 Loss: 0.002500 | Val Acc: 0.750437 loss: 0.018175\n",
      "[224/300] 85.72 sec(s) Train Acc: 0.949068 Loss: 0.002506 | Val Acc: 0.758017 loss: 0.018694\n",
      "[225/300] 85.77 sec(s) Train Acc: 0.951044 Loss: 0.002540 | Val Acc: 0.764723 loss: 0.019032\n",
      "[226/300] 85.77 sec(s) Train Acc: 0.948054 Loss: 0.002554 | Val Acc: 0.758601 loss: 0.019117\n",
      "[227/300] 85.74 sec(s) Train Acc: 0.952970 Loss: 0.002319 | Val Acc: 0.758601 loss: 0.018751\n",
      "[228/300] 85.69 sec(s) Train Acc: 0.950537 Loss: 0.002456 | Val Acc: 0.771429 loss: 0.017421\n",
      "[229/300] 85.75 sec(s) Train Acc: 0.954389 Loss: 0.002292 | Val Acc: 0.750729 loss: 0.019886\n",
      "[230/300] 85.70 sec(s) Train Acc: 0.952260 Loss: 0.002416 | Val Acc: 0.757143 loss: 0.018364\n",
      "[231/300] 85.68 sec(s) Train Acc: 0.952412 Loss: 0.002413 | Val Acc: 0.767930 loss: 0.018758\n",
      "[232/300] 85.74 sec(s) Train Acc: 0.951044 Loss: 0.002384 | Val Acc: 0.769388 loss: 0.019010\n",
      "[233/300] 85.77 sec(s) Train Acc: 0.949270 Loss: 0.002464 | Val Acc: 0.763265 loss: 0.017036\n",
      "[234/300] 85.64 sec(s) Train Acc: 0.950537 Loss: 0.002429 | Val Acc: 0.757143 loss: 0.018663\n",
      "[235/300] 85.68 sec(s) Train Acc: 0.954896 Loss: 0.002406 | Val Acc: 0.766764 loss: 0.019292\n",
      "[236/300] 85.76 sec(s) Train Acc: 0.954034 Loss: 0.002320 | Val Acc: 0.758892 loss: 0.019272\n",
      "[237/300] 85.82 sec(s) Train Acc: 0.953274 Loss: 0.002252 | Val Acc: 0.766764 loss: 0.017527\n",
      "[238/300] 85.61 sec(s) Train Acc: 0.952666 Loss: 0.002362 | Val Acc: 0.767930 loss: 0.017310\n",
      "[239/300] 85.72 sec(s) Train Acc: 0.954034 Loss: 0.002232 | Val Acc: 0.760933 loss: 0.018452\n",
      "[240/300] 85.79 sec(s) Train Acc: 0.952412 Loss: 0.002408 | Val Acc: 0.764140 loss: 0.017977\n",
      "[241/300] 85.71 sec(s) Train Acc: 0.954845 Loss: 0.002238 | Val Acc: 0.769388 loss: 0.019547\n",
      "[242/300] 85.68 sec(s) Train Acc: 0.955048 Loss: 0.002268 | Val Acc: 0.767638 loss: 0.018900\n",
      "[243/300] 85.75 sec(s) Train Acc: 0.954642 Loss: 0.002227 | Val Acc: 0.765598 loss: 0.017894\n",
      "[244/300] 85.59 sec(s) Train Acc: 0.952919 Loss: 0.002374 | Val Acc: 0.778426 loss: 0.017310\n",
      "[245/300] 85.66 sec(s) Train Acc: 0.953173 Loss: 0.002264 | Val Acc: 0.769971 loss: 0.016989\n",
      "[246/300] 85.62 sec(s) Train Acc: 0.954085 Loss: 0.002295 | Val Acc: 0.760058 loss: 0.019265\n",
      "[247/300] 85.63 sec(s) Train Acc: 0.952970 Loss: 0.002367 | Val Acc: 0.766472 loss: 0.019477\n",
      "[248/300] 85.65 sec(s) Train Acc: 0.956163 Loss: 0.002236 | Val Acc: 0.776093 loss: 0.017438\n",
      "[249/300] 85.72 sec(s) Train Acc: 0.955960 Loss: 0.002222 | Val Acc: 0.771720 loss: 0.019441\n",
      "[250/300] 85.65 sec(s) Train Acc: 0.956416 Loss: 0.002182 | Val Acc: 0.761808 loss: 0.016901\n",
      "[251/300] 85.58 sec(s) Train Acc: 0.957480 Loss: 0.002177 | Val Acc: 0.770554 loss: 0.018163\n",
      "[252/300] 85.79 sec(s) Train Acc: 0.953223 Loss: 0.002411 | Val Acc: 0.763265 loss: 0.017904\n",
      "[253/300] 85.63 sec(s) Train Acc: 0.956213 Loss: 0.002198 | Val Acc: 0.766472 loss: 0.017905\n",
      "[254/300] 85.69 sec(s) Train Acc: 0.956011 Loss: 0.002232 | Val Acc: 0.758601 loss: 0.022200\n",
      "[255/300] 85.58 sec(s) Train Acc: 0.955757 Loss: 0.002254 | Val Acc: 0.760641 loss: 0.019764\n",
      "[256/300] 85.64 sec(s) Train Acc: 0.954287 Loss: 0.002318 | Val Acc: 0.772012 loss: 0.018340\n",
      "[257/300] 85.70 sec(s) Train Acc: 0.956365 Loss: 0.002197 | Val Acc: 0.774636 loss: 0.018521\n",
      "[258/300] 85.73 sec(s) Train Acc: 0.956264 Loss: 0.002255 | Val Acc: 0.767055 loss: 0.018701\n",
      "[259/300] 85.73 sec(s) Train Acc: 0.955352 Loss: 0.002214 | Val Acc: 0.761516 loss: 0.018972\n",
      "[260/300] 85.75 sec(s) Train Acc: 0.956872 Loss: 0.002102 | Val Acc: 0.768805 loss: 0.019491\n",
      "[261/300] 85.56 sec(s) Train Acc: 0.954287 Loss: 0.002363 | Val Acc: 0.765015 loss: 0.019564\n",
      "[262/300] 85.55 sec(s) Train Acc: 0.957683 Loss: 0.002160 | Val Acc: 0.762099 loss: 0.021648\n",
      "[263/300] 85.68 sec(s) Train Acc: 0.956872 Loss: 0.002208 | Val Acc: 0.766472 loss: 0.018914\n",
      "[264/300] 85.63 sec(s) Train Acc: 0.957024 Loss: 0.002157 | Val Acc: 0.770262 loss: 0.018671\n",
      "[265/300] 85.71 sec(s) Train Acc: 0.957987 Loss: 0.002142 | Val Acc: 0.758309 loss: 0.020274\n",
      "[266/300] 85.71 sec(s) Train Acc: 0.957075 Loss: 0.002238 | Val Acc: 0.763557 loss: 0.018770\n",
      "[267/300] 85.76 sec(s) Train Acc: 0.958798 Loss: 0.002101 | Val Acc: 0.769971 loss: 0.018312\n",
      "[268/300] 85.65 sec(s) Train Acc: 0.957987 Loss: 0.002134 | Val Acc: 0.765015 loss: 0.019713\n",
      "[269/300] 85.73 sec(s) Train Acc: 0.954034 Loss: 0.002285 | Val Acc: 0.773469 loss: 0.016861\n",
      "[270/300] 85.66 sec(s) Train Acc: 0.955706 Loss: 0.002193 | Val Acc: 0.769096 loss: 0.017973\n",
      "[271/300] 85.73 sec(s) Train Acc: 0.957176 Loss: 0.002071 | Val Acc: 0.770554 loss: 0.016701\n",
      "[272/300] 85.75 sec(s) Train Acc: 0.958139 Loss: 0.002074 | Val Acc: 0.773761 loss: 0.017838\n",
      "[273/300] 85.81 sec(s) Train Acc: 0.955402 Loss: 0.002190 | Val Acc: 0.756560 loss: 0.019259\n",
      "[274/300] 85.79 sec(s) Train Acc: 0.958240 Loss: 0.002155 | Val Acc: 0.770845 loss: 0.017182\n",
      "[275/300] 85.81 sec(s) Train Acc: 0.957075 Loss: 0.002185 | Val Acc: 0.764431 loss: 0.019156\n",
      "[276/300] 85.75 sec(s) Train Acc: 0.958443 Loss: 0.002050 | Val Acc: 0.764723 loss: 0.019819\n",
      "[277/300] 85.77 sec(s) Train Acc: 0.959203 Loss: 0.002083 | Val Acc: 0.762974 loss: 0.017502\n",
      "[278/300] 85.76 sec(s) Train Acc: 0.956011 Loss: 0.002178 | Val Acc: 0.767638 loss: 0.018664\n",
      "[279/300] 85.68 sec(s) Train Acc: 0.959051 Loss: 0.002048 | Val Acc: 0.765889 loss: 0.021223\n",
      "[280/300] 85.70 sec(s) Train Acc: 0.961687 Loss: 0.001957 | Val Acc: 0.765306 loss: 0.018850\n",
      "[281/300] 85.65 sec(s) Train Acc: 0.958088 Loss: 0.002126 | Val Acc: 0.767347 loss: 0.019305\n",
      "[282/300] 85.73 sec(s) Train Acc: 0.955504 Loss: 0.002306 | Val Acc: 0.773761 loss: 0.017080\n",
      "[283/300] 85.68 sec(s) Train Acc: 0.959609 Loss: 0.001999 | Val Acc: 0.761516 loss: 0.020750\n",
      "[284/300] 85.66 sec(s) Train Acc: 0.956011 Loss: 0.002205 | Val Acc: 0.775510 loss: 0.018914\n",
      "[285/300] 85.73 sec(s) Train Acc: 0.959355 Loss: 0.002101 | Val Acc: 0.767638 loss: 0.017808\n",
      "[286/300] 85.54 sec(s) Train Acc: 0.960926 Loss: 0.001994 | Val Acc: 0.765306 loss: 0.017370\n",
      "[287/300] 85.63 sec(s) Train Acc: 0.957987 Loss: 0.002077 | Val Acc: 0.772886 loss: 0.018750\n",
      "[288/300] 85.68 sec(s) Train Acc: 0.960268 Loss: 0.001878 | Val Acc: 0.781341 loss: 0.018255\n",
      "[289/300] 85.68 sec(s) Train Acc: 0.960420 Loss: 0.002076 | Val Acc: 0.764140 loss: 0.019319\n",
      "[290/300] 85.64 sec(s) Train Acc: 0.958697 Loss: 0.002080 | Val Acc: 0.772886 loss: 0.017622\n",
      "[291/300] 85.58 sec(s) Train Acc: 0.961383 Loss: 0.001930 | Val Acc: 0.785423 loss: 0.020637\n",
      "[292/300] 85.67 sec(s) Train Acc: 0.959457 Loss: 0.002019 | Val Acc: 0.776385 loss: 0.019768\n",
      "[293/300] 85.75 sec(s) Train Acc: 0.959001 Loss: 0.002030 | Val Acc: 0.760641 loss: 0.019557\n",
      "[294/300] 85.70 sec(s) Train Acc: 0.958139 Loss: 0.002069 | Val Acc: 0.779300 loss: 0.018358\n",
      "[295/300] 85.74 sec(s) Train Acc: 0.959811 Loss: 0.002027 | Val Acc: 0.774344 loss: 0.020922\n",
      "[296/300] 85.69 sec(s) Train Acc: 0.958798 Loss: 0.002029 | Val Acc: 0.782216 loss: 0.018505\n",
      "[297/300] 85.67 sec(s) Train Acc: 0.961129 Loss: 0.001905 | Val Acc: 0.765015 loss: 0.020784\n",
      "[298/300] 85.76 sec(s) Train Acc: 0.960318 Loss: 0.001968 | Val Acc: 0.769388 loss: 0.017933\n",
      "[299/300] 85.70 sec(s) Train Acc: 0.962447 Loss: 0.001944 | Val Acc: 0.776968 loss: 0.018362\n",
      "[300/300] 85.72 sec(s) Train Acc: 0.960977 Loss: 0.002006 | Val Acc: 0.770554 loss: 0.018496\n"
     ]
    }
   ],
   "source": [
    "model = Classifier().cuda()\n",
    "# model = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 300\n",
    "\n",
    "# # use apex to optimize\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "#         train_pred = model(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "#             val_pred = model(data[0].cpu())\n",
    "#             batch_loss = loss(val_pred, data[1].cpu())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-ssSxXlsI_T"
   },
   "source": [
    "得到好的參數後，我們使用 training set 和 validation set 共同訓練（資料量變多，模型效果較好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKoUxLun8lFG",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.89311144813877, 102.3587941606983, 126.59376063616554]\n",
      "[72.80305392379675, 75.35438507973123, 79.31408066842762]\n"
     ]
    }
   ],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ConcatDataset([\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform1),\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform2),\n",
    "])\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print([train_val_x[:,:,:,0].mean(), train_val_x[:,:,:,1].mean(), train_val_x[:,:,:,2].mean()])\n",
    "print([train_val_x[:,:,:,0].std(), train_val_x[:,:,:,1].std(), train_val_x[:,:,:,2].std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoAS5TtRsfOo",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/350] 108.08 sec(s) Train Acc: 0.240335 Loss: 0.033483\n",
      "[002/350] 108.39 sec(s) Train Acc: 0.274669 Loss: 0.031916\n",
      "[003/350] 108.49 sec(s) Train Acc: 0.314004 Loss: 0.030542\n",
      "[004/350] 108.46 sec(s) Train Acc: 0.362440 Loss: 0.028751\n",
      "[005/350] 108.45 sec(s) Train Acc: 0.390042 Loss: 0.027354\n",
      "[006/350] 108.38 sec(s) Train Acc: 0.423097 Loss: 0.026155\n",
      "[007/350] 108.39 sec(s) Train Acc: 0.443554 Loss: 0.025198\n",
      "[008/350] 108.54 sec(s) Train Acc: 0.472435 Loss: 0.024228\n",
      "[009/350] 108.51 sec(s) Train Acc: 0.493870 Loss: 0.023268\n",
      "[010/350] 108.43 sec(s) Train Acc: 0.505190 Loss: 0.022426\n",
      "[011/350] 108.48 sec(s) Train Acc: 0.530987 Loss: 0.021664\n",
      "[012/350] 108.51 sec(s) Train Acc: 0.544111 Loss: 0.021036\n",
      "[013/350] 108.42 sec(s) Train Acc: 0.552572 Loss: 0.020502\n",
      "[014/350] 108.44 sec(s) Train Acc: 0.567802 Loss: 0.019918\n",
      "[015/350] 108.40 sec(s) Train Acc: 0.584236 Loss: 0.019195\n",
      "[016/350] 108.42 sec(s) Train Acc: 0.590930 Loss: 0.018864\n",
      "[017/350] 108.46 sec(s) Train Acc: 0.600594 Loss: 0.018389\n",
      "[018/350] 108.48 sec(s) Train Acc: 0.618193 Loss: 0.017793\n",
      "[019/350] 108.42 sec(s) Train Acc: 0.623947 Loss: 0.017366\n",
      "[020/350] 108.33 sec(s) Train Acc: 0.640456 Loss: 0.016845\n",
      "[021/350] 108.36 sec(s) Train Acc: 0.643539 Loss: 0.016491\n",
      "[022/350] 108.35 sec(s) Train Acc: 0.656250 Loss: 0.016105\n",
      "[023/350] 108.44 sec(s) Train Acc: 0.663245 Loss: 0.015661\n",
      "[024/350] 108.43 sec(s) Train Acc: 0.669487 Loss: 0.015496\n",
      "[025/350] 108.43 sec(s) Train Acc: 0.681784 Loss: 0.014948\n",
      "[026/350] 108.50 sec(s) Train Acc: 0.685093 Loss: 0.014601\n",
      "[027/350] 108.50 sec(s) Train Acc: 0.696713 Loss: 0.014260\n",
      "[028/350] 108.45 sec(s) Train Acc: 0.700662 Loss: 0.014018\n",
      "[029/350] 108.51 sec(s) Train Acc: 0.707506 Loss: 0.013653\n",
      "[030/350] 108.47 sec(s) Train Acc: 0.715554 Loss: 0.013427\n",
      "[031/350] 108.44 sec(s) Train Acc: 0.719540 Loss: 0.013090\n",
      "[032/350] 108.43 sec(s) Train Acc: 0.728941 Loss: 0.012680\n",
      "[033/350] 108.43 sec(s) Train Acc: 0.733416 Loss: 0.012499\n",
      "[034/350] 108.49 sec(s) Train Acc: 0.741689 Loss: 0.012234\n",
      "[035/350] 108.53 sec(s) Train Acc: 0.748909 Loss: 0.011801\n",
      "[036/350] 108.46 sec(s) Train Acc: 0.754024 Loss: 0.011605\n",
      "[037/350] 108.47 sec(s) Train Acc: 0.763087 Loss: 0.011339\n",
      "[038/350] 108.52 sec(s) Train Acc: 0.763087 Loss: 0.011201\n",
      "[039/350] 108.43 sec(s) Train Acc: 0.772789 Loss: 0.010818\n",
      "[040/350] 108.38 sec(s) Train Acc: 0.773842 Loss: 0.010779\n",
      "[041/350] 108.48 sec(s) Train Acc: 0.779483 Loss: 0.010518\n",
      "[042/350] 108.60 sec(s) Train Acc: 0.780987 Loss: 0.010351\n",
      "[043/350] 108.48 sec(s) Train Acc: 0.793096 Loss: 0.009914\n",
      "[044/350] 108.52 sec(s) Train Acc: 0.797082 Loss: 0.009694\n",
      "[045/350] 108.50 sec(s) Train Acc: 0.800353 Loss: 0.009476\n",
      "[046/350] 108.40 sec(s) Train Acc: 0.801632 Loss: 0.009431\n",
      "[047/350] 108.47 sec(s) Train Acc: 0.808288 Loss: 0.009186\n",
      "[048/350] 108.53 sec(s) Train Acc: 0.814531 Loss: 0.008909\n",
      "[049/350] 108.50 sec(s) Train Acc: 0.815772 Loss: 0.008903\n",
      "[050/350] 108.51 sec(s) Train Acc: 0.820472 Loss: 0.008677\n",
      "[051/350] 108.53 sec(s) Train Acc: 0.820623 Loss: 0.008597\n",
      "[052/350] 108.53 sec(s) Train Acc: 0.821412 Loss: 0.008504\n",
      "[053/350] 108.46 sec(s) Train Acc: 0.831528 Loss: 0.008080\n",
      "[054/350] 108.45 sec(s) Train Acc: 0.834424 Loss: 0.007987\n",
      "[055/350] 108.43 sec(s) Train Acc: 0.833296 Loss: 0.007944\n",
      "[056/350] 108.42 sec(s) Train Acc: 0.837207 Loss: 0.007834\n",
      "[057/350] 108.56 sec(s) Train Acc: 0.846006 Loss: 0.007513\n",
      "[058/350] 108.46 sec(s) Train Acc: 0.846382 Loss: 0.007404\n",
      "[059/350] 108.50 sec(s) Train Acc: 0.845179 Loss: 0.007510\n",
      "[060/350] 108.50 sec(s) Train Acc: 0.845367 Loss: 0.007380\n",
      "[061/350] 108.46 sec(s) Train Acc: 0.853001 Loss: 0.007108\n",
      "[062/350] 108.45 sec(s) Train Acc: 0.857288 Loss: 0.006918\n",
      "[063/350] 108.44 sec(s) Train Acc: 0.856235 Loss: 0.006948\n",
      "[064/350] 108.38 sec(s) Train Acc: 0.861274 Loss: 0.006742\n",
      "[065/350] 108.40 sec(s) Train Acc: 0.860672 Loss: 0.006773\n",
      "[066/350] 108.49 sec(s) Train Acc: 0.860259 Loss: 0.006726\n",
      "[067/350] 108.42 sec(s) Train Acc: 0.865937 Loss: 0.006612\n",
      "[068/350] 108.44 sec(s) Train Acc: 0.871089 Loss: 0.006355\n",
      "[069/350] 108.52 sec(s) Train Acc: 0.872368 Loss: 0.006233\n",
      "[070/350] 108.38 sec(s) Train Acc: 0.869171 Loss: 0.006316\n",
      "[071/350] 108.47 sec(s) Train Acc: 0.875451 Loss: 0.006175\n",
      "[072/350] 108.46 sec(s) Train Acc: 0.874699 Loss: 0.006082\n",
      "[073/350] 108.47 sec(s) Train Acc: 0.877256 Loss: 0.006006\n",
      "[074/350] 108.46 sec(s) Train Acc: 0.874850 Loss: 0.005990\n",
      "[075/350] 108.47 sec(s) Train Acc: 0.883499 Loss: 0.005672\n",
      "[076/350] 108.54 sec(s) Train Acc: 0.885041 Loss: 0.005688\n",
      "[077/350] 108.41 sec(s) Train Acc: 0.883198 Loss: 0.005722\n",
      "[078/350] 108.53 sec(s) Train Acc: 0.881130 Loss: 0.005726\n",
      "[079/350] 108.44 sec(s) Train Acc: 0.883612 Loss: 0.005674\n",
      "[080/350] 108.53 sec(s) Train Acc: 0.889929 Loss: 0.005430\n",
      "[081/350] 108.45 sec(s) Train Acc: 0.885868 Loss: 0.005583\n",
      "[082/350] 108.45 sec(s) Train Acc: 0.889967 Loss: 0.005385\n",
      "[083/350] 108.54 sec(s) Train Acc: 0.889440 Loss: 0.005379\n",
      "[084/350] 108.47 sec(s) Train Acc: 0.887861 Loss: 0.005473\n",
      "[085/350] 108.43 sec(s) Train Acc: 0.894818 Loss: 0.005097\n",
      "[086/350] 108.56 sec(s) Train Acc: 0.894555 Loss: 0.005154\n",
      "[087/350] 108.45 sec(s) Train Acc: 0.894893 Loss: 0.005112\n",
      "[088/350] 108.45 sec(s) Train Acc: 0.896059 Loss: 0.005130\n",
      "[089/350] 108.43 sec(s) Train Acc: 0.894517 Loss: 0.005097\n",
      "[090/350] 108.46 sec(s) Train Acc: 0.899707 Loss: 0.004933\n",
      "[091/350] 108.38 sec(s) Train Acc: 0.899782 Loss: 0.004845\n",
      "[092/350] 108.45 sec(s) Train Acc: 0.902715 Loss: 0.004696\n",
      "[093/350] 108.32 sec(s) Train Acc: 0.902264 Loss: 0.004822\n",
      "[094/350] 108.45 sec(s) Train Acc: 0.903016 Loss: 0.004726\n",
      "[095/350] 108.52 sec(s) Train Acc: 0.900308 Loss: 0.004839\n",
      "[096/350] 108.46 sec(s) Train Acc: 0.902941 Loss: 0.004716\n",
      "[097/350] 108.59 sec(s) Train Acc: 0.909484 Loss: 0.004520\n",
      "[098/350] 108.38 sec(s) Train Acc: 0.906927 Loss: 0.004589\n",
      "[099/350] 108.44 sec(s) Train Acc: 0.908694 Loss: 0.004494\n",
      "[100/350] 108.42 sec(s) Train Acc: 0.908920 Loss: 0.004513\n",
      "[101/350] 108.45 sec(s) Train Acc: 0.911778 Loss: 0.004342\n",
      "[102/350] 108.45 sec(s) Train Acc: 0.907341 Loss: 0.004453\n",
      "[103/350] 108.48 sec(s) Train Acc: 0.910575 Loss: 0.004456\n",
      "[104/350] 108.44 sec(s) Train Acc: 0.911891 Loss: 0.004306\n",
      "[105/350] 108.46 sec(s) Train Acc: 0.912643 Loss: 0.004250\n",
      "[106/350] 108.36 sec(s) Train Acc: 0.916027 Loss: 0.004118\n",
      "[107/350] 108.39 sec(s) Train Acc: 0.913019 Loss: 0.004239\n",
      "[108/350] 108.35 sec(s) Train Acc: 0.912304 Loss: 0.004377\n",
      "[109/350] 108.31 sec(s) Train Acc: 0.916215 Loss: 0.004097\n",
      "[110/350] 108.47 sec(s) Train Acc: 0.912455 Loss: 0.004332\n",
      "[111/350] 109.47 sec(s) Train Acc: 0.913545 Loss: 0.004256\n",
      "[112/350] 108.94 sec(s) Train Acc: 0.913357 Loss: 0.004190\n",
      "[113/350] 108.90 sec(s) Train Acc: 0.918208 Loss: 0.004050\n",
      "[114/350] 108.79 sec(s) Train Acc: 0.920390 Loss: 0.003945\n",
      "[115/350] 108.81 sec(s) Train Acc: 0.918020 Loss: 0.004050\n",
      "[116/350] 108.79 sec(s) Train Acc: 0.917268 Loss: 0.004016\n",
      "[117/350] 108.77 sec(s) Train Acc: 0.917908 Loss: 0.003929\n",
      "[118/350] 108.61 sec(s) Train Acc: 0.921668 Loss: 0.003811\n",
      "[119/350] 108.72 sec(s) Train Acc: 0.923285 Loss: 0.003835\n",
      "[120/350] 108.59 sec(s) Train Acc: 0.924564 Loss: 0.003789\n",
      "[121/350] 108.46 sec(s) Train Acc: 0.922495 Loss: 0.003819\n",
      "[122/350] 108.51 sec(s) Train Acc: 0.921217 Loss: 0.003877\n",
      "[123/350] 108.53 sec(s) Train Acc: 0.920540 Loss: 0.003930\n",
      "[124/350] 108.48 sec(s) Train Acc: 0.922796 Loss: 0.003801\n",
      "[125/350] 108.46 sec(s) Train Acc: 0.923135 Loss: 0.003742\n",
      "[126/350] 108.45 sec(s) Train Acc: 0.925203 Loss: 0.003737\n",
      "[127/350] 108.54 sec(s) Train Acc: 0.926143 Loss: 0.003824\n",
      "[128/350] 108.45 sec(s) Train Acc: 0.924263 Loss: 0.003729\n",
      "[129/350] 108.46 sec(s) Train Acc: 0.924940 Loss: 0.003630\n",
      "[130/350] 108.46 sec(s) Train Acc: 0.923812 Loss: 0.003697\n",
      "[131/350] 108.39 sec(s) Train Acc: 0.927723 Loss: 0.003641\n",
      "[132/350] 108.37 sec(s) Train Acc: 0.926895 Loss: 0.003585\n",
      "[133/350] 108.43 sec(s) Train Acc: 0.928738 Loss: 0.003539\n",
      "[134/350] 108.45 sec(s) Train Acc: 0.929415 Loss: 0.003483\n",
      "[135/350] 108.52 sec(s) Train Acc: 0.926369 Loss: 0.003556\n",
      "[136/350] 108.49 sec(s) Train Acc: 0.928888 Loss: 0.003546\n",
      "[137/350] 108.49 sec(s) Train Acc: 0.929001 Loss: 0.003523\n",
      "[138/350] 108.41 sec(s) Train Acc: 0.929452 Loss: 0.003547\n",
      "[139/350] 108.47 sec(s) Train Acc: 0.932574 Loss: 0.003364\n",
      "[140/350] 108.35 sec(s) Train Acc: 0.933363 Loss: 0.003349\n",
      "[141/350] 108.41 sec(s) Train Acc: 0.931859 Loss: 0.003314\n",
      "[142/350] 108.38 sec(s) Train Acc: 0.928136 Loss: 0.003560\n",
      "[143/350] 108.49 sec(s) Train Acc: 0.934529 Loss: 0.003264\n",
      "[144/350] 108.43 sec(s) Train Acc: 0.932574 Loss: 0.003284\n",
      "[145/350] 108.54 sec(s) Train Acc: 0.931032 Loss: 0.003394\n",
      "[146/350] 108.44 sec(s) Train Acc: 0.935093 Loss: 0.003258\n",
      "[147/350] 108.41 sec(s) Train Acc: 0.933664 Loss: 0.003265\n",
      "[148/350] 108.47 sec(s) Train Acc: 0.933025 Loss: 0.003299\n",
      "[149/350] 108.32 sec(s) Train Acc: 0.933363 Loss: 0.003236\n",
      "[150/350] 108.36 sec(s) Train Acc: 0.933664 Loss: 0.003323\n",
      "[151/350] 108.45 sec(s) Train Acc: 0.937538 Loss: 0.003084\n",
      "[152/350] 108.53 sec(s) Train Acc: 0.933326 Loss: 0.003228\n",
      "[153/350] 108.45 sec(s) Train Acc: 0.937500 Loss: 0.003197\n",
      "[154/350] 108.27 sec(s) Train Acc: 0.937613 Loss: 0.003094\n",
      "[155/350] 108.40 sec(s) Train Acc: 0.936184 Loss: 0.003126\n",
      "[156/350] 108.41 sec(s) Train Acc: 0.938365 Loss: 0.003057\n",
      "[157/350] 108.37 sec(s) Train Acc: 0.939907 Loss: 0.003070\n",
      "[158/350] 108.35 sec(s) Train Acc: 0.937688 Loss: 0.003004\n",
      "[159/350] 108.47 sec(s) Train Acc: 0.937650 Loss: 0.003076\n",
      "[160/350] 108.36 sec(s) Train Acc: 0.937462 Loss: 0.003122\n",
      "[161/350] 108.41 sec(s) Train Acc: 0.936372 Loss: 0.003179\n",
      "[162/350] 108.43 sec(s) Train Acc: 0.937726 Loss: 0.003107\n",
      "[163/350] 108.33 sec(s) Train Acc: 0.939455 Loss: 0.003041\n",
      "[164/350] 108.43 sec(s) Train Acc: 0.939944 Loss: 0.003026\n",
      "[165/350] 108.43 sec(s) Train Acc: 0.940020 Loss: 0.003076\n",
      "[166/350] 108.32 sec(s) Train Acc: 0.939907 Loss: 0.003014\n",
      "[167/350] 108.50 sec(s) Train Acc: 0.939418 Loss: 0.002956\n",
      "[168/350] 108.53 sec(s) Train Acc: 0.938666 Loss: 0.002981\n",
      "[169/350] 108.41 sec(s) Train Acc: 0.940546 Loss: 0.002974\n",
      "[170/350] 108.46 sec(s) Train Acc: 0.942878 Loss: 0.002903\n",
      "[171/350] 108.34 sec(s) Train Acc: 0.942389 Loss: 0.002834\n",
      "[172/350] 108.35 sec(s) Train Acc: 0.939982 Loss: 0.002991\n",
      "[173/350] 108.32 sec(s) Train Acc: 0.945360 Loss: 0.002791\n",
      "[174/350] 108.38 sec(s) Train Acc: 0.942464 Loss: 0.002837\n",
      "[175/350] 108.49 sec(s) Train Acc: 0.941749 Loss: 0.002889\n",
      "[176/350] 108.48 sec(s) Train Acc: 0.943028 Loss: 0.002892\n",
      "[177/350] 108.42 sec(s) Train Acc: 0.943442 Loss: 0.002824\n",
      "[178/350] 108.37 sec(s) Train Acc: 0.942802 Loss: 0.002815\n",
      "[179/350] 108.42 sec(s) Train Acc: 0.943329 Loss: 0.002852\n",
      "[180/350] 108.38 sec(s) Train Acc: 0.941637 Loss: 0.002914\n",
      "[181/350] 108.41 sec(s) Train Acc: 0.945999 Loss: 0.002666\n",
      "[182/350] 108.44 sec(s) Train Acc: 0.946412 Loss: 0.002697\n",
      "[183/350] 108.47 sec(s) Train Acc: 0.943893 Loss: 0.002767\n",
      "[184/350] 108.38 sec(s) Train Acc: 0.948067 Loss: 0.002594\n",
      "[185/350] 108.51 sec(s) Train Acc: 0.944382 Loss: 0.002770\n",
      "[186/350] 108.45 sec(s) Train Acc: 0.945059 Loss: 0.002756\n",
      "[187/350] 108.47 sec(s) Train Acc: 0.947691 Loss: 0.002609\n",
      "[188/350] 108.46 sec(s) Train Acc: 0.945209 Loss: 0.002775\n",
      "[189/350] 108.51 sec(s) Train Acc: 0.943667 Loss: 0.002736\n",
      "[190/350] 108.51 sec(s) Train Acc: 0.945397 Loss: 0.002731\n",
      "[191/350] 108.54 sec(s) Train Acc: 0.949007 Loss: 0.002545\n",
      "[192/350] 108.55 sec(s) Train Acc: 0.946337 Loss: 0.002735\n",
      "[193/350] 108.55 sec(s) Train Acc: 0.946525 Loss: 0.002662\n",
      "[194/350] 108.49 sec(s) Train Acc: 0.945171 Loss: 0.002760\n",
      "[195/350] 108.51 sec(s) Train Acc: 0.949383 Loss: 0.002560\n",
      "[196/350] 108.46 sec(s) Train Acc: 0.947202 Loss: 0.002623\n",
      "[197/350] 108.56 sec(s) Train Acc: 0.949045 Loss: 0.002568\n",
      "[198/350] 108.44 sec(s) Train Acc: 0.947804 Loss: 0.002614\n",
      "[199/350] 108.49 sec(s) Train Acc: 0.946224 Loss: 0.002646\n",
      "[200/350] 108.43 sec(s) Train Acc: 0.948443 Loss: 0.002578\n",
      "[201/350] 108.42 sec(s) Train Acc: 0.948932 Loss: 0.002531\n",
      "[202/350] 108.49 sec(s) Train Acc: 0.947541 Loss: 0.002622\n",
      "[203/350] 108.56 sec(s) Train Acc: 0.948180 Loss: 0.002615\n",
      "[204/350] 108.43 sec(s) Train Acc: 0.948782 Loss: 0.002614\n",
      "[205/350] 108.53 sec(s) Train Acc: 0.948406 Loss: 0.002614\n",
      "[206/350] 108.52 sec(s) Train Acc: 0.948782 Loss: 0.002589\n",
      "[207/350] 108.50 sec(s) Train Acc: 0.950699 Loss: 0.002535\n",
      "[208/350] 108.42 sec(s) Train Acc: 0.950248 Loss: 0.002468\n",
      "[209/350] 108.56 sec(s) Train Acc: 0.949872 Loss: 0.002516\n",
      "[210/350] 108.47 sec(s) Train Acc: 0.951301 Loss: 0.002464\n",
      "[211/350] 108.50 sec(s) Train Acc: 0.950436 Loss: 0.002499\n",
      "[212/350] 108.46 sec(s) Train Acc: 0.947879 Loss: 0.002549\n",
      "[213/350] 108.49 sec(s) Train Acc: 0.952580 Loss: 0.002401\n",
      "[214/350] 108.45 sec(s) Train Acc: 0.950211 Loss: 0.002506\n",
      "[215/350] 108.44 sec(s) Train Acc: 0.949947 Loss: 0.002492\n",
      "[216/350] 108.54 sec(s) Train Acc: 0.952316 Loss: 0.002367\n",
      "[217/350] 108.54 sec(s) Train Acc: 0.950850 Loss: 0.002425\n",
      "[218/350] 108.47 sec(s) Train Acc: 0.950060 Loss: 0.002499\n",
      "[219/350] 108.53 sec(s) Train Acc: 0.951452 Loss: 0.002396\n",
      "[220/350] 108.50 sec(s) Train Acc: 0.953144 Loss: 0.002328\n",
      "[221/350] 108.45 sec(s) Train Acc: 0.951602 Loss: 0.002428\n",
      "[222/350] 108.43 sec(s) Train Acc: 0.953069 Loss: 0.002386\n",
      "[223/350] 108.47 sec(s) Train Acc: 0.952918 Loss: 0.002367\n",
      "[224/350] 108.45 sec(s) Train Acc: 0.953031 Loss: 0.002309\n",
      "[225/350] 108.52 sec(s) Train Acc: 0.953520 Loss: 0.002302\n",
      "[226/350] 108.47 sec(s) Train Acc: 0.954874 Loss: 0.002257\n",
      "[227/350] 108.48 sec(s) Train Acc: 0.952542 Loss: 0.002412\n",
      "[228/350] 108.41 sec(s) Train Acc: 0.954197 Loss: 0.002344\n",
      "[229/350] 108.50 sec(s) Train Acc: 0.954761 Loss: 0.002286\n",
      "[230/350] 108.43 sec(s) Train Acc: 0.953144 Loss: 0.002401\n",
      "[231/350] 108.41 sec(s) Train Acc: 0.954723 Loss: 0.002307\n",
      "[232/350] 108.48 sec(s) Train Acc: 0.951602 Loss: 0.002397\n",
      "[233/350] 108.37 sec(s) Train Acc: 0.951978 Loss: 0.002369\n",
      "[234/350] 108.45 sec(s) Train Acc: 0.954610 Loss: 0.002365\n",
      "[235/350] 108.55 sec(s) Train Acc: 0.953031 Loss: 0.002317\n",
      "[236/350] 108.52 sec(s) Train Acc: 0.954648 Loss: 0.002241\n",
      "[237/350] 108.44 sec(s) Train Acc: 0.952354 Loss: 0.002339\n",
      "[238/350] 108.47 sec(s) Train Acc: 0.951000 Loss: 0.002426\n",
      "[239/350] 108.40 sec(s) Train Acc: 0.956039 Loss: 0.002190\n",
      "[240/350] 108.47 sec(s) Train Acc: 0.956378 Loss: 0.002203\n",
      "[241/350] 108.36 sec(s) Train Acc: 0.954949 Loss: 0.002210\n",
      "[242/350] 108.39 sec(s) Train Acc: 0.955174 Loss: 0.002297\n",
      "[243/350] 108.42 sec(s) Train Acc: 0.955475 Loss: 0.002174\n",
      "[244/350] 108.49 sec(s) Train Acc: 0.957769 Loss: 0.002089\n",
      "[245/350] 108.57 sec(s) Train Acc: 0.955363 Loss: 0.002212\n",
      "[246/350] 108.48 sec(s) Train Acc: 0.954535 Loss: 0.002299\n",
      "[247/350] 108.43 sec(s) Train Acc: 0.954648 Loss: 0.002239\n",
      "[248/350] 108.39 sec(s) Train Acc: 0.955588 Loss: 0.002261\n",
      "[249/350] 108.57 sec(s) Train Acc: 0.959612 Loss: 0.002039\n",
      "[250/350] 108.66 sec(s) Train Acc: 0.955663 Loss: 0.002184\n",
      "[251/350] 108.53 sec(s) Train Acc: 0.956491 Loss: 0.002157\n",
      "[252/350] 108.42 sec(s) Train Acc: 0.960590 Loss: 0.002087\n",
      "[253/350] 108.44 sec(s) Train Acc: 0.955776 Loss: 0.002259\n",
      "[254/350] 108.43 sec(s) Train Acc: 0.957243 Loss: 0.002178\n",
      "[255/350] 108.28 sec(s) Train Acc: 0.957506 Loss: 0.002134\n",
      "[256/350] 108.47 sec(s) Train Acc: 0.958446 Loss: 0.002120\n",
      "[257/350] 108.41 sec(s) Train Acc: 0.958371 Loss: 0.002048\n",
      "[258/350] 108.46 sec(s) Train Acc: 0.955287 Loss: 0.002178\n",
      "[259/350] 108.42 sec(s) Train Acc: 0.956265 Loss: 0.002206\n",
      "[260/350] 108.43 sec(s) Train Acc: 0.958484 Loss: 0.002098\n",
      "[261/350] 108.54 sec(s) Train Acc: 0.957544 Loss: 0.002146\n",
      "[262/350] 108.41 sec(s) Train Acc: 0.959386 Loss: 0.002024\n",
      "[263/350] 108.48 sec(s) Train Acc: 0.958145 Loss: 0.002154\n",
      "[264/350] 108.45 sec(s) Train Acc: 0.957468 Loss: 0.002136\n",
      "[265/350] 108.51 sec(s) Train Acc: 0.956679 Loss: 0.002151\n",
      "[266/350] 108.46 sec(s) Train Acc: 0.957243 Loss: 0.002117\n",
      "[267/350] 108.42 sec(s) Train Acc: 0.958484 Loss: 0.002046\n",
      "[268/350] 108.48 sec(s) Train Acc: 0.959499 Loss: 0.002037\n",
      "[269/350] 108.40 sec(s) Train Acc: 0.960590 Loss: 0.001994\n",
      "[270/350] 108.53 sec(s) Train Acc: 0.957807 Loss: 0.002109\n",
      "[271/350] 108.39 sec(s) Train Acc: 0.957092 Loss: 0.002112\n",
      "[272/350] 108.47 sec(s) Train Acc: 0.958183 Loss: 0.002041\n",
      "[273/350] 108.37 sec(s) Train Acc: 0.957957 Loss: 0.002150\n",
      "[274/350] 108.49 sec(s) Train Acc: 0.960101 Loss: 0.001966\n",
      "[275/350] 108.43 sec(s) Train Acc: 0.960740 Loss: 0.002023\n",
      "[276/350] 108.38 sec(s) Train Acc: 0.959762 Loss: 0.001984\n",
      "[277/350] 108.41 sec(s) Train Acc: 0.960063 Loss: 0.002021\n",
      "[278/350] 108.33 sec(s) Train Acc: 0.961530 Loss: 0.001972\n",
      "[279/350] 108.43 sec(s) Train Acc: 0.959650 Loss: 0.001999\n",
      "[280/350] 108.37 sec(s) Train Acc: 0.962207 Loss: 0.001961\n",
      "[281/350] 108.35 sec(s) Train Acc: 0.959687 Loss: 0.002020\n",
      "[282/350] 108.41 sec(s) Train Acc: 0.959048 Loss: 0.002080\n",
      "[283/350] 108.42 sec(s) Train Acc: 0.960477 Loss: 0.002025\n",
      "[284/350] 108.44 sec(s) Train Acc: 0.961229 Loss: 0.001906\n",
      "[285/350] 108.35 sec(s) Train Acc: 0.962319 Loss: 0.001865\n",
      "[286/350] 108.37 sec(s) Train Acc: 0.960289 Loss: 0.002077\n",
      "[287/350] 108.46 sec(s) Train Acc: 0.961793 Loss: 0.001944\n",
      "[288/350] 108.51 sec(s) Train Acc: 0.962508 Loss: 0.001882\n",
      "[289/350] 108.52 sec(s) Train Acc: 0.960815 Loss: 0.001941\n",
      "[290/350] 108.43 sec(s) Train Acc: 0.961831 Loss: 0.001915\n",
      "[291/350] 108.42 sec(s) Train Acc: 0.962094 Loss: 0.001896\n",
      "[292/350] 108.39 sec(s) Train Acc: 0.961680 Loss: 0.001944\n",
      "[293/350] 108.42 sec(s) Train Acc: 0.966042 Loss: 0.001805\n",
      "[294/350] 108.46 sec(s) Train Acc: 0.959273 Loss: 0.002017\n",
      "[295/350] 108.41 sec(s) Train Acc: 0.962319 Loss: 0.001957\n",
      "[296/350] 108.42 sec(s) Train Acc: 0.961304 Loss: 0.001935\n",
      "[297/350] 108.41 sec(s) Train Acc: 0.960552 Loss: 0.001929\n",
      "[298/350] 108.47 sec(s) Train Acc: 0.961868 Loss: 0.001799\n",
      "[299/350] 108.41 sec(s) Train Acc: 0.963372 Loss: 0.001842\n",
      "[300/350] 108.32 sec(s) Train Acc: 0.961868 Loss: 0.001837\n",
      "[301/350] 108.42 sec(s) Train Acc: 0.961379 Loss: 0.001869\n",
      "[302/350] 108.52 sec(s) Train Acc: 0.963260 Loss: 0.001833\n",
      "[303/350] 108.56 sec(s) Train Acc: 0.961868 Loss: 0.001893\n",
      "[304/350] 108.51 sec(s) Train Acc: 0.961003 Loss: 0.001923\n",
      "[305/350] 108.50 sec(s) Train Acc: 0.963937 Loss: 0.001741\n",
      "[306/350] 108.46 sec(s) Train Acc: 0.964162 Loss: 0.001850\n",
      "[307/350] 108.53 sec(s) Train Acc: 0.964012 Loss: 0.001858\n",
      "[308/350] 108.57 sec(s) Train Acc: 0.962921 Loss: 0.001863\n",
      "[309/350] 108.53 sec(s) Train Acc: 0.964012 Loss: 0.001787\n",
      "[310/350] 108.63 sec(s) Train Acc: 0.963034 Loss: 0.001842\n",
      "[311/350] 108.50 sec(s) Train Acc: 0.963109 Loss: 0.001856\n",
      "[312/350] 108.57 sec(s) Train Acc: 0.963786 Loss: 0.001810\n",
      "[313/350] 108.49 sec(s) Train Acc: 0.961981 Loss: 0.001856\n",
      "[314/350] 108.50 sec(s) Train Acc: 0.962846 Loss: 0.001845\n",
      "[315/350] 108.47 sec(s) Train Acc: 0.963824 Loss: 0.001838\n",
      "[316/350] 108.50 sec(s) Train Acc: 0.964275 Loss: 0.001810\n",
      "[317/350] 108.83 sec(s) Train Acc: 0.964839 Loss: 0.001819\n",
      "[318/350] 108.84 sec(s) Train Acc: 0.964463 Loss: 0.001771\n",
      "[319/350] 108.81 sec(s) Train Acc: 0.964125 Loss: 0.001793\n",
      "[320/350] 108.82 sec(s) Train Acc: 0.962959 Loss: 0.001811\n",
      "[321/350] 108.83 sec(s) Train Acc: 0.963372 Loss: 0.001821\n",
      "[322/350] 108.84 sec(s) Train Acc: 0.965065 Loss: 0.001744\n",
      "[323/350] 109.24 sec(s) Train Acc: 0.965854 Loss: 0.001761\n",
      "[324/350] 108.78 sec(s) Train Acc: 0.964801 Loss: 0.001770\n",
      "[325/350] 108.91 sec(s) Train Acc: 0.965290 Loss: 0.001749\n",
      "[326/350] 108.78 sec(s) Train Acc: 0.965441 Loss: 0.001668\n",
      "[327/350] 108.86 sec(s) Train Acc: 0.965742 Loss: 0.001746\n",
      "[328/350] 108.79 sec(s) Train Acc: 0.965930 Loss: 0.001705\n",
      "[329/350] 108.82 sec(s) Train Acc: 0.963974 Loss: 0.001794\n",
      "[330/350] 108.71 sec(s) Train Acc: 0.963372 Loss: 0.001812\n",
      "[331/350] 108.73 sec(s) Train Acc: 0.966268 Loss: 0.001727\n",
      "[332/350] 108.77 sec(s) Train Acc: 0.966983 Loss: 0.001600\n",
      "[333/350] 108.67 sec(s) Train Acc: 0.966343 Loss: 0.001674\n",
      "[334/350] 108.53 sec(s) Train Acc: 0.965441 Loss: 0.001743\n",
      "[335/350] 108.47 sec(s) Train Acc: 0.964538 Loss: 0.001736\n",
      "[336/350] 108.58 sec(s) Train Acc: 0.964425 Loss: 0.001759\n",
      "[337/350] 108.50 sec(s) Train Acc: 0.964200 Loss: 0.001762\n",
      "[338/350] 108.37 sec(s) Train Acc: 0.966080 Loss: 0.001719\n",
      "[339/350] 108.36 sec(s) Train Acc: 0.964425 Loss: 0.001796\n",
      "[340/350] 108.48 sec(s) Train Acc: 0.964989 Loss: 0.001765\n",
      "[341/350] 108.38 sec(s) Train Acc: 0.965328 Loss: 0.001730\n",
      "[342/350] 108.34 sec(s) Train Acc: 0.964914 Loss: 0.001749\n",
      "[343/350] 108.43 sec(s) Train Acc: 0.965742 Loss: 0.001750\n",
      "[344/350] 108.54 sec(s) Train Acc: 0.967359 Loss: 0.001626\n",
      "[345/350] 108.68 sec(s) Train Acc: 0.966268 Loss: 0.001670\n",
      "[346/350] 108.41 sec(s) Train Acc: 0.966343 Loss: 0.001724\n",
      "[347/350] 108.40 sec(s) Train Acc: 0.968186 Loss: 0.001628\n",
      "[348/350] 108.36 sec(s) Train Acc: 0.966795 Loss: 0.001678\n",
      "[349/350] 108.48 sec(s) Train Acc: 0.963636 Loss: 0.001766\n",
      "[350/350] 108.47 sec(s) Train Acc: 0.966757 Loss: 0.001624\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "# model_best = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 350\n",
    "\n",
    "# use apex to optimize\n",
    "# model_best, optimizer = amp.initialize(model_best, optimizer, opt_level=\"O3\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "#         train_pred = model_best(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model_best, 'model.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2o1oCMXy61_3"
   },
   "source": [
    "# Testing\n",
    "利用剛剛 train 好的 model 進行 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAR6sn8U661G"
   },
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HznI9_-ocrq"
   },
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "#         test_pred = model_best(data.cpu())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t2q2Th85ZUE"
   },
   "outputs": [],
   "source": [
    "#將結果寫入 csv 檔\n",
    "with open(\"predict.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 釋放記憶體\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
