{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_a2USyd4giE"
   },
   "source": [
    "# **Homework 3 - Convolutional Neural Network**\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhzdomRTOKoJ"
   },
   "outputs": [],
   "source": [
    "# !gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip # 下載資料集\n",
    "# !unzip food-11.zip # 解壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sVrKci4PUFW"
   },
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from apex import amp\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0i9ZCPrOVN_"
   },
   "source": [
    "#Read image\n",
    "利用 OpenCV (cv2) 讀入照片並存放在 numpy array 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf7QPifJQNUK"
   },
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to 128 x ? or ? x 128\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = 128 / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = 128, 128\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ebVIY5HQQH7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "# 分別將 training set、validation set、testing set 用 readfile 函式讀進來\n",
    "workspace_dir = './food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gq5KVMM3OHY6"
   },
   "source": [
    "# Dataset\n",
    "在 PyTorch 中，我們可以利用 torch.utils.data 的 Dataset 及 DataLoader 來\"包裝\" data，使後續的 training 及 testing 更為方便。\n",
    "\n",
    "Dataset 需要 overload 兩個函數：\\_\\_len\\_\\_ 及 \\_\\_getitem\\_\\_\n",
    "\n",
    "\\_\\_len\\_\\_ 必須要回傳 dataset 的大小，而 \\_\\_getitem\\_\\_ 則定義了當程式利用 [ ] 取值時，dataset 應該要怎麼回傳資料。\n",
    "\n",
    "實際上我們並不會直接使用到這兩個函數，但是使用 DataLoader 在 enumerate Dataset 時會使用到，沒有實做的話會在程式運行階段出現 error。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKd2abixQghI"
   },
   "outputs": [],
   "source": [
    "# training 時做 data augmentation\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective()\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomRotation(40)\n",
    "    ]),\n",
    "    transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomOrder([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomPerspective()\n",
    "        ]),\n",
    "        transforms.RandomAffine(10), # 隨機線性轉換\n",
    "        transforms.RandomResizedCrop((128, 128), scale=(0.8, 1.0)), # 隨機子圖\n",
    "    ]),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(), # 隨機色溫等\n",
    "        transforms.RandomGrayscale(),\n",
    "    ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "    transforms.RandomErasing(0.2)\n",
    "])\n",
    "# testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qz6jeMnkQl0_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9YhZo7POPYG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1c-GwrMQqMl"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(3, 128, 5, 1, 3),  # [3, 128, 128]\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(3, 128, 3, 1, 1),  # [64, 128, 128]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "            \n",
    "            nn.Dropout2d(0.5),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(1),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "            \n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 100),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(100, 30),\n",
    "            nn.PReLU(1),\n",
    "\n",
    "            nn.Linear(30, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEnGbriXORN3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5x-FH2Kr_jh"
   },
   "source": [
    "使用 training set 訓練，並使用 validation set 尋找好的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHaFE-8oQtkC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:440: UserWarning: torch.gels is deprecated in favour of torch.lstsq and will be removed in the next release. Please use torch.lstsq instead.\n",
      "  res = torch.gels(B, A)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] 85.26 sec(s) Train Acc: 0.231603 Loss: 0.033751 | Val Acc: 0.283965 loss: 0.031214\n",
      "[002/300] 84.91 sec(s) Train Acc: 0.279749 Loss: 0.032084 | Val Acc: 0.332070 loss: 0.030581\n",
      "[003/300] 84.92 sec(s) Train Acc: 0.317505 Loss: 0.030593 | Val Acc: 0.328863 loss: 0.029614\n",
      "[004/300] 84.98 sec(s) Train Acc: 0.340817 Loss: 0.029438 | Val Acc: 0.363557 loss: 0.027770\n",
      "[005/300] 84.75 sec(s) Train Acc: 0.369451 Loss: 0.028267 | Val Acc: 0.428280 loss: 0.026063\n",
      "[006/300] 84.85 sec(s) Train Acc: 0.404825 Loss: 0.026986 | Val Acc: 0.444606 loss: 0.024675\n",
      "[007/300] 85.04 sec(s) Train Acc: 0.433306 Loss: 0.025772 | Val Acc: 0.478717 loss: 0.023667\n",
      "[008/300] 84.94 sec(s) Train Acc: 0.456416 Loss: 0.024808 | Val Acc: 0.475219 loss: 0.024637\n",
      "[009/300] 84.92 sec(s) Train Acc: 0.476637 Loss: 0.023954 | Val Acc: 0.519825 loss: 0.022081\n",
      "[010/300] 84.82 sec(s) Train Acc: 0.498682 Loss: 0.022874 | Val Acc: 0.509913 loss: 0.022766\n",
      "[011/300] 84.96 sec(s) Train Acc: 0.516825 Loss: 0.022174 | Val Acc: 0.533819 loss: 0.021801\n",
      "[012/300] 84.85 sec(s) Train Acc: 0.534360 Loss: 0.021335 | Val Acc: 0.548105 loss: 0.020949\n",
      "[013/300] 84.80 sec(s) Train Acc: 0.551287 Loss: 0.020613 | Val Acc: 0.574344 loss: 0.019913\n",
      "[014/300] 84.95 sec(s) Train Acc: 0.564160 Loss: 0.020067 | Val Acc: 0.585423 loss: 0.019493\n",
      "[015/300] 84.92 sec(s) Train Acc: 0.575715 Loss: 0.019347 | Val Acc: 0.571429 loss: 0.019955\n",
      "[016/300] 84.80 sec(s) Train Acc: 0.591374 Loss: 0.018751 | Val Acc: 0.613120 loss: 0.018243\n",
      "[017/300] 84.94 sec(s) Train Acc: 0.608656 Loss: 0.018223 | Val Acc: 0.589504 loss: 0.019060\n",
      "[018/300] 84.91 sec(s) Train Acc: 0.615244 Loss: 0.017603 | Val Acc: 0.609621 loss: 0.018446\n",
      "[019/300] 84.91 sec(s) Train Acc: 0.630549 Loss: 0.017032 | Val Acc: 0.588921 loss: 0.019939\n",
      "[020/300] 84.93 sec(s) Train Acc: 0.643878 Loss: 0.016508 | Val Acc: 0.614286 loss: 0.018824\n",
      "[021/300] 84.98 sec(s) Train Acc: 0.657004 Loss: 0.015993 | Val Acc: 0.596501 loss: 0.019672\n",
      "[022/300] 84.90 sec(s) Train Acc: 0.661818 Loss: 0.015663 | Val Acc: 0.619242 loss: 0.018892\n",
      "[023/300] 84.97 sec(s) Train Acc: 0.676262 Loss: 0.015048 | Val Acc: 0.633236 loss: 0.018136\n",
      "[024/300] 84.95 sec(s) Train Acc: 0.684168 Loss: 0.014674 | Val Acc: 0.627405 loss: 0.018445\n",
      "[025/300] 85.04 sec(s) Train Acc: 0.693848 Loss: 0.014052 | Val Acc: 0.641399 loss: 0.019050\n",
      "[026/300] 84.88 sec(s) Train Acc: 0.700740 Loss: 0.013819 | Val Acc: 0.644606 loss: 0.018469\n",
      "[027/300] 84.93 sec(s) Train Acc: 0.713004 Loss: 0.013282 | Val Acc: 0.650437 loss: 0.017611\n",
      "[028/300] 84.94 sec(s) Train Acc: 0.726181 Loss: 0.012721 | Val Acc: 0.641983 loss: 0.018046\n",
      "[029/300] 84.97 sec(s) Train Acc: 0.732566 Loss: 0.012410 | Val Acc: 0.658017 loss: 0.017752\n",
      "[030/300] 84.92 sec(s) Train Acc: 0.739155 Loss: 0.012190 | Val Acc: 0.668513 loss: 0.016768\n",
      "[031/300] 84.97 sec(s) Train Acc: 0.751520 Loss: 0.011644 | Val Acc: 0.668805 loss: 0.016962\n",
      "[032/300] 85.00 sec(s) Train Acc: 0.754764 Loss: 0.011281 | Val Acc: 0.669971 loss: 0.018275\n",
      "[033/300] 85.06 sec(s) Train Acc: 0.769968 Loss: 0.010836 | Val Acc: 0.662099 loss: 0.017381\n",
      "[034/300] 84.98 sec(s) Train Acc: 0.778988 Loss: 0.010364 | Val Acc: 0.668805 loss: 0.017280\n",
      "[035/300] 85.11 sec(s) Train Acc: 0.784563 Loss: 0.010237 | Val Acc: 0.660350 loss: 0.018547\n",
      "[036/300] 85.06 sec(s) Train Acc: 0.786742 Loss: 0.010054 | Val Acc: 0.669388 loss: 0.017889\n",
      "[037/300] 85.02 sec(s) Train Acc: 0.793736 Loss: 0.009550 | Val Acc: 0.676676 loss: 0.018384\n",
      "[038/300] 85.01 sec(s) Train Acc: 0.802453 Loss: 0.009311 | Val Acc: 0.671137 loss: 0.018146\n",
      "[039/300] 84.99 sec(s) Train Acc: 0.808028 Loss: 0.009021 | Val Acc: 0.659475 loss: 0.019478\n",
      "[040/300] 84.91 sec(s) Train Acc: 0.815427 Loss: 0.008768 | Val Acc: 0.665889 loss: 0.019185\n",
      "[041/300] 84.96 sec(s) Train Acc: 0.824701 Loss: 0.008334 | Val Acc: 0.669388 loss: 0.018954\n",
      "[042/300] 84.98 sec(s) Train Acc: 0.827438 Loss: 0.008138 | Val Acc: 0.678134 loss: 0.018736\n",
      "[043/300] 85.03 sec(s) Train Acc: 0.832708 Loss: 0.007908 | Val Acc: 0.697376 loss: 0.018482\n",
      "[044/300] 85.00 sec(s) Train Acc: 0.836611 Loss: 0.007574 | Val Acc: 0.669971 loss: 0.018837\n",
      "[045/300] 84.92 sec(s) Train Acc: 0.843807 Loss: 0.007402 | Val Acc: 0.690671 loss: 0.019560\n",
      "[046/300] 84.91 sec(s) Train Acc: 0.847963 Loss: 0.007296 | Val Acc: 0.682799 loss: 0.020323\n",
      "[047/300] 84.98 sec(s) Train Acc: 0.850294 Loss: 0.007142 | Val Acc: 0.686880 loss: 0.018648\n",
      "[048/300] 84.85 sec(s) Train Acc: 0.859163 Loss: 0.006828 | Val Acc: 0.664140 loss: 0.020435\n",
      "[049/300] 84.96 sec(s) Train Acc: 0.859264 Loss: 0.006814 | Val Acc: 0.667930 loss: 0.021430\n",
      "[050/300] 84.97 sec(s) Train Acc: 0.865143 Loss: 0.006465 | Val Acc: 0.682799 loss: 0.020051\n",
      "[051/300] 84.96 sec(s) Train Acc: 0.871782 Loss: 0.006141 | Val Acc: 0.689213 loss: 0.018804\n",
      "[052/300] 84.87 sec(s) Train Acc: 0.873049 Loss: 0.006173 | Val Acc: 0.704665 loss: 0.019340\n",
      "[053/300] 84.88 sec(s) Train Acc: 0.877205 Loss: 0.005909 | Val Acc: 0.685714 loss: 0.021555\n",
      "[054/300] 84.91 sec(s) Train Acc: 0.878319 Loss: 0.006009 | Val Acc: 0.695918 loss: 0.019821\n",
      "[055/300] 84.87 sec(s) Train Acc: 0.881715 Loss: 0.005818 | Val Acc: 0.685423 loss: 0.019305\n",
      "[056/300] 84.90 sec(s) Train Acc: 0.889266 Loss: 0.005441 | Val Acc: 0.690671 loss: 0.020554\n",
      "[057/300] 84.91 sec(s) Train Acc: 0.887340 Loss: 0.005478 | Val Acc: 0.699125 loss: 0.020461\n",
      "[058/300] 84.89 sec(s) Train Acc: 0.887340 Loss: 0.005446 | Val Acc: 0.693878 loss: 0.019952\n",
      "[059/300] 84.84 sec(s) Train Acc: 0.889520 Loss: 0.005339 | Val Acc: 0.677551 loss: 0.022046\n",
      "[060/300] 84.85 sec(s) Train Acc: 0.895297 Loss: 0.005060 | Val Acc: 0.697959 loss: 0.019725\n",
      "[061/300] 84.91 sec(s) Train Acc: 0.893675 Loss: 0.005106 | Val Acc: 0.682507 loss: 0.020986\n",
      "[062/300] 84.96 sec(s) Train Acc: 0.897983 Loss: 0.004893 | Val Acc: 0.697668 loss: 0.019725\n",
      "[063/300] 84.88 sec(s) Train Acc: 0.901480 Loss: 0.004899 | Val Acc: 0.697376 loss: 0.020873\n",
      "[064/300] 84.83 sec(s) Train Acc: 0.904318 Loss: 0.004681 | Val Acc: 0.693878 loss: 0.020987\n",
      "[065/300] 84.83 sec(s) Train Acc: 0.902493 Loss: 0.004800 | Val Acc: 0.698251 loss: 0.020445\n",
      "[066/300] 84.96 sec(s) Train Acc: 0.909234 Loss: 0.004400 | Val Acc: 0.695044 loss: 0.021040\n",
      "[067/300] 84.83 sec(s) Train Acc: 0.909943 Loss: 0.004387 | Val Acc: 0.683382 loss: 0.022603\n",
      "[068/300] 84.82 sec(s) Train Acc: 0.909588 Loss: 0.004404 | Val Acc: 0.680466 loss: 0.023165\n",
      "[069/300] 84.87 sec(s) Train Acc: 0.913693 Loss: 0.004303 | Val Acc: 0.696793 loss: 0.021234\n",
      "[070/300] 84.87 sec(s) Train Acc: 0.914606 Loss: 0.004235 | Val Acc: 0.687755 loss: 0.021612\n",
      "[071/300] 84.82 sec(s) Train Acc: 0.912731 Loss: 0.004318 | Val Acc: 0.693294 loss: 0.021234\n",
      "[072/300] 84.88 sec(s) Train Acc: 0.919116 Loss: 0.004022 | Val Acc: 0.700292 loss: 0.022539\n",
      "[073/300] 84.80 sec(s) Train Acc: 0.920332 Loss: 0.003936 | Val Acc: 0.708455 loss: 0.021529\n",
      "[074/300] 84.93 sec(s) Train Acc: 0.920687 Loss: 0.003861 | Val Acc: 0.702915 loss: 0.022714\n",
      "[075/300] 84.93 sec(s) Train Acc: 0.920484 Loss: 0.003877 | Val Acc: 0.706706 loss: 0.022363\n",
      "[076/300] 85.03 sec(s) Train Acc: 0.919420 Loss: 0.004007 | Val Acc: 0.698542 loss: 0.021165\n",
      "[077/300] 84.93 sec(s) Train Acc: 0.922512 Loss: 0.003847 | Val Acc: 0.687464 loss: 0.023772\n",
      "[078/300] 84.84 sec(s) Train Acc: 0.921295 Loss: 0.003847 | Val Acc: 0.698251 loss: 0.022456\n",
      "[079/300] 84.93 sec(s) Train Acc: 0.925907 Loss: 0.003659 | Val Acc: 0.710787 loss: 0.021856\n",
      "[080/300] 84.92 sec(s) Train Acc: 0.925096 Loss: 0.003759 | Val Acc: 0.691837 loss: 0.022316\n",
      "[081/300] 85.04 sec(s) Train Acc: 0.926921 Loss: 0.003619 | Val Acc: 0.702332 loss: 0.020944\n",
      "[082/300] 84.89 sec(s) Train Acc: 0.927732 Loss: 0.003582 | Val Acc: 0.695044 loss: 0.023537\n",
      "[083/300] 84.83 sec(s) Train Acc: 0.928036 Loss: 0.003510 | Val Acc: 0.691837 loss: 0.022072\n",
      "[084/300] 84.93 sec(s) Train Acc: 0.928492 Loss: 0.003510 | Val Acc: 0.707289 loss: 0.020096\n",
      "[085/300] 84.86 sec(s) Train Acc: 0.932546 Loss: 0.003374 | Val Acc: 0.701749 loss: 0.022754\n",
      "[086/300] 85.08 sec(s) Train Acc: 0.932394 Loss: 0.003389 | Val Acc: 0.702915 loss: 0.023912\n",
      "[087/300] 84.96 sec(s) Train Acc: 0.930823 Loss: 0.003441 | Val Acc: 0.700875 loss: 0.022160\n",
      "[088/300] 84.93 sec(s) Train Acc: 0.931887 Loss: 0.003370 | Val Acc: 0.702915 loss: 0.023163\n",
      "[089/300] 85.02 sec(s) Train Acc: 0.934016 Loss: 0.003282 | Val Acc: 0.693878 loss: 0.024386\n",
      "[090/300] 85.49 sec(s) Train Acc: 0.937310 Loss: 0.003149 | Val Acc: 0.711662 loss: 0.022556\n",
      "[091/300] 85.11 sec(s) Train Acc: 0.934725 Loss: 0.003321 | Val Acc: 0.697085 loss: 0.022633\n",
      "[092/300] 85.17 sec(s) Train Acc: 0.933357 Loss: 0.003275 | Val Acc: 0.711953 loss: 0.021484\n",
      "[093/300] 85.16 sec(s) Train Acc: 0.938577 Loss: 0.003144 | Val Acc: 0.709038 loss: 0.021794\n",
      "[094/300] 85.06 sec(s) Train Acc: 0.937158 Loss: 0.003147 | Val Acc: 0.713703 loss: 0.022536\n",
      "[095/300] 85.13 sec(s) Train Acc: 0.939895 Loss: 0.003006 | Val Acc: 0.707580 loss: 0.025398\n",
      "[096/300] 85.11 sec(s) Train Acc: 0.935435 Loss: 0.003192 | Val Acc: 0.715452 loss: 0.021953\n",
      "[097/300] 85.12 sec(s) Train Acc: 0.941972 Loss: 0.002920 | Val Acc: 0.697085 loss: 0.022603\n",
      "[098/300] 84.97 sec(s) Train Acc: 0.944101 Loss: 0.002913 | Val Acc: 0.710496 loss: 0.023768\n",
      "[099/300] 85.10 sec(s) Train Acc: 0.942226 Loss: 0.002847 | Val Acc: 0.702332 loss: 0.023989\n",
      "[100/300] 85.08 sec(s) Train Acc: 0.938425 Loss: 0.003062 | Val Acc: 0.702624 loss: 0.022632\n",
      "[101/300] 85.05 sec(s) Train Acc: 0.944101 Loss: 0.002864 | Val Acc: 0.707289 loss: 0.022670\n",
      "[102/300] 84.87 sec(s) Train Acc: 0.940401 Loss: 0.002944 | Val Acc: 0.702041 loss: 0.024390\n",
      "[103/300] 84.94 sec(s) Train Acc: 0.941466 Loss: 0.002946 | Val Acc: 0.707289 loss: 0.021425\n",
      "[104/300] 84.91 sec(s) Train Acc: 0.945672 Loss: 0.002677 | Val Acc: 0.705831 loss: 0.022681\n",
      "[105/300] 84.95 sec(s) Train Acc: 0.943239 Loss: 0.002909 | Val Acc: 0.708455 loss: 0.022402\n",
      "[106/300] 84.82 sec(s) Train Acc: 0.944506 Loss: 0.002714 | Val Acc: 0.716327 loss: 0.023458\n",
      "[107/300] 84.82 sec(s) Train Acc: 0.944760 Loss: 0.002723 | Val Acc: 0.706706 loss: 0.023116\n",
      "[108/300] 84.96 sec(s) Train Acc: 0.945976 Loss: 0.002752 | Val Acc: 0.717201 loss: 0.022927\n",
      "[109/300] 84.91 sec(s) Train Acc: 0.948409 Loss: 0.002585 | Val Acc: 0.699125 loss: 0.023924\n",
      "[110/300] 84.96 sec(s) Train Acc: 0.947040 Loss: 0.002709 | Val Acc: 0.705831 loss: 0.023033\n",
      "[111/300] 84.95 sec(s) Train Acc: 0.945165 Loss: 0.002673 | Val Acc: 0.706997 loss: 0.023998\n",
      "[112/300] 84.86 sec(s) Train Acc: 0.947598 Loss: 0.002588 | Val Acc: 0.707289 loss: 0.023763\n",
      "[113/300] 84.98 sec(s) Train Acc: 0.945013 Loss: 0.002756 | Val Acc: 0.705248 loss: 0.023716\n",
      "[114/300] 84.90 sec(s) Train Acc: 0.946838 Loss: 0.002555 | Val Acc: 0.704373 loss: 0.024017\n",
      "[115/300] 85.03 sec(s) Train Acc: 0.947648 Loss: 0.002658 | Val Acc: 0.705539 loss: 0.022556\n",
      "[116/300] 84.83 sec(s) Train Acc: 0.949169 Loss: 0.002580 | Val Acc: 0.705831 loss: 0.023423\n",
      "[117/300] 84.96 sec(s) Train Acc: 0.946888 Loss: 0.002659 | Val Acc: 0.704956 loss: 0.023888\n",
      "[118/300] 84.91 sec(s) Train Acc: 0.949929 Loss: 0.002554 | Val Acc: 0.704956 loss: 0.024278\n",
      "[119/300] 84.93 sec(s) Train Acc: 0.950537 Loss: 0.002458 | Val Acc: 0.712245 loss: 0.023135\n",
      "[120/300] 84.93 sec(s) Train Acc: 0.950334 Loss: 0.002483 | Val Acc: 0.711079 loss: 0.025050\n",
      "[121/300] 84.92 sec(s) Train Acc: 0.949118 Loss: 0.002492 | Val Acc: 0.710787 loss: 0.024840\n",
      "[122/300] 85.00 sec(s) Train Acc: 0.951095 Loss: 0.002447 | Val Acc: 0.707872 loss: 0.023747\n",
      "[123/300] 84.92 sec(s) Train Acc: 0.952615 Loss: 0.002457 | Val Acc: 0.699708 loss: 0.025050\n",
      "[124/300] 84.90 sec(s) Train Acc: 0.950284 Loss: 0.002468 | Val Acc: 0.706706 loss: 0.025174\n",
      "[125/300] 84.96 sec(s) Train Acc: 0.950334 Loss: 0.002479 | Val Acc: 0.685423 loss: 0.025769\n",
      "[126/300] 84.96 sec(s) Train Acc: 0.953527 Loss: 0.002336 | Val Acc: 0.702624 loss: 0.025489\n",
      "[127/300] 85.02 sec(s) Train Acc: 0.953223 Loss: 0.002401 | Val Acc: 0.718950 loss: 0.022115\n",
      "[128/300] 85.03 sec(s) Train Acc: 0.951247 Loss: 0.002432 | Val Acc: 0.704373 loss: 0.024021\n",
      "[129/300] 84.98 sec(s) Train Acc: 0.951753 Loss: 0.002432 | Val Acc: 0.708455 loss: 0.024568\n",
      "[130/300] 84.95 sec(s) Train Acc: 0.952818 Loss: 0.002413 | Val Acc: 0.701458 loss: 0.026191\n",
      "[131/300] 84.99 sec(s) Train Acc: 0.954997 Loss: 0.002316 | Val Acc: 0.709621 loss: 0.023887\n",
      "[132/300] 84.96 sec(s) Train Acc: 0.954845 Loss: 0.002327 | Val Acc: 0.706122 loss: 0.021826\n",
      "[133/300] 85.03 sec(s) Train Acc: 0.955250 Loss: 0.002230 | Val Acc: 0.719534 loss: 0.022421\n",
      "[134/300] 84.96 sec(s) Train Acc: 0.955605 Loss: 0.002235 | Val Acc: 0.710496 loss: 0.025306\n",
      "[135/300] 85.05 sec(s) Train Acc: 0.952058 Loss: 0.002366 | Val Acc: 0.717784 loss: 0.023467\n",
      "[136/300] 84.87 sec(s) Train Acc: 0.957683 Loss: 0.002200 | Val Acc: 0.718076 loss: 0.022422\n",
      "[137/300] 84.94 sec(s) Train Acc: 0.955554 Loss: 0.002243 | Val Acc: 0.711370 loss: 0.022663\n",
      "[138/300] 84.98 sec(s) Train Acc: 0.956112 Loss: 0.002254 | Val Acc: 0.714869 loss: 0.022725\n",
      "[139/300] 84.99 sec(s) Train Acc: 0.955909 Loss: 0.002218 | Val Acc: 0.711370 loss: 0.024947\n",
      "[140/300] 85.00 sec(s) Train Acc: 0.956973 Loss: 0.002166 | Val Acc: 0.712828 loss: 0.025651\n",
      "[141/300] 84.97 sec(s) Train Acc: 0.957075 Loss: 0.002148 | Val Acc: 0.719825 loss: 0.022415\n",
      "[142/300] 84.94 sec(s) Train Acc: 0.959457 Loss: 0.001998 | Val Acc: 0.713703 loss: 0.024402\n",
      "[143/300] 85.04 sec(s) Train Acc: 0.956517 Loss: 0.002182 | Val Acc: 0.725073 loss: 0.023675\n",
      "[144/300] 84.87 sec(s) Train Acc: 0.956669 Loss: 0.002150 | Val Acc: 0.724198 loss: 0.023361\n",
      "[145/300] 84.90 sec(s) Train Acc: 0.955554 Loss: 0.002204 | Val Acc: 0.717493 loss: 0.024501\n",
      "[146/300] 84.88 sec(s) Train Acc: 0.957278 Loss: 0.002248 | Val Acc: 0.720700 loss: 0.023412\n",
      "[147/300] 85.00 sec(s) Train Acc: 0.956213 Loss: 0.002173 | Val Acc: 0.717784 loss: 0.022912\n",
      "[148/300] 84.89 sec(s) Train Acc: 0.959761 Loss: 0.002084 | Val Acc: 0.716035 loss: 0.024178\n",
      "[149/300] 84.91 sec(s) Train Acc: 0.958038 Loss: 0.002094 | Val Acc: 0.718659 loss: 0.023650\n",
      "[150/300] 84.91 sec(s) Train Acc: 0.957886 Loss: 0.002091 | Val Acc: 0.711662 loss: 0.025085\n",
      "[151/300] 84.97 sec(s) Train Acc: 0.960673 Loss: 0.002066 | Val Acc: 0.722157 loss: 0.023907\n",
      "[152/300] 84.96 sec(s) Train Acc: 0.957328 Loss: 0.002213 | Val Acc: 0.701458 loss: 0.025894\n",
      "[153/300] 84.96 sec(s) Train Acc: 0.958139 Loss: 0.002174 | Val Acc: 0.717493 loss: 0.024447\n",
      "[154/300] 84.83 sec(s) Train Acc: 0.961433 Loss: 0.001947 | Val Acc: 0.716910 loss: 0.024224\n",
      "[155/300] 84.82 sec(s) Train Acc: 0.963258 Loss: 0.001918 | Val Acc: 0.723615 loss: 0.024167\n",
      "[156/300] 84.81 sec(s) Train Acc: 0.961230 Loss: 0.002031 | Val Acc: 0.719534 loss: 0.023310\n",
      "[157/300] 84.81 sec(s) Train Acc: 0.960116 Loss: 0.002053 | Val Acc: 0.710787 loss: 0.023086\n",
      "[158/300] 84.91 sec(s) Train Acc: 0.963663 Loss: 0.001859 | Val Acc: 0.716910 loss: 0.023724\n",
      "[159/300] 84.83 sec(s) Train Acc: 0.961687 Loss: 0.001996 | Val Acc: 0.715160 loss: 0.024126\n",
      "[160/300] 84.76 sec(s) Train Acc: 0.956517 Loss: 0.002103 | Val Acc: 0.711953 loss: 0.023379\n",
      "[161/300] 84.80 sec(s) Train Acc: 0.964373 Loss: 0.001872 | Val Acc: 0.729738 loss: 0.022033\n",
      "[162/300] 84.75 sec(s) Train Acc: 0.962852 Loss: 0.001912 | Val Acc: 0.703207 loss: 0.028302\n",
      "[163/300] 84.87 sec(s) Train Acc: 0.959659 Loss: 0.002003 | Val Acc: 0.722741 loss: 0.023154\n",
      "[164/300] 84.87 sec(s) Train Acc: 0.960825 Loss: 0.001995 | Val Acc: 0.715452 loss: 0.026025\n",
      "[165/300] 84.88 sec(s) Train Acc: 0.961028 Loss: 0.001980 | Val Acc: 0.723907 loss: 0.021785\n",
      "[166/300] 84.91 sec(s) Train Acc: 0.957328 Loss: 0.002146 | Val Acc: 0.725073 loss: 0.024481\n",
      "[167/300] 84.79 sec(s) Train Acc: 0.962345 Loss: 0.001884 | Val Acc: 0.702624 loss: 0.024431\n",
      "[168/300] 84.86 sec(s) Train Acc: 0.963714 Loss: 0.001869 | Val Acc: 0.712828 loss: 0.025878\n",
      "[169/300] 84.91 sec(s) Train Acc: 0.959710 Loss: 0.002003 | Val Acc: 0.727405 loss: 0.023082\n",
      "[170/300] 84.87 sec(s) Train Acc: 0.963207 Loss: 0.001805 | Val Acc: 0.722157 loss: 0.023684\n",
      "[171/300] 84.77 sec(s) Train Acc: 0.962295 Loss: 0.001953 | Val Acc: 0.716035 loss: 0.023364\n",
      "[172/300] 84.91 sec(s) Train Acc: 0.962092 Loss: 0.001959 | Val Acc: 0.725364 loss: 0.023727\n",
      "[173/300] 84.94 sec(s) Train Acc: 0.961433 Loss: 0.001946 | Val Acc: 0.719242 loss: 0.023305\n",
      "[174/300] 84.87 sec(s) Train Acc: 0.963055 Loss: 0.001870 | Val Acc: 0.723615 loss: 0.022649\n",
      "[175/300] 84.83 sec(s) Train Acc: 0.964727 Loss: 0.001847 | Val Acc: 0.714286 loss: 0.023757\n",
      "[176/300] 84.94 sec(s) Train Acc: 0.965589 Loss: 0.001768 | Val Acc: 0.722449 loss: 0.025842\n",
      "[177/300] 85.00 sec(s) Train Acc: 0.962599 Loss: 0.001873 | Val Acc: 0.719825 loss: 0.024389\n",
      "[178/300] 84.99 sec(s) Train Acc: 0.960977 Loss: 0.002023 | Val Acc: 0.718659 loss: 0.024062\n",
      "[179/300] 85.39 sec(s) Train Acc: 0.964221 Loss: 0.001813 | Val Acc: 0.706122 loss: 0.024479\n",
      "[180/300] 85.04 sec(s) Train Acc: 0.966197 Loss: 0.001724 | Val Acc: 0.733528 loss: 0.023432\n",
      "[181/300] 85.19 sec(s) Train Acc: 0.965437 Loss: 0.001748 | Val Acc: 0.727988 loss: 0.022531\n",
      "[182/300] 85.12 sec(s) Train Acc: 0.963460 Loss: 0.001852 | Val Acc: 0.725948 loss: 0.022498\n",
      "[183/300] 85.10 sec(s) Train Acc: 0.962497 Loss: 0.001854 | Val Acc: 0.716327 loss: 0.024995\n",
      "[184/300] 85.09 sec(s) Train Acc: 0.963106 Loss: 0.001828 | Val Acc: 0.697959 loss: 0.026223\n",
      "[185/300] 85.09 sec(s) Train Acc: 0.964119 Loss: 0.001845 | Val Acc: 0.718076 loss: 0.025081\n",
      "[186/300] 85.26 sec(s) Train Acc: 0.963663 Loss: 0.001795 | Val Acc: 0.717201 loss: 0.025144\n",
      "[187/300] 85.11 sec(s) Train Acc: 0.964677 Loss: 0.001758 | Val Acc: 0.723615 loss: 0.022442\n",
      "[188/300] 85.14 sec(s) Train Acc: 0.966907 Loss: 0.001743 | Val Acc: 0.710496 loss: 0.027278\n",
      "[189/300] 84.96 sec(s) Train Acc: 0.967261 Loss: 0.001681 | Val Acc: 0.722157 loss: 0.026397\n",
      "[190/300] 85.09 sec(s) Train Acc: 0.966197 Loss: 0.001785 | Val Acc: 0.734402 loss: 0.022860\n",
      "[191/300] 84.83 sec(s) Train Acc: 0.962447 Loss: 0.001873 | Val Acc: 0.714577 loss: 0.025303\n",
      "[192/300] 84.81 sec(s) Train Acc: 0.965285 Loss: 0.001871 | Val Acc: 0.730612 loss: 0.023140\n",
      "[193/300] 84.87 sec(s) Train Acc: 0.966298 Loss: 0.001650 | Val Acc: 0.723615 loss: 0.023644\n",
      "[194/300] 84.80 sec(s) Train Acc: 0.965792 Loss: 0.001743 | Val Acc: 0.730612 loss: 0.023061\n",
      "[195/300] 84.89 sec(s) Train Acc: 0.962802 Loss: 0.001902 | Val Acc: 0.725073 loss: 0.021220\n",
      "[196/300] 84.85 sec(s) Train Acc: 0.967413 Loss: 0.001649 | Val Acc: 0.733819 loss: 0.024985\n",
      "[197/300] 84.86 sec(s) Train Acc: 0.967616 Loss: 0.001741 | Val Acc: 0.732070 loss: 0.023483\n",
      "[198/300] 84.81 sec(s) Train Acc: 0.968072 Loss: 0.001666 | Val Acc: 0.734694 loss: 0.023951\n",
      "[199/300] 84.86 sec(s) Train Acc: 0.966805 Loss: 0.001727 | Val Acc: 0.725656 loss: 0.023870\n",
      "[200/300] 84.90 sec(s) Train Acc: 0.967464 Loss: 0.001686 | Val Acc: 0.716618 loss: 0.025414\n",
      "[201/300] 84.88 sec(s) Train Acc: 0.966704 Loss: 0.001710 | Val Acc: 0.730029 loss: 0.024106\n",
      "[202/300] 84.84 sec(s) Train Acc: 0.967008 Loss: 0.001725 | Val Acc: 0.719825 loss: 0.024766\n",
      "[203/300] 84.80 sec(s) Train Acc: 0.965082 Loss: 0.001708 | Val Acc: 0.714869 loss: 0.029595\n",
      "[204/300] 84.94 sec(s) Train Acc: 0.965386 Loss: 0.001721 | Val Acc: 0.729446 loss: 0.023134\n",
      "[205/300] 84.91 sec(s) Train Acc: 0.969086 Loss: 0.001632 | Val Acc: 0.697085 loss: 0.025740\n",
      "[206/300] 84.86 sec(s) Train Acc: 0.966907 Loss: 0.001723 | Val Acc: 0.711953 loss: 0.024843\n",
      "[207/300] 84.82 sec(s) Train Acc: 0.968731 Loss: 0.001628 | Val Acc: 0.722449 loss: 0.023586\n",
      "[208/300] 84.89 sec(s) Train Acc: 0.968174 Loss: 0.001641 | Val Acc: 0.712245 loss: 0.024458\n",
      "[209/300] 85.05 sec(s) Train Acc: 0.965944 Loss: 0.001765 | Val Acc: 0.722157 loss: 0.022714\n",
      "[210/300] 84.96 sec(s) Train Acc: 0.966957 Loss: 0.001655 | Val Acc: 0.706997 loss: 0.027284\n",
      "[211/300] 84.95 sec(s) Train Acc: 0.965285 Loss: 0.001742 | Val Acc: 0.726531 loss: 0.025339\n",
      "[212/300] 84.89 sec(s) Train Acc: 0.964474 Loss: 0.001797 | Val Acc: 0.728863 loss: 0.022700\n",
      "[213/300] 84.88 sec(s) Train Acc: 0.969390 Loss: 0.001525 | Val Acc: 0.721866 loss: 0.022980\n",
      "[214/300] 84.79 sec(s) Train Acc: 0.967565 Loss: 0.001658 | Val Acc: 0.728571 loss: 0.023587\n",
      "[215/300] 84.75 sec(s) Train Acc: 0.970606 Loss: 0.001463 | Val Acc: 0.735860 loss: 0.024350\n",
      "[216/300] 84.87 sec(s) Train Acc: 0.970454 Loss: 0.001545 | Val Acc: 0.728571 loss: 0.025698\n",
      "[217/300] 84.93 sec(s) Train Acc: 0.969694 Loss: 0.001517 | Val Acc: 0.719825 loss: 0.025775\n",
      "[218/300] 84.91 sec(s) Train Acc: 0.967717 Loss: 0.001678 | Val Acc: 0.724490 loss: 0.021785\n",
      "[219/300] 85.04 sec(s) Train Acc: 0.966704 Loss: 0.001693 | Val Acc: 0.729155 loss: 0.026128\n",
      "[220/300] 84.85 sec(s) Train Acc: 0.967059 Loss: 0.001629 | Val Acc: 0.725656 loss: 0.022742\n",
      "[221/300] 84.88 sec(s) Train Acc: 0.967717 Loss: 0.001630 | Val Acc: 0.732653 loss: 0.025665\n",
      "[222/300] 84.93 sec(s) Train Acc: 0.970150 Loss: 0.001512 | Val Acc: 0.730612 loss: 0.023894\n",
      "[223/300] 84.91 sec(s) Train Acc: 0.970353 Loss: 0.001526 | Val Acc: 0.735277 loss: 0.025337\n",
      "[224/300] 84.85 sec(s) Train Acc: 0.967565 Loss: 0.001603 | Val Acc: 0.723324 loss: 0.023926\n",
      "[225/300] 84.90 sec(s) Train Acc: 0.969238 Loss: 0.001583 | Val Acc: 0.724198 loss: 0.026112\n",
      "[226/300] 84.90 sec(s) Train Acc: 0.970707 Loss: 0.001476 | Val Acc: 0.718950 loss: 0.023661\n",
      "[227/300] 84.89 sec(s) Train Acc: 0.970454 Loss: 0.001548 | Val Acc: 0.729446 loss: 0.025148\n",
      "[228/300] 84.92 sec(s) Train Acc: 0.966400 Loss: 0.001721 | Val Acc: 0.725948 loss: 0.024437\n",
      "[229/300] 84.87 sec(s) Train Acc: 0.968731 Loss: 0.001547 | Val Acc: 0.727988 loss: 0.023900\n",
      "[230/300] 84.93 sec(s) Train Acc: 0.971012 Loss: 0.001546 | Val Acc: 0.735860 loss: 0.023205\n",
      "[231/300] 84.97 sec(s) Train Acc: 0.969947 Loss: 0.001480 | Val Acc: 0.739942 loss: 0.023340\n",
      "[232/300] 84.95 sec(s) Train Acc: 0.970353 Loss: 0.001475 | Val Acc: 0.720408 loss: 0.024714\n",
      "[233/300] 84.87 sec(s) Train Acc: 0.970606 Loss: 0.001479 | Val Acc: 0.727697 loss: 0.025086\n",
      "[234/300] 84.93 sec(s) Train Acc: 0.970707 Loss: 0.001467 | Val Acc: 0.723032 loss: 0.028425\n",
      "[235/300] 84.93 sec(s) Train Acc: 0.972684 Loss: 0.001442 | Val Acc: 0.731195 loss: 0.025552\n",
      "[236/300] 84.91 sec(s) Train Acc: 0.971012 Loss: 0.001588 | Val Acc: 0.731195 loss: 0.024583\n",
      "[237/300] 84.90 sec(s) Train Acc: 0.969694 Loss: 0.001503 | Val Acc: 0.731778 loss: 0.025194\n",
      "[238/300] 84.92 sec(s) Train Acc: 0.970353 Loss: 0.001543 | Val Acc: 0.724198 loss: 0.022753\n",
      "[239/300] 84.91 sec(s) Train Acc: 0.969136 Loss: 0.001537 | Val Acc: 0.732653 loss: 0.023741\n",
      "[240/300] 84.87 sec(s) Train Acc: 0.971822 Loss: 0.001465 | Val Acc: 0.720117 loss: 0.026191\n",
      "[241/300] 84.89 sec(s) Train Acc: 0.970201 Loss: 0.001581 | Val Acc: 0.729446 loss: 0.021774\n",
      "[242/300] 84.89 sec(s) Train Acc: 0.971468 Loss: 0.001444 | Val Acc: 0.726822 loss: 0.025248\n",
      "[243/300] 84.90 sec(s) Train Acc: 0.970049 Loss: 0.001494 | Val Acc: 0.721283 loss: 0.023057\n",
      "[244/300] 84.96 sec(s) Train Acc: 0.971822 Loss: 0.001430 | Val Acc: 0.722741 loss: 0.025644\n",
      "[245/300] 84.96 sec(s) Train Acc: 0.972279 Loss: 0.001455 | Val Acc: 0.725364 loss: 0.024888\n",
      "[246/300] 84.94 sec(s) Train Acc: 0.972481 Loss: 0.001411 | Val Acc: 0.730904 loss: 0.026285\n",
      "[247/300] 84.89 sec(s) Train Acc: 0.972228 Loss: 0.001448 | Val Acc: 0.722741 loss: 0.025545\n",
      "[248/300] 84.81 sec(s) Train Acc: 0.973343 Loss: 0.001404 | Val Acc: 0.722449 loss: 0.026052\n",
      "[249/300] 84.96 sec(s) Train Acc: 0.970555 Loss: 0.001547 | Val Acc: 0.717201 loss: 0.025413\n",
      "[250/300] 84.90 sec(s) Train Acc: 0.971214 Loss: 0.001424 | Val Acc: 0.738776 loss: 0.024532\n",
      "[251/300] 84.96 sec(s) Train Acc: 0.971214 Loss: 0.001463 | Val Acc: 0.721283 loss: 0.025753\n",
      "[252/300] 84.97 sec(s) Train Acc: 0.966805 Loss: 0.001610 | Val Acc: 0.723032 loss: 0.022703\n",
      "[253/300] 84.91 sec(s) Train Acc: 0.972735 Loss: 0.001427 | Val Acc: 0.734111 loss: 0.026204\n",
      "[254/300] 84.99 sec(s) Train Acc: 0.972431 Loss: 0.001437 | Val Acc: 0.731778 loss: 0.025691\n",
      "[255/300] 84.96 sec(s) Train Acc: 0.971062 Loss: 0.001477 | Val Acc: 0.729155 loss: 0.025950\n",
      "[256/300] 85.00 sec(s) Train Acc: 0.971012 Loss: 0.001509 | Val Acc: 0.715452 loss: 0.022938\n",
      "[257/300] 85.09 sec(s) Train Acc: 0.971721 Loss: 0.001385 | Val Acc: 0.725656 loss: 0.024162\n",
      "[258/300] 84.92 sec(s) Train Acc: 0.973241 Loss: 0.001392 | Val Acc: 0.716327 loss: 0.027254\n",
      "[259/300] 84.92 sec(s) Train Acc: 0.974002 Loss: 0.001367 | Val Acc: 0.728571 loss: 0.023093\n",
      "[260/300] 85.03 sec(s) Train Acc: 0.973698 Loss: 0.001393 | Val Acc: 0.723032 loss: 0.022236\n",
      "[261/300] 84.87 sec(s) Train Acc: 0.970150 Loss: 0.001532 | Val Acc: 0.734402 loss: 0.023398\n",
      "[262/300] 84.96 sec(s) Train Acc: 0.973191 Loss: 0.001322 | Val Acc: 0.723615 loss: 0.025839\n",
      "[263/300] 84.88 sec(s) Train Acc: 0.974306 Loss: 0.001315 | Val Acc: 0.724490 loss: 0.025686\n",
      "[264/300] 84.88 sec(s) Train Acc: 0.975674 Loss: 0.001283 | Val Acc: 0.725073 loss: 0.024418\n",
      "[265/300] 85.03 sec(s) Train Acc: 0.973951 Loss: 0.001335 | Val Acc: 0.716910 loss: 0.024458\n",
      "[266/300] 84.98 sec(s) Train Acc: 0.972583 Loss: 0.001412 | Val Acc: 0.727405 loss: 0.022375\n",
      "[267/300] 84.96 sec(s) Train Acc: 0.971670 Loss: 0.001421 | Val Acc: 0.726239 loss: 0.023874\n",
      "[268/300] 84.89 sec(s) Train Acc: 0.972937 Loss: 0.001365 | Val Acc: 0.732945 loss: 0.023589\n",
      "[269/300] 84.92 sec(s) Train Acc: 0.974863 Loss: 0.001286 | Val Acc: 0.739359 loss: 0.022171\n",
      "[270/300] 84.94 sec(s) Train Acc: 0.973393 Loss: 0.001419 | Val Acc: 0.726822 loss: 0.020723\n",
      "[271/300] 84.93 sec(s) Train Acc: 0.973140 Loss: 0.001318 | Val Acc: 0.722449 loss: 0.029851\n",
      "[272/300] 84.89 sec(s) Train Acc: 0.974356 Loss: 0.001290 | Val Acc: 0.726822 loss: 0.030147\n",
      "[273/300] 84.91 sec(s) Train Acc: 0.974559 Loss: 0.001280 | Val Acc: 0.721574 loss: 0.024027\n",
      "[274/300] 84.91 sec(s) Train Acc: 0.972329 Loss: 0.001379 | Val Acc: 0.739067 loss: 0.022739\n",
      "[275/300] 84.89 sec(s) Train Acc: 0.974812 Loss: 0.001306 | Val Acc: 0.724490 loss: 0.023583\n",
      "[276/300] 84.89 sec(s) Train Acc: 0.973039 Loss: 0.001409 | Val Acc: 0.741691 loss: 0.023068\n",
      "[277/300] 84.88 sec(s) Train Acc: 0.975623 Loss: 0.001271 | Val Acc: 0.732653 loss: 0.023804\n",
      "[278/300] 84.96 sec(s) Train Acc: 0.974407 Loss: 0.001296 | Val Acc: 0.729155 loss: 0.024449\n",
      "[279/300] 85.00 sec(s) Train Acc: 0.974002 Loss: 0.001316 | Val Acc: 0.733236 loss: 0.020182\n",
      "[280/300] 84.96 sec(s) Train Acc: 0.974052 Loss: 0.001348 | Val Acc: 0.725364 loss: 0.026658\n",
      "[281/300] 84.84 sec(s) Train Acc: 0.973596 Loss: 0.001346 | Val Acc: 0.736152 loss: 0.023129\n",
      "[282/300] 84.98 sec(s) Train Acc: 0.974458 Loss: 0.001310 | Val Acc: 0.730321 loss: 0.025485\n",
      "[283/300] 84.92 sec(s) Train Acc: 0.973900 Loss: 0.001352 | Val Acc: 0.732945 loss: 0.025849\n",
      "[284/300] 84.94 sec(s) Train Acc: 0.976941 Loss: 0.001185 | Val Acc: 0.732945 loss: 0.023780\n",
      "[285/300] 84.90 sec(s) Train Acc: 0.975978 Loss: 0.001289 | Val Acc: 0.732070 loss: 0.025615\n",
      "[286/300] 85.01 sec(s) Train Acc: 0.973444 Loss: 0.001360 | Val Acc: 0.730321 loss: 0.024227\n",
      "[287/300] 84.86 sec(s) Train Acc: 0.975269 Loss: 0.001279 | Val Acc: 0.737026 loss: 0.023349\n",
      "[288/300] 84.99 sec(s) Train Acc: 0.972785 Loss: 0.001368 | Val Acc: 0.729738 loss: 0.020261\n",
      "[289/300] 84.88 sec(s) Train Acc: 0.976434 Loss: 0.001157 | Val Acc: 0.733236 loss: 0.026365\n",
      "[290/300] 85.01 sec(s) Train Acc: 0.974863 Loss: 0.001290 | Val Acc: 0.730029 loss: 0.025137\n",
      "[291/300] 85.02 sec(s) Train Acc: 0.975319 Loss: 0.001301 | Val Acc: 0.731487 loss: 0.025464\n",
      "[292/300] 84.83 sec(s) Train Acc: 0.973292 Loss: 0.001417 | Val Acc: 0.729446 loss: 0.023428\n",
      "[293/300] 84.86 sec(s) Train Acc: 0.974204 Loss: 0.001326 | Val Acc: 0.727988 loss: 0.024554\n",
      "[294/300] 84.82 sec(s) Train Acc: 0.977093 Loss: 0.001183 | Val Acc: 0.733528 loss: 0.024823\n",
      "[295/300] 85.07 sec(s) Train Acc: 0.974508 Loss: 0.001317 | Val Acc: 0.731487 loss: 0.023643\n",
      "[296/300] 84.87 sec(s) Train Acc: 0.975319 Loss: 0.001243 | Val Acc: 0.734111 loss: 0.025100\n",
      "[297/300] 84.86 sec(s) Train Acc: 0.974965 Loss: 0.001232 | Val Acc: 0.730904 loss: 0.023414\n",
      "[298/300] 85.04 sec(s) Train Acc: 0.976282 Loss: 0.001274 | Val Acc: 0.725073 loss: 0.025673\n",
      "[299/300] 84.84 sec(s) Train Acc: 0.977397 Loss: 0.001201 | Val Acc: 0.745481 loss: 0.026600\n",
      "[300/300] 84.82 sec(s) Train Acc: 0.975826 Loss: 0.001277 | Val Acc: 0.736735 loss: 0.024002\n"
     ]
    }
   ],
   "source": [
    "model = Classifier().cuda()\n",
    "# model = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 300\n",
    "\n",
    "# # use apex to optimize\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "#         train_pred = model(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "#             val_pred = model(data[0].cpu())\n",
    "#             batch_loss = loss(val_pred, data[1].cpu())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-ssSxXlsI_T"
   },
   "source": [
    "得到好的參數後，我們使用 training set 和 validation set 共同訓練（資料量變多，模型效果較好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKoUxLun8lFG"
   },
   "outputs": [],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ConcatDataset([\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform1),\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform2),\n",
    "])\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoAS5TtRsfOo",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/250] 106.97 sec(s) Train Acc: 0.230520 Loss: 0.033811\n",
      "[002/250] 107.50 sec(s) Train Acc: 0.288658 Loss: 0.031306\n",
      "[003/250] 107.75 sec(s) Train Acc: 0.335214 Loss: 0.029508\n",
      "[004/250] 107.67 sec(s) Train Acc: 0.377820 Loss: 0.027924\n",
      "[005/250] 108.67 sec(s) Train Acc: 0.419073 Loss: 0.026260\n",
      "[006/250] 107.79 sec(s) Train Acc: 0.449835 Loss: 0.025025\n",
      "[007/250] 107.91 sec(s) Train Acc: 0.476722 Loss: 0.023769\n",
      "[008/250] 108.00 sec(s) Train Acc: 0.507483 Loss: 0.022662\n",
      "[009/250] 107.82 sec(s) Train Acc: 0.524519 Loss: 0.021819\n",
      "[010/250] 107.86 sec(s) Train Acc: 0.542080 Loss: 0.021008\n",
      "[011/250] 107.98 sec(s) Train Acc: 0.561221 Loss: 0.020157\n",
      "[012/250] 107.77 sec(s) Train Acc: 0.573368 Loss: 0.019602\n",
      "[013/250] 107.92 sec(s) Train Acc: 0.593487 Loss: 0.018683\n",
      "[014/250] 107.71 sec(s) Train Acc: 0.613906 Loss: 0.018037\n",
      "[015/250] 107.70 sec(s) Train Acc: 0.623383 Loss: 0.017454\n",
      "[016/250] 107.55 sec(s) Train Acc: 0.639516 Loss: 0.016814\n",
      "[017/250] 107.73 sec(s) Train Acc: 0.648090 Loss: 0.016392\n",
      "[018/250] 107.61 sec(s) Train Acc: 0.663997 Loss: 0.015670\n",
      "[019/250] 107.73 sec(s) Train Acc: 0.671631 Loss: 0.015427\n",
      "[020/250] 107.58 sec(s) Train Acc: 0.680205 Loss: 0.014902\n",
      "[021/250] 107.61 sec(s) Train Acc: 0.695886 Loss: 0.014124\n",
      "[022/250] 108.17 sec(s) Train Acc: 0.704122 Loss: 0.013889\n",
      "[023/250] 107.93 sec(s) Train Acc: 0.713974 Loss: 0.013310\n",
      "[024/250] 107.81 sec(s) Train Acc: 0.723714 Loss: 0.012886\n",
      "[025/250] 107.93 sec(s) Train Acc: 0.732062 Loss: 0.012452\n",
      "[026/250] 107.91 sec(s) Train Acc: 0.739282 Loss: 0.012286\n",
      "[027/250] 107.90 sec(s) Train Acc: 0.750978 Loss: 0.011674\n",
      "[028/250] 107.99 sec(s) Train Acc: 0.760567 Loss: 0.011312\n",
      "[029/250] 107.84 sec(s) Train Acc: 0.766546 Loss: 0.011067\n",
      "[030/250] 107.73 sec(s) Train Acc: 0.769517 Loss: 0.010708\n",
      "[031/250] 107.65 sec(s) Train Acc: 0.778317 Loss: 0.010345\n",
      "[032/250] 107.70 sec(s) Train Acc: 0.789110 Loss: 0.009916\n",
      "[033/250] 107.77 sec(s) Train Acc: 0.796443 Loss: 0.009635\n",
      "[034/250] 107.57 sec(s) Train Acc: 0.804001 Loss: 0.009330\n",
      "[035/250] 107.80 sec(s) Train Acc: 0.807611 Loss: 0.009087\n",
      "[036/250] 107.77 sec(s) Train Acc: 0.820623 Loss: 0.008508\n",
      "[037/250] 107.71 sec(s) Train Acc: 0.825098 Loss: 0.008340\n",
      "[038/250] 107.67 sec(s) Train Acc: 0.833484 Loss: 0.008069\n",
      "[039/250] 107.61 sec(s) Train Acc: 0.835665 Loss: 0.007786\n",
      "[040/250] 107.72 sec(s) Train Acc: 0.839877 Loss: 0.007528\n",
      "[041/250] 107.69 sec(s) Train Acc: 0.845743 Loss: 0.007477\n",
      "[042/250] 107.66 sec(s) Train Acc: 0.851271 Loss: 0.007046\n",
      "[043/250] 107.74 sec(s) Train Acc: 0.856912 Loss: 0.006973\n",
      "[044/250] 107.68 sec(s) Train Acc: 0.860033 Loss: 0.006811\n",
      "[045/250] 107.68 sec(s) Train Acc: 0.865147 Loss: 0.006487\n",
      "[046/250] 107.67 sec(s) Train Acc: 0.871992 Loss: 0.006278\n",
      "[047/250] 107.66 sec(s) Train Acc: 0.872142 Loss: 0.006186\n",
      "[048/250] 107.75 sec(s) Train Acc: 0.873345 Loss: 0.006226\n",
      "[049/250] 107.80 sec(s) Train Acc: 0.879813 Loss: 0.005925\n",
      "[050/250] 107.71 sec(s) Train Acc: 0.881092 Loss: 0.005768\n",
      "[051/250] 107.59 sec(s) Train Acc: 0.882822 Loss: 0.005644\n",
      "[052/250] 107.74 sec(s) Train Acc: 0.886846 Loss: 0.005486\n",
      "[053/250] 107.72 sec(s) Train Acc: 0.890381 Loss: 0.005252\n",
      "[054/250] 107.72 sec(s) Train Acc: 0.889403 Loss: 0.005355\n",
      "[055/250] 107.77 sec(s) Train Acc: 0.893727 Loss: 0.005209\n",
      "[056/250] 107.80 sec(s) Train Acc: 0.896999 Loss: 0.005032\n",
      "[057/250] 107.70 sec(s) Train Acc: 0.901324 Loss: 0.004862\n",
      "[058/250] 107.80 sec(s) Train Acc: 0.902001 Loss: 0.004804\n",
      "[059/250] 107.75 sec(s) Train Acc: 0.905423 Loss: 0.004608\n",
      "[060/250] 107.59 sec(s) Train Acc: 0.904896 Loss: 0.004619\n",
      "[061/250] 107.82 sec(s) Train Acc: 0.908882 Loss: 0.004477\n",
      "[062/250] 107.65 sec(s) Train Acc: 0.908055 Loss: 0.004422\n",
      "[063/250] 107.72 sec(s) Train Acc: 0.912906 Loss: 0.004318\n",
      "[064/250] 107.79 sec(s) Train Acc: 0.911515 Loss: 0.004342\n",
      "[065/250] 107.65 sec(s) Train Acc: 0.911440 Loss: 0.004319\n",
      "[066/250] 107.72 sec(s) Train Acc: 0.916140 Loss: 0.004141\n",
      "[067/250] 107.78 sec(s) Train Acc: 0.918359 Loss: 0.004010\n",
      "[068/250] 107.67 sec(s) Train Acc: 0.918171 Loss: 0.004058\n",
      "[069/250] 107.63 sec(s) Train Acc: 0.915952 Loss: 0.004130\n",
      "[070/250] 107.81 sec(s) Train Acc: 0.919525 Loss: 0.003922\n",
      "[071/250] 107.77 sec(s) Train Acc: 0.923586 Loss: 0.003807\n",
      "[072/250] 107.78 sec(s) Train Acc: 0.923436 Loss: 0.003725\n",
      "[073/250] 107.72 sec(s) Train Acc: 0.924639 Loss: 0.003613\n",
      "[074/250] 107.81 sec(s) Train Acc: 0.926369 Loss: 0.003634\n",
      "[075/250] 107.81 sec(s) Train Acc: 0.926482 Loss: 0.003684\n",
      "[076/250] 107.80 sec(s) Train Acc: 0.924526 Loss: 0.003760\n",
      "[077/250] 107.64 sec(s) Train Acc: 0.929490 Loss: 0.003439\n",
      "[078/250] 107.64 sec(s) Train Acc: 0.928926 Loss: 0.003563\n",
      "[079/250] 107.68 sec(s) Train Acc: 0.932386 Loss: 0.003353\n",
      "[080/250] 107.59 sec(s) Train Acc: 0.928362 Loss: 0.003561\n",
      "[081/250] 107.73 sec(s) Train Acc: 0.929528 Loss: 0.003507\n",
      "[082/250] 107.60 sec(s) Train Acc: 0.933326 Loss: 0.003225\n",
      "[083/250] 107.54 sec(s) Train Acc: 0.933213 Loss: 0.003313\n",
      "[084/250] 108.26 sec(s) Train Acc: 0.934604 Loss: 0.003219\n",
      "[085/250] 108.37 sec(s) Train Acc: 0.933965 Loss: 0.003254\n",
      "[086/250] 108.18 sec(s) Train Acc: 0.935657 Loss: 0.003250\n",
      "[087/250] 108.25 sec(s) Train Acc: 0.935281 Loss: 0.003204\n",
      "[088/250] 108.26 sec(s) Train Acc: 0.935657 Loss: 0.003244\n",
      "[089/250] 108.25 sec(s) Train Acc: 0.942313 Loss: 0.002997\n",
      "[090/250] 108.16 sec(s) Train Acc: 0.936861 Loss: 0.003102\n",
      "[091/250] 108.22 sec(s) Train Acc: 0.936259 Loss: 0.003156\n",
      "[092/250] 108.22 sec(s) Train Acc: 0.937350 Loss: 0.003131\n",
      "[093/250] 108.86 sec(s) Train Acc: 0.937274 Loss: 0.002987\n",
      "[094/250] 108.39 sec(s) Train Acc: 0.939079 Loss: 0.003043\n",
      "[095/250] 107.93 sec(s) Train Acc: 0.942953 Loss: 0.002869\n",
      "[096/250] 107.94 sec(s) Train Acc: 0.939907 Loss: 0.002935\n",
      "[097/250] 107.96 sec(s) Train Acc: 0.943291 Loss: 0.002878\n",
      "[098/250] 108.00 sec(s) Train Acc: 0.942802 Loss: 0.002883\n",
      "[099/250] 108.02 sec(s) Train Acc: 0.944043 Loss: 0.002754\n",
      "[100/250] 107.91 sec(s) Train Acc: 0.942878 Loss: 0.002796\n",
      "[101/250] 107.92 sec(s) Train Acc: 0.944043 Loss: 0.002792\n",
      "[102/250] 107.95 sec(s) Train Acc: 0.945961 Loss: 0.002732\n",
      "[103/250] 107.75 sec(s) Train Acc: 0.945021 Loss: 0.002772\n",
      "[104/250] 107.71 sec(s) Train Acc: 0.945924 Loss: 0.002698\n",
      "[105/250] 107.64 sec(s) Train Acc: 0.946036 Loss: 0.002722\n",
      "[106/250] 107.80 sec(s) Train Acc: 0.946676 Loss: 0.002697\n",
      "[107/250] 107.59 sec(s) Train Acc: 0.946337 Loss: 0.002694\n",
      "[108/250] 107.68 sec(s) Train Acc: 0.944570 Loss: 0.002765\n",
      "[109/250] 107.74 sec(s) Train Acc: 0.947202 Loss: 0.002563\n",
      "[110/250] 107.69 sec(s) Train Acc: 0.948255 Loss: 0.002580\n",
      "[111/250] 107.66 sec(s) Train Acc: 0.950361 Loss: 0.002508\n",
      "[112/250] 107.68 sec(s) Train Acc: 0.948669 Loss: 0.002604\n",
      "[113/250] 107.64 sec(s) Train Acc: 0.948067 Loss: 0.002570\n",
      "[114/250] 107.58 sec(s) Train Acc: 0.947127 Loss: 0.002581\n",
      "[115/250] 107.99 sec(s) Train Acc: 0.950173 Loss: 0.002510\n",
      "[116/250] 108.01 sec(s) Train Acc: 0.949947 Loss: 0.002512\n",
      "[117/250] 107.85 sec(s) Train Acc: 0.950887 Loss: 0.002436\n",
      "[118/250] 107.96 sec(s) Train Acc: 0.948782 Loss: 0.002491\n",
      "[119/250] 107.94 sec(s) Train Acc: 0.947992 Loss: 0.002537\n",
      "[120/250] 107.89 sec(s) Train Acc: 0.952166 Loss: 0.002403\n",
      "[121/250] 107.97 sec(s) Train Acc: 0.950211 Loss: 0.002469\n",
      "[122/250] 108.01 sec(s) Train Acc: 0.954460 Loss: 0.002366\n",
      "[123/250] 107.99 sec(s) Train Acc: 0.953106 Loss: 0.002413\n",
      "[124/250] 107.98 sec(s) Train Acc: 0.954046 Loss: 0.002295\n",
      "[125/250] 108.04 sec(s) Train Acc: 0.954761 Loss: 0.002276\n",
      "[126/250] 107.90 sec(s) Train Acc: 0.953971 Loss: 0.002339\n",
      "[127/250] 107.89 sec(s) Train Acc: 0.952993 Loss: 0.002332\n",
      "[128/250] 107.91 sec(s) Train Acc: 0.954272 Loss: 0.002314\n",
      "[129/250] 107.79 sec(s) Train Acc: 0.957393 Loss: 0.002184\n",
      "[130/250] 107.96 sec(s) Train Acc: 0.957506 Loss: 0.002153\n",
      "[131/250] 107.64 sec(s) Train Acc: 0.954949 Loss: 0.002240\n",
      "[132/250] 107.75 sec(s) Train Acc: 0.954084 Loss: 0.002323\n",
      "[133/250] 107.62 sec(s) Train Acc: 0.954648 Loss: 0.002271\n",
      "[134/250] 107.66 sec(s) Train Acc: 0.955626 Loss: 0.002237\n",
      "[135/250] 107.54 sec(s) Train Acc: 0.955212 Loss: 0.002288\n",
      "[136/250] 107.69 sec(s) Train Acc: 0.956867 Loss: 0.002246\n",
      "[137/250] 107.63 sec(s) Train Acc: 0.958973 Loss: 0.002109\n",
      "[138/250] 107.71 sec(s) Train Acc: 0.956340 Loss: 0.002224\n",
      "[139/250] 107.61 sec(s) Train Acc: 0.957055 Loss: 0.002209\n",
      "[140/250] 107.63 sec(s) Train Acc: 0.955927 Loss: 0.002195\n",
      "[141/250] 107.72 sec(s) Train Acc: 0.958785 Loss: 0.002083\n",
      "[142/250] 107.79 sec(s) Train Acc: 0.958032 Loss: 0.002105\n",
      "[143/250] 107.81 sec(s) Train Acc: 0.958108 Loss: 0.002138\n",
      "[144/250] 107.69 sec(s) Train Acc: 0.959687 Loss: 0.002055\n",
      "[145/250] 107.72 sec(s) Train Acc: 0.957694 Loss: 0.002107\n",
      "[146/250] 107.72 sec(s) Train Acc: 0.957280 Loss: 0.002163\n",
      "[147/250] 107.69 sec(s) Train Acc: 0.958785 Loss: 0.002092\n",
      "[148/250] 107.73 sec(s) Train Acc: 0.959198 Loss: 0.002072\n",
      "[149/250] 107.60 sec(s) Train Acc: 0.960289 Loss: 0.002058\n",
      "[150/250] 107.64 sec(s) Train Acc: 0.961041 Loss: 0.001933\n",
      "[151/250] 107.59 sec(s) Train Acc: 0.957393 Loss: 0.002055\n",
      "[152/250] 107.63 sec(s) Train Acc: 0.957506 Loss: 0.002114\n",
      "[153/250] 107.56 sec(s) Train Acc: 0.959725 Loss: 0.002042\n",
      "[154/250] 107.64 sec(s) Train Acc: 0.958973 Loss: 0.002063\n",
      "[155/250] 107.65 sec(s) Train Acc: 0.961718 Loss: 0.001945\n",
      "[156/250] 107.70 sec(s) Train Acc: 0.959650 Loss: 0.002019\n",
      "[157/250] 107.64 sec(s) Train Acc: 0.963109 Loss: 0.001895\n",
      "[158/250] 107.65 sec(s) Train Acc: 0.960966 Loss: 0.001950\n",
      "[159/250] 107.68 sec(s) Train Acc: 0.960665 Loss: 0.002012\n",
      "[160/250] 107.58 sec(s) Train Acc: 0.962056 Loss: 0.001963\n",
      "[161/250] 107.58 sec(s) Train Acc: 0.961718 Loss: 0.001978\n",
      "[162/250] 107.74 sec(s) Train Acc: 0.962470 Loss: 0.001908\n",
      "[163/250] 107.54 sec(s) Train Acc: 0.959875 Loss: 0.001910\n",
      "[164/250] 107.67 sec(s) Train Acc: 0.963636 Loss: 0.001854\n",
      "[165/250] 107.78 sec(s) Train Acc: 0.960665 Loss: 0.001997\n",
      "[166/250] 107.80 sec(s) Train Acc: 0.964313 Loss: 0.001781\n",
      "[167/250] 107.69 sec(s) Train Acc: 0.961567 Loss: 0.001896\n",
      "[168/250] 107.69 sec(s) Train Acc: 0.965478 Loss: 0.001736\n",
      "[169/250] 107.75 sec(s) Train Acc: 0.965102 Loss: 0.001747\n",
      "[170/250] 107.75 sec(s) Train Acc: 0.964049 Loss: 0.001789\n",
      "[171/250] 107.68 sec(s) Train Acc: 0.964801 Loss: 0.001788\n",
      "[172/250] 107.75 sec(s) Train Acc: 0.960966 Loss: 0.001893\n",
      "[173/250] 107.75 sec(s) Train Acc: 0.961831 Loss: 0.001937\n",
      "[174/250] 107.73 sec(s) Train Acc: 0.964989 Loss: 0.001784\n",
      "[175/250] 107.85 sec(s) Train Acc: 0.965441 Loss: 0.001719\n",
      "[176/250] 107.80 sec(s) Train Acc: 0.964877 Loss: 0.001764\n",
      "[177/250] 107.73 sec(s) Train Acc: 0.966456 Loss: 0.001690\n",
      "[178/250] 107.72 sec(s) Train Acc: 0.963748 Loss: 0.001821\n",
      "[179/250] 107.78 sec(s) Train Acc: 0.963711 Loss: 0.001857\n",
      "[180/250] 107.82 sec(s) Train Acc: 0.964049 Loss: 0.001874\n",
      "[181/250] 107.65 sec(s) Train Acc: 0.962545 Loss: 0.001904\n",
      "[182/250] 107.88 sec(s) Train Acc: 0.964313 Loss: 0.001791\n",
      "[183/250] 107.64 sec(s) Train Acc: 0.966494 Loss: 0.001719\n",
      "[184/250] 107.74 sec(s) Train Acc: 0.965140 Loss: 0.001732\n",
      "[185/250] 107.82 sec(s) Train Acc: 0.964613 Loss: 0.001735\n",
      "[186/250] 107.74 sec(s) Train Acc: 0.966118 Loss: 0.001735\n",
      "[187/250] 107.80 sec(s) Train Acc: 0.963974 Loss: 0.001821\n",
      "[188/250] 107.74 sec(s) Train Acc: 0.965967 Loss: 0.001750\n",
      "[189/250] 107.72 sec(s) Train Acc: 0.967020 Loss: 0.001691\n",
      "[190/250] 107.77 sec(s) Train Acc: 0.968600 Loss: 0.001643\n",
      "[191/250] 107.78 sec(s) Train Acc: 0.966418 Loss: 0.001661\n",
      "[192/250] 107.63 sec(s) Train Acc: 0.966719 Loss: 0.001741\n",
      "[193/250] 107.61 sec(s) Train Acc: 0.964952 Loss: 0.001763\n",
      "[194/250] 107.72 sec(s) Train Acc: 0.967208 Loss: 0.001679\n",
      "[195/250] 107.71 sec(s) Train Acc: 0.966456 Loss: 0.001676\n",
      "[196/250] 107.58 sec(s) Train Acc: 0.967321 Loss: 0.001711\n",
      "[197/250] 107.78 sec(s) Train Acc: 0.966005 Loss: 0.001775\n",
      "[198/250] 107.71 sec(s) Train Acc: 0.968825 Loss: 0.001556\n",
      "[199/250] 107.64 sec(s) Train Acc: 0.965930 Loss: 0.001661\n",
      "[200/250] 107.70 sec(s) Train Acc: 0.967923 Loss: 0.001636\n",
      "[201/250] 107.68 sec(s) Train Acc: 0.967735 Loss: 0.001668\n",
      "[202/250] 107.71 sec(s) Train Acc: 0.966268 Loss: 0.001663\n",
      "[203/250] 107.67 sec(s) Train Acc: 0.969389 Loss: 0.001600\n",
      "[204/250] 107.60 sec(s) Train Acc: 0.965892 Loss: 0.001753\n",
      "[205/250] 107.58 sec(s) Train Acc: 0.969465 Loss: 0.001576\n",
      "[206/250] 107.74 sec(s) Train Acc: 0.968073 Loss: 0.001555\n",
      "[207/250] 107.66 sec(s) Train Acc: 0.965666 Loss: 0.001702\n",
      "[208/250] 107.69 sec(s) Train Acc: 0.970141 Loss: 0.001520\n",
      "[209/250] 107.55 sec(s) Train Acc: 0.969953 Loss: 0.001544\n",
      "[210/250] 107.75 sec(s) Train Acc: 0.970668 Loss: 0.001496\n",
      "[211/250] 108.22 sec(s) Train Acc: 0.968186 Loss: 0.001599\n",
      "[212/250] 107.97 sec(s) Train Acc: 0.970141 Loss: 0.001520\n",
      "[213/250] 107.98 sec(s) Train Acc: 0.970179 Loss: 0.001536\n",
      "[214/250] 107.87 sec(s) Train Acc: 0.969540 Loss: 0.001616\n",
      "[215/250] 108.55 sec(s) Train Acc: 0.964125 Loss: 0.001834\n",
      "[216/250] 108.02 sec(s) Train Acc: 0.969314 Loss: 0.001604\n",
      "[217/250] 108.18 sec(s) Train Acc: 0.968562 Loss: 0.001586\n",
      "[218/250] 107.99 sec(s) Train Acc: 0.970555 Loss: 0.001481\n",
      "[219/250] 108.00 sec(s) Train Acc: 0.968863 Loss: 0.001571\n",
      "[220/250] 108.15 sec(s) Train Acc: 0.968562 Loss: 0.001551\n",
      "[221/250] 107.97 sec(s) Train Acc: 0.971796 Loss: 0.001437\n",
      "[222/250] 109.51 sec(s) Train Acc: 0.971044 Loss: 0.001492\n",
      "[223/250] 109.43 sec(s) Train Acc: 0.970931 Loss: 0.001486\n",
      "[224/250] 108.02 sec(s) Train Acc: 0.971495 Loss: 0.001497\n",
      "[225/250] 108.20 sec(s) Train Acc: 0.970141 Loss: 0.001545\n",
      "[226/250] 108.49 sec(s) Train Acc: 0.968035 Loss: 0.001673\n",
      "[227/250] 108.32 sec(s) Train Acc: 0.970179 Loss: 0.001535\n",
      "[228/250] 108.06 sec(s) Train Acc: 0.971984 Loss: 0.001428\n",
      "[229/250] 108.09 sec(s) Train Acc: 0.970442 Loss: 0.001479\n",
      "[230/250] 108.03 sec(s) Train Acc: 0.971006 Loss: 0.001456\n",
      "[231/250] 108.23 sec(s) Train Acc: 0.972059 Loss: 0.001392\n",
      "[232/250] 108.71 sec(s) Train Acc: 0.970781 Loss: 0.001475\n",
      "[233/250] 108.00 sec(s) Train Acc: 0.971006 Loss: 0.001492\n",
      "[234/250] 108.00 sec(s) Train Acc: 0.968487 Loss: 0.001598\n",
      "[235/250] 107.95 sec(s) Train Acc: 0.971871 Loss: 0.001442\n",
      "[236/250] 107.94 sec(s) Train Acc: 0.971345 Loss: 0.001426\n",
      "[237/250] 108.03 sec(s) Train Acc: 0.972962 Loss: 0.001381\n",
      "[238/250] 109.01 sec(s) Train Acc: 0.970029 Loss: 0.001538\n",
      "[239/250] 108.07 sec(s) Train Acc: 0.973940 Loss: 0.001369\n",
      "[240/250] 108.04 sec(s) Train Acc: 0.971909 Loss: 0.001381\n",
      "[241/250] 108.07 sec(s) Train Acc: 0.973037 Loss: 0.001350\n",
      "[242/250] 107.98 sec(s) Train Acc: 0.972623 Loss: 0.001405\n",
      "[243/250] 107.98 sec(s) Train Acc: 0.971570 Loss: 0.001426\n",
      "[244/250] 108.03 sec(s) Train Acc: 0.970555 Loss: 0.001446\n",
      "[245/250] 107.80 sec(s) Train Acc: 0.973075 Loss: 0.001355\n",
      "[246/250] 107.89 sec(s) Train Acc: 0.972887 Loss: 0.001393\n",
      "[247/250] 108.28 sec(s) Train Acc: 0.972435 Loss: 0.001398\n",
      "[248/250] 107.96 sec(s) Train Acc: 0.970856 Loss: 0.001452\n",
      "[249/250] 108.00 sec(s) Train Acc: 0.972435 Loss: 0.001418\n",
      "[250/250] 107.99 sec(s) Train Acc: 0.974579 Loss: 0.001320\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "# model_best = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 250\n",
    "\n",
    "# use apex to optimize\n",
    "# model_best, optimizer = amp.initialize(model_best, optimizer, opt_level=\"O3\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "#         train_pred = model_best(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model_best, 'model.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2o1oCMXy61_3"
   },
   "source": [
    "# Testing\n",
    "利用剛剛 train 好的 model 進行 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAR6sn8U661G"
   },
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HznI9_-ocrq"
   },
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "#         test_pred = model_best(data.cpu())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t2q2Th85ZUE"
   },
   "outputs": [],
   "source": [
    "#將結果寫入 csv 檔\n",
    "with open(\"predict.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 釋放記憶體\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
