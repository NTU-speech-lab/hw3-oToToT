{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_a2USyd4giE"
   },
   "source": [
    "# **Homework 3 - Convolutional Neural Network**\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhzdomRTOKoJ"
   },
   "outputs": [],
   "source": [
    "# !gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip # 下載資料集\n",
    "# !unzip food-11.zip # 解壓縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sVrKci4PUFW"
   },
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "# from apex import amp\n",
    "import time\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0i9ZCPrOVN_"
   },
   "source": [
    "#Read image\n",
    "利用 OpenCV (cv2) 讀入照片並存放在 numpy array 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf7QPifJQNUK"
   },
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        # resize to 128 x ? or ? x 128\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        rate = 128 / max(height, width)\n",
    "        height = int(height * rate)\n",
    "        width = int(width * rate)\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        # pad black\n",
    "        # from https://blog.csdn.net/qq_20622615/article/details/80929746\n",
    "        W, H = 128, 128\n",
    "        top = (H - height) // 2\n",
    "        bottom = (H - height) // 2\n",
    "        if top + bottom + height < H:\n",
    "            bottom += 1\n",
    "        left = (W - width) // 2\n",
    "        right = (W - width) // 2\n",
    "        if left + right + width < W:\n",
    "            right += 1\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        # to np array\n",
    "        x[i, :, :] = img\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ebVIY5HQQH7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "# 分別將 training set、validation set、testing set 用 readfile 函式讀進來\n",
    "workspace_dir = './food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gq5KVMM3OHY6"
   },
   "source": [
    "# Dataset\n",
    "在 PyTorch 中，我們可以利用 torch.utils.data 的 Dataset 及 DataLoader 來\"包裝\" data，使後續的 training 及 testing 更為方便。\n",
    "\n",
    "Dataset 需要 overload 兩個函數：\\_\\_len\\_\\_ 及 \\_\\_getitem\\_\\_\n",
    "\n",
    "\\_\\_len\\_\\_ 必須要回傳 dataset 的大小，而 \\_\\_getitem\\_\\_ 則定義了當程式利用 [ ] 取值時，dataset 應該要怎麼回傳資料。\n",
    "\n",
    "實際上我們並不會直接使用到這兩個函數，但是使用 DataLoader 在 enumerate Dataset 時會使用到，沒有實做的話會在程式運行階段出現 error。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKd2abixQghI"
   },
   "outputs": [],
   "source": [
    "# training 時做 data augmentation\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "#     transforms.RandomChoice([\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomPerspective()\n",
    "#     ]),\n",
    "#     transforms.RandomChoice([\n",
    "#         transforms.RandomAffine(10), # 隨機線性轉換\n",
    "#         transforms.RandomRotation(40)\n",
    "#     ]),\n",
    "#     transforms.ColorJitter(), # 隨機色溫等\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "#     transforms.Normalize(\n",
    "#         [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "#         [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "#     )\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "#     transforms.RandomOrder([\n",
    "#         transforms.RandomChoice([\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomPerspective()\n",
    "#         ]),\n",
    "#         transforms.RandomAffine(30), # 隨機線性轉換\n",
    "#         transforms.RandomResizedCrop((128, 128), scale=(0.5, 1.0)), # 隨機子圖\n",
    "#     ]),\n",
    "#     transforms.RandomChoice([\n",
    "#         transforms.ColorJitter(), # 隨機色溫等\n",
    "#         transforms.RandomGrayscale(),\n",
    "#     ]),\n",
    "    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization)\n",
    "#     transforms.RandomErasing(0.2),\n",
    "#     transforms.Normalize(\n",
    "#         [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "#         [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "#     )\n",
    "])\n",
    "# testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         [77.89311144813877 / 255, 102.3587941606983 / 255, 126.59376063616554 / 255],\n",
    "#         [72.80305392379675 / 255, 75.35438507973123 / 255, 79.31408066842762 / 255]\n",
    "#     )\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qz6jeMnkQl0_"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = ConcatDataset([\n",
    "    ImgDataset(train_x, train_y, train_transform1),\n",
    "    ImgDataset(train_x, train_y, train_transform2)\n",
    "])\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9YhZo7POPYG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1c-GwrMQqMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 128, 128]           3,584\n",
      "       BatchNorm2d-2        [-1, 128, 128, 128]             256\n",
      "              ReLU-3        [-1, 128, 128, 128]               0\n",
      "         MaxPool2d-4          [-1, 128, 64, 64]               0\n",
      "         Dropout2d-5          [-1, 128, 64, 64]               0\n",
      "            Conv2d-6          [-1, 128, 64, 64]         147,584\n",
      "       BatchNorm2d-7          [-1, 128, 64, 64]             256\n",
      "              ReLU-8          [-1, 128, 64, 64]               0\n",
      "         MaxPool2d-9          [-1, 128, 32, 32]               0\n",
      "           Conv2d-10          [-1, 256, 32, 32]         295,168\n",
      "      BatchNorm2d-11          [-1, 256, 32, 32]             512\n",
      "            PReLU-12          [-1, 256, 32, 32]               1\n",
      "        MaxPool2d-13          [-1, 256, 16, 16]               0\n",
      "        Dropout2d-14          [-1, 256, 16, 16]               0\n",
      "           Conv2d-15          [-1, 512, 16, 16]       1,180,160\n",
      "      BatchNorm2d-16          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-17          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-18            [-1, 512, 8, 8]               0\n",
      "           Conv2d-19            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-20            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-21            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-22            [-1, 512, 4, 4]               0\n",
      "           Linear-23                  [-1, 512]       4,194,816\n",
      "             ReLU-24                  [-1, 512]               0\n",
      "           Linear-25                  [-1, 256]         131,328\n",
      "             ReLU-26                  [-1, 256]               0\n",
      "           Linear-27                  [-1, 128]          32,896\n",
      "          Dropout-28                  [-1, 128]               0\n",
      "             ReLU-29                  [-1, 128]               0\n",
      "           Linear-30                  [-1, 100]          12,900\n",
      "             ReLU-31                  [-1, 100]               0\n",
      "           Linear-32                   [-1, 30]           3,030\n",
      "            PReLU-33                   [-1, 30]               1\n",
      "           Linear-34                   [-1, 11]             341\n",
      "================================================================\n",
      "Total params: 8,364,689\n",
      "Trainable params: 8,364,689\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 80.08\n",
      "Params size (MB): 31.91\n",
      "Estimated Total Size (MB): 112.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(3, 128, 5, 1, 3),  # [3, 128, 128]\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(3, 128, 3, 1, 1),  # [64, 128, 128]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "            \n",
    "            nn.Dropout2d(0.5),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(1),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "            \n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 100),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(100, 30),\n",
    "            nn.PReLU(1),\n",
    "\n",
    "            nn.Linear(30, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)\n",
    "    \n",
    "summary(Classifier().cuda(), (3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEnGbriXORN3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5x-FH2Kr_jh"
   },
   "source": [
    "使用 training set 訓練，並使用 validation set 尋找好的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHaFE-8oQtkC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] 77.69 sec(s) Train Acc: 0.244324 Loss: 0.033043 | Val Acc: 0.238484 loss: 0.033209\n",
      "[002/300] 77.88 sec(s) Train Acc: 0.306254 Loss: 0.030685 | Val Acc: 0.342274 loss: 0.029555\n",
      "[003/300] 77.94 sec(s) Train Acc: 0.355210 Loss: 0.028729 | Val Acc: 0.372012 loss: 0.027895\n",
      "[004/300] 78.02 sec(s) Train Acc: 0.412579 Loss: 0.026301 | Val Acc: 0.444023 loss: 0.025678\n",
      "[005/300] 77.98 sec(s) Train Acc: 0.471366 Loss: 0.023853 | Val Acc: 0.480758 loss: 0.024237\n",
      "[006/300] 77.99 sec(s) Train Acc: 0.517636 Loss: 0.021865 | Val Acc: 0.488338 loss: 0.023791\n",
      "[007/300] 78.07 sec(s) Train Acc: 0.565680 Loss: 0.019769 | Val Acc: 0.506706 loss: 0.023223\n",
      "[008/300] 78.03 sec(s) Train Acc: 0.608301 Loss: 0.017772 | Val Acc: 0.508746 loss: 0.024137\n",
      "[009/300] 78.01 sec(s) Train Acc: 0.652341 Loss: 0.015831 | Val Acc: 0.549563 loss: 0.022705\n",
      "[010/300] 78.02 sec(s) Train Acc: 0.701449 Loss: 0.013590 | Val Acc: 0.536443 loss: 0.023708\n",
      "[011/300] 78.00 sec(s) Train Acc: 0.749290 Loss: 0.011425 | Val Acc: 0.560058 loss: 0.023835\n",
      "[012/300] 78.08 sec(s) Train Acc: 0.793939 Loss: 0.009404 | Val Acc: 0.532070 loss: 0.027561\n",
      "[013/300] 78.04 sec(s) Train Acc: 0.832911 Loss: 0.007733 | Val Acc: 0.540816 loss: 0.028465\n",
      "[014/300] 78.05 sec(s) Train Acc: 0.865700 Loss: 0.006193 | Val Acc: 0.548688 loss: 0.030324\n",
      "[015/300] 78.07 sec(s) Train Acc: 0.892560 Loss: 0.004998 | Val Acc: 0.544898 loss: 0.031039\n",
      "[016/300] 78.02 sec(s) Train Acc: 0.919674 Loss: 0.003768 | Val Acc: 0.534694 loss: 0.037593\n",
      "[017/300] 77.95 sec(s) Train Acc: 0.927833 Loss: 0.003364 | Val Acc: 0.548105 loss: 0.042259\n",
      "[018/300] 78.02 sec(s) Train Acc: 0.942378 Loss: 0.002821 | Val Acc: 0.536152 loss: 0.041009\n",
      "[019/300] 77.97 sec(s) Train Acc: 0.948915 Loss: 0.002522 | Val Acc: 0.551895 loss: 0.038747\n",
      "[020/300] 78.08 sec(s) Train Acc: 0.955402 Loss: 0.002200 | Val Acc: 0.543440 loss: 0.041587\n",
      "[021/300] 78.05 sec(s) Train Acc: 0.965234 Loss: 0.001732 | Val Acc: 0.549563 loss: 0.041304\n",
      "[022/300] 78.00 sec(s) Train Acc: 0.972684 Loss: 0.001328 | Val Acc: 0.536443 loss: 0.051747\n",
      "[023/300] 78.06 sec(s) Train Acc: 0.970150 Loss: 0.001525 | Val Acc: 0.515160 loss: 0.045382\n",
      "[024/300] 78.11 sec(s) Train Acc: 0.969136 Loss: 0.001580 | Val Acc: 0.538192 loss: 0.044111\n",
      "[025/300] 78.01 sec(s) Train Acc: 0.977042 Loss: 0.001238 | Val Acc: 0.548688 loss: 0.045731\n",
      "[026/300] 78.00 sec(s) Train Acc: 0.976992 Loss: 0.001143 | Val Acc: 0.549271 loss: 0.048394\n",
      "[027/300] 78.02 sec(s) Train Acc: 0.978107 Loss: 0.001142 | Val Acc: 0.540233 loss: 0.046120\n",
      "[028/300] 78.03 sec(s) Train Acc: 0.981705 Loss: 0.000965 | Val Acc: 0.555685 loss: 0.045750\n",
      "[029/300] 78.01 sec(s) Train Acc: 0.984087 Loss: 0.000945 | Val Acc: 0.536735 loss: 0.046750\n",
      "[030/300] 78.01 sec(s) Train Acc: 0.980235 Loss: 0.001033 | Val Acc: 0.543149 loss: 0.044404\n",
      "[031/300] 78.04 sec(s) Train Acc: 0.984239 Loss: 0.000844 | Val Acc: 0.539942 loss: 0.047017\n",
      "[032/300] 78.02 sec(s) Train Acc: 0.985455 Loss: 0.000708 | Val Acc: 0.544606 loss: 0.044347\n",
      "[033/300] 78.07 sec(s) Train Acc: 0.984087 Loss: 0.000860 | Val Acc: 0.549271 loss: 0.045759\n",
      "[034/300] 78.02 sec(s) Train Acc: 0.984543 Loss: 0.000844 | Val Acc: 0.548688 loss: 0.045008\n",
      "[035/300] 78.05 sec(s) Train Acc: 0.985404 Loss: 0.000770 | Val Acc: 0.533528 loss: 0.047974\n",
      "[036/300] 78.16 sec(s) Train Acc: 0.981908 Loss: 0.000969 | Val Acc: 0.550437 loss: 0.042647\n",
      "[037/300] 78.04 sec(s) Train Acc: 0.986418 Loss: 0.000760 | Val Acc: 0.555394 loss: 0.046770\n",
      "[038/300] 78.03 sec(s) Train Acc: 0.987381 Loss: 0.000730 | Val Acc: 0.541108 loss: 0.046625\n",
      "[039/300] 78.00 sec(s) Train Acc: 0.985506 Loss: 0.000773 | Val Acc: 0.546064 loss: 0.046834\n",
      "[040/300] 77.97 sec(s) Train Acc: 0.987533 Loss: 0.000658 | Val Acc: 0.558309 loss: 0.043963\n",
      "[041/300] 77.99 sec(s) Train Acc: 0.988547 Loss: 0.000590 | Val Acc: 0.546356 loss: 0.048421\n",
      "[042/300] 77.96 sec(s) Train Acc: 0.986823 Loss: 0.000747 | Val Acc: 0.541691 loss: 0.047333\n",
      "[043/300] 78.01 sec(s) Train Acc: 0.989104 Loss: 0.000569 | Val Acc: 0.525948 loss: 0.051652\n",
      "[044/300] 77.99 sec(s) Train Acc: 0.986773 Loss: 0.000785 | Val Acc: 0.538776 loss: 0.046080\n",
      "[045/300] 77.99 sec(s) Train Acc: 0.990270 Loss: 0.000572 | Val Acc: 0.547522 loss: 0.046401\n",
      "[046/300] 78.02 sec(s) Train Acc: 0.988293 Loss: 0.000606 | Val Acc: 0.547813 loss: 0.047071\n",
      "[047/300] 77.99 sec(s) Train Acc: 0.992956 Loss: 0.000401 | Val Acc: 0.543149 loss: 0.045638\n",
      "[048/300] 78.00 sec(s) Train Acc: 0.989966 Loss: 0.000527 | Val Acc: 0.537609 loss: 0.048458\n",
      "[049/300] 78.01 sec(s) Train Acc: 0.990726 Loss: 0.000556 | Val Acc: 0.555685 loss: 0.045001\n",
      "[050/300] 78.03 sec(s) Train Acc: 0.991587 Loss: 0.000469 | Val Acc: 0.552187 loss: 0.047586\n",
      "[051/300] 78.03 sec(s) Train Acc: 0.990422 Loss: 0.000516 | Val Acc: 0.536735 loss: 0.050219\n",
      "[052/300] 78.03 sec(s) Train Acc: 0.992347 Loss: 0.000437 | Val Acc: 0.525364 loss: 0.051284\n",
      "[053/300] 78.03 sec(s) Train Acc: 0.990979 Loss: 0.000481 | Val Acc: 0.537318 loss: 0.046998\n",
      "[054/300] 78.04 sec(s) Train Acc: 0.990574 Loss: 0.000552 | Val Acc: 0.533528 loss: 0.048548\n",
      "[055/300] 77.96 sec(s) Train Acc: 0.991233 Loss: 0.000475 | Val Acc: 0.539650 loss: 0.048568\n",
      "[056/300] 77.92 sec(s) Train Acc: 0.990168 Loss: 0.000552 | Val Acc: 0.549854 loss: 0.046276\n",
      "[057/300] 77.97 sec(s) Train Acc: 0.991638 Loss: 0.000442 | Val Acc: 0.548397 loss: 0.047835\n",
      "[058/300] 78.00 sec(s) Train Acc: 0.990827 Loss: 0.000516 | Val Acc: 0.553644 loss: 0.044267\n",
      "[059/300] 78.00 sec(s) Train Acc: 0.995946 Loss: 0.000238 | Val Acc: 0.557726 loss: 0.051875\n",
      "[060/300] 78.01 sec(s) Train Acc: 0.995692 Loss: 0.000225 | Val Acc: 0.545190 loss: 0.051164\n",
      "[061/300] 78.03 sec(s) Train Acc: 0.992702 Loss: 0.000482 | Val Acc: 0.545481 loss: 0.045804\n",
      "[062/300] 77.99 sec(s) Train Acc: 0.994071 Loss: 0.000350 | Val Acc: 0.535277 loss: 0.051046\n",
      "[063/300] 78.03 sec(s) Train Acc: 0.989763 Loss: 0.000582 | Val Acc: 0.543149 loss: 0.048255\n",
      "[064/300] 78.01 sec(s) Train Acc: 0.992601 Loss: 0.000437 | Val Acc: 0.555685 loss: 0.049349\n",
      "[065/300] 78.00 sec(s) Train Acc: 0.993158 Loss: 0.000409 | Val Acc: 0.542274 loss: 0.046761\n",
      "[066/300] 78.00 sec(s) Train Acc: 0.996199 Loss: 0.000210 | Val Acc: 0.555102 loss: 0.048645\n",
      "[067/300] 78.01 sec(s) Train Acc: 0.994476 Loss: 0.000346 | Val Acc: 0.565015 loss: 0.049630\n",
      "[068/300] 77.97 sec(s) Train Acc: 0.994273 Loss: 0.000360 | Val Acc: 0.527114 loss: 0.048626\n",
      "[069/300] 77.99 sec(s) Train Acc: 0.993969 Loss: 0.000324 | Val Acc: 0.541399 loss: 0.057873\n",
      "[070/300] 78.00 sec(s) Train Acc: 0.993260 Loss: 0.000417 | Val Acc: 0.543149 loss: 0.051185\n",
      "[071/300] 78.00 sec(s) Train Acc: 0.992094 Loss: 0.000474 | Val Acc: 0.552770 loss: 0.052212\n",
      "[072/300] 78.00 sec(s) Train Acc: 0.991841 Loss: 0.000467 | Val Acc: 0.541108 loss: 0.051873\n",
      "[073/300] 78.02 sec(s) Train Acc: 0.995388 Loss: 0.000269 | Val Acc: 0.531778 loss: 0.055539\n",
      "[074/300] 78.01 sec(s) Train Acc: 0.994425 Loss: 0.000305 | Val Acc: 0.545190 loss: 0.048342\n",
      "[075/300] 78.03 sec(s) Train Acc: 0.992804 Loss: 0.000433 | Val Acc: 0.539650 loss: 0.043664\n",
      "[076/300] 78.02 sec(s) Train Acc: 0.994020 Loss: 0.000364 | Val Acc: 0.539942 loss: 0.048437\n",
      "[077/300] 78.00 sec(s) Train Acc: 0.995895 Loss: 0.000231 | Val Acc: 0.535569 loss: 0.055853\n",
      "[078/300] 78.00 sec(s) Train Acc: 0.993108 Loss: 0.000430 | Val Acc: 0.539067 loss: 0.047266\n",
      "[079/300] 78.01 sec(s) Train Acc: 0.995135 Loss: 0.000310 | Val Acc: 0.550146 loss: 0.057649\n",
      "[080/300] 78.01 sec(s) Train Acc: 0.997061 Loss: 0.000158 | Val Acc: 0.539067 loss: 0.059368\n",
      "[081/300] 78.02 sec(s) Train Acc: 0.992246 Loss: 0.000459 | Val Acc: 0.539359 loss: 0.052802\n",
      "[082/300] 78.00 sec(s) Train Acc: 0.995540 Loss: 0.000253 | Val Acc: 0.520700 loss: 0.056960\n",
      "[083/300] 78.00 sec(s) Train Acc: 0.995946 Loss: 0.000256 | Val Acc: 0.520991 loss: 0.058607\n",
      "[084/300] 77.98 sec(s) Train Acc: 0.993716 Loss: 0.000419 | Val Acc: 0.543732 loss: 0.055246\n",
      "[085/300] 77.99 sec(s) Train Acc: 0.995642 Loss: 0.000264 | Val Acc: 0.544315 loss: 0.053453\n",
      "[086/300] 78.00 sec(s) Train Acc: 0.995794 Loss: 0.000247 | Val Acc: 0.538776 loss: 0.057783\n",
      "[087/300] 77.98 sec(s) Train Acc: 0.996554 Loss: 0.000217 | Val Acc: 0.540233 loss: 0.058801\n",
      "[088/300] 78.01 sec(s) Train Acc: 0.996706 Loss: 0.000193 | Val Acc: 0.523324 loss: 0.062076\n",
      "[089/300] 78.06 sec(s) Train Acc: 0.993513 Loss: 0.000384 | Val Acc: 0.541399 loss: 0.064269\n",
      "[090/300] 78.08 sec(s) Train Acc: 0.993969 Loss: 0.000404 | Val Acc: 0.545773 loss: 0.048572\n",
      "[091/300] 78.06 sec(s) Train Acc: 0.994577 Loss: 0.000358 | Val Acc: 0.536443 loss: 0.056676\n",
      "[092/300] 78.01 sec(s) Train Acc: 0.996959 Loss: 0.000209 | Val Acc: 0.536735 loss: 0.055223\n",
      "[093/300] 77.98 sec(s) Train Acc: 0.996757 Loss: 0.000227 | Val Acc: 0.526239 loss: 0.056003\n",
      "[094/300] 77.98 sec(s) Train Acc: 0.994881 Loss: 0.000305 | Val Acc: 0.549563 loss: 0.051605\n",
      "[095/300] 77.98 sec(s) Train Acc: 0.994780 Loss: 0.000319 | Val Acc: 0.523324 loss: 0.051385\n",
      "[096/300] 77.97 sec(s) Train Acc: 0.995033 Loss: 0.000286 | Val Acc: 0.549854 loss: 0.056744\n",
      "[097/300] 78.02 sec(s) Train Acc: 0.996300 Loss: 0.000262 | Val Acc: 0.541108 loss: 0.054993\n",
      "[098/300] 78.01 sec(s) Train Acc: 0.995388 Loss: 0.000261 | Val Acc: 0.527405 loss: 0.047671\n",
      "[099/300] 78.02 sec(s) Train Acc: 0.994831 Loss: 0.000304 | Val Acc: 0.531487 loss: 0.049397\n",
      "[100/300] 77.98 sec(s) Train Acc: 0.994729 Loss: 0.000285 | Val Acc: 0.539359 loss: 0.060881\n",
      "[101/300] 77.97 sec(s) Train Acc: 0.995033 Loss: 0.000329 | Val Acc: 0.531778 loss: 0.049656\n",
      "[102/300] 77.99 sec(s) Train Acc: 0.995946 Loss: 0.000279 | Val Acc: 0.531778 loss: 0.048832\n",
      "[103/300] 78.01 sec(s) Train Acc: 0.996554 Loss: 0.000196 | Val Acc: 0.532653 loss: 0.055279\n",
      "[104/300] 78.02 sec(s) Train Acc: 0.997567 Loss: 0.000133 | Val Acc: 0.541108 loss: 0.060153\n",
      "[105/300] 78.14 sec(s) Train Acc: 0.997365 Loss: 0.000168 | Val Acc: 0.533528 loss: 0.059128\n",
      "[106/300] 78.05 sec(s) Train Acc: 0.994881 Loss: 0.000329 | Val Acc: 0.535277 loss: 0.053665\n",
      "[107/300] 77.99 sec(s) Train Acc: 0.996959 Loss: 0.000172 | Val Acc: 0.536152 loss: 0.060192\n",
      "[108/300] 77.98 sec(s) Train Acc: 0.994729 Loss: 0.000338 | Val Acc: 0.540816 loss: 0.051401\n",
      "[109/300] 78.02 sec(s) Train Acc: 0.997263 Loss: 0.000150 | Val Acc: 0.539067 loss: 0.057030\n",
      "[110/300] 78.02 sec(s) Train Acc: 0.997567 Loss: 0.000122 | Val Acc: 0.521283 loss: 0.067800\n",
      "[111/300] 78.01 sec(s) Train Acc: 0.994679 Loss: 0.000322 | Val Acc: 0.522741 loss: 0.060930\n",
      "[112/300] 78.02 sec(s) Train Acc: 0.994273 Loss: 0.000334 | Val Acc: 0.522449 loss: 0.051354\n",
      "[113/300] 78.03 sec(s) Train Acc: 0.995439 Loss: 0.000265 | Val Acc: 0.536735 loss: 0.062363\n",
      "[114/300] 78.04 sec(s) Train Acc: 0.997213 Loss: 0.000145 | Val Acc: 0.525948 loss: 0.065658\n",
      "[115/300] 78.04 sec(s) Train Acc: 0.995743 Loss: 0.000277 | Val Acc: 0.525364 loss: 0.061383\n",
      "[116/300] 78.02 sec(s) Train Acc: 0.995895 Loss: 0.000314 | Val Acc: 0.541691 loss: 0.055973\n",
      "[117/300] 78.03 sec(s) Train Acc: 0.998024 Loss: 0.000110 | Val Acc: 0.519534 loss: 0.060548\n",
      "[118/300] 78.05 sec(s) Train Acc: 0.996909 Loss: 0.000161 | Val Acc: 0.546356 loss: 0.064563\n",
      "[119/300] 78.02 sec(s) Train Acc: 0.998632 Loss: 0.000122 | Val Acc: 0.530612 loss: 0.057156\n",
      "[120/300] 78.03 sec(s) Train Acc: 0.994121 Loss: 0.000351 | Val Acc: 0.541108 loss: 0.066398\n",
      "[121/300] 78.05 sec(s) Train Acc: 0.994273 Loss: 0.000379 | Val Acc: 0.539359 loss: 0.056239\n",
      "[122/300] 78.06 sec(s) Train Acc: 0.998024 Loss: 0.000125 | Val Acc: 0.549854 loss: 0.060712\n",
      "[123/300] 78.05 sec(s) Train Acc: 0.997567 Loss: 0.000144 | Val Acc: 0.522449 loss: 0.063796\n",
      "[124/300] 78.07 sec(s) Train Acc: 0.997821 Loss: 0.000119 | Val Acc: 0.535277 loss: 0.059693\n",
      "[125/300] 78.09 sec(s) Train Acc: 0.996757 Loss: 0.000208 | Val Acc: 0.545190 loss: 0.068092\n",
      "[126/300] 78.05 sec(s) Train Acc: 0.995895 Loss: 0.000337 | Val Acc: 0.537318 loss: 0.054785\n",
      "[127/300] 78.04 sec(s) Train Acc: 0.996706 Loss: 0.000204 | Val Acc: 0.529446 loss: 0.056215\n",
      "[128/300] 78.03 sec(s) Train Acc: 0.998733 Loss: 0.000083 | Val Acc: 0.537318 loss: 0.067314\n",
      "[129/300] 77.99 sec(s) Train Acc: 0.994881 Loss: 0.000350 | Val Acc: 0.538484 loss: 0.061008\n",
      "[130/300] 77.96 sec(s) Train Acc: 0.997618 Loss: 0.000153 | Val Acc: 0.540233 loss: 0.060373\n",
      "[131/300] 77.94 sec(s) Train Acc: 0.997365 Loss: 0.000133 | Val Acc: 0.517784 loss: 0.058394\n",
      "[132/300] 77.99 sec(s) Train Acc: 0.996148 Loss: 0.000270 | Val Acc: 0.529446 loss: 0.064925\n",
      "[133/300] 78.00 sec(s) Train Acc: 0.995642 Loss: 0.000387 | Val Acc: 0.536152 loss: 0.051281\n",
      "[134/300] 78.00 sec(s) Train Acc: 0.996858 Loss: 0.000196 | Val Acc: 0.523032 loss: 0.056907\n",
      "[135/300] 78.01 sec(s) Train Acc: 0.997314 Loss: 0.000150 | Val Acc: 0.548980 loss: 0.057440\n",
      "[136/300] 78.01 sec(s) Train Acc: 0.997517 Loss: 0.000157 | Val Acc: 0.533819 loss: 0.057013\n",
      "[137/300] 78.02 sec(s) Train Acc: 0.997061 Loss: 0.000224 | Val Acc: 0.532362 loss: 0.063601\n",
      "[138/300] 78.01 sec(s) Train Acc: 0.997517 Loss: 0.000175 | Val Acc: 0.526531 loss: 0.062924\n",
      "[139/300] 78.07 sec(s) Train Acc: 0.996503 Loss: 0.000205 | Val Acc: 0.515452 loss: 0.058319\n",
      "[140/300] 78.03 sec(s) Train Acc: 0.996858 Loss: 0.000190 | Val Acc: 0.542566 loss: 0.063793\n",
      "[141/300] 78.03 sec(s) Train Acc: 0.997618 Loss: 0.000161 | Val Acc: 0.538484 loss: 0.058464\n",
      "[142/300] 78.01 sec(s) Train Acc: 0.997061 Loss: 0.000212 | Val Acc: 0.547522 loss: 0.059467\n",
      "[143/300] 78.03 sec(s) Train Acc: 0.999037 Loss: 0.000068 | Val Acc: 0.548397 loss: 0.072387\n",
      "[144/300] 78.00 sec(s) Train Acc: 0.997517 Loss: 0.000185 | Val Acc: 0.518659 loss: 0.058057\n",
      "[145/300] 78.01 sec(s) Train Acc: 0.995540 Loss: 0.000292 | Val Acc: 0.537026 loss: 0.055986\n",
      "[146/300] 78.01 sec(s) Train Acc: 0.997719 Loss: 0.000148 | Val Acc: 0.534402 loss: 0.064331\n",
      "[147/300] 78.00 sec(s) Train Acc: 0.998226 Loss: 0.000122 | Val Acc: 0.526239 loss: 0.065667\n",
      "[148/300] 78.00 sec(s) Train Acc: 0.996452 Loss: 0.000245 | Val Acc: 0.543149 loss: 0.064139\n",
      "[149/300] 77.99 sec(s) Train Acc: 0.997415 Loss: 0.000148 | Val Acc: 0.530612 loss: 0.054912\n",
      "[150/300] 78.00 sec(s) Train Acc: 0.996199 Loss: 0.000297 | Val Acc: 0.535860 loss: 0.059849\n",
      "[151/300] 77.99 sec(s) Train Acc: 0.997669 Loss: 0.000141 | Val Acc: 0.547230 loss: 0.065975\n",
      "[152/300] 78.01 sec(s) Train Acc: 0.996959 Loss: 0.000242 | Val Acc: 0.537609 loss: 0.049979\n",
      "[153/300] 77.99 sec(s) Train Acc: 0.996300 Loss: 0.000249 | Val Acc: 0.540233 loss: 0.061815\n",
      "[154/300] 78.01 sec(s) Train Acc: 0.998429 Loss: 0.000121 | Val Acc: 0.541983 loss: 0.058351\n",
      "[155/300] 78.02 sec(s) Train Acc: 0.997061 Loss: 0.000179 | Val Acc: 0.535860 loss: 0.058804\n",
      "[156/300] 78.03 sec(s) Train Acc: 0.996503 Loss: 0.000194 | Val Acc: 0.529738 loss: 0.057650\n",
      "[157/300] 78.01 sec(s) Train Acc: 0.996807 Loss: 0.000191 | Val Acc: 0.533528 loss: 0.069557\n",
      "[158/300] 78.05 sec(s) Train Acc: 0.998074 Loss: 0.000120 | Val Acc: 0.531195 loss: 0.063791\n",
      "[159/300] 78.01 sec(s) Train Acc: 0.996757 Loss: 0.000218 | Val Acc: 0.535277 loss: 0.062021\n",
      "[160/300] 77.99 sec(s) Train Acc: 0.996047 Loss: 0.000228 | Val Acc: 0.532653 loss: 0.065933\n",
      "[161/300] 78.15 sec(s) Train Acc: 0.996402 Loss: 0.000252 | Val Acc: 0.535569 loss: 0.062866\n",
      "[162/300] 78.02 sec(s) Train Acc: 0.997719 Loss: 0.000138 | Val Acc: 0.530029 loss: 0.068817\n",
      "[163/300] 78.04 sec(s) Train Acc: 0.998378 Loss: 0.000106 | Val Acc: 0.519825 loss: 0.067389\n",
      "[164/300] 78.00 sec(s) Train Acc: 0.998530 Loss: 0.000103 | Val Acc: 0.534111 loss: 0.068977\n",
      "[165/300] 77.95 sec(s) Train Acc: 0.997213 Loss: 0.000199 | Val Acc: 0.531487 loss: 0.054384\n",
      "[166/300] 77.96 sec(s) Train Acc: 0.998125 Loss: 0.000125 | Val Acc: 0.530904 loss: 0.059131\n",
      "[167/300] 77.97 sec(s) Train Acc: 0.996858 Loss: 0.000182 | Val Acc: 0.529738 loss: 0.069688\n",
      "[168/300] 78.00 sec(s) Train Acc: 0.998784 Loss: 0.000104 | Val Acc: 0.532945 loss: 0.066592\n",
      "[169/300] 77.99 sec(s) Train Acc: 0.996706 Loss: 0.000221 | Val Acc: 0.514869 loss: 0.062826\n",
      "[170/300] 78.00 sec(s) Train Acc: 0.997618 Loss: 0.000178 | Val Acc: 0.529155 loss: 0.068748\n",
      "[171/300] 78.02 sec(s) Train Acc: 0.995743 Loss: 0.000332 | Val Acc: 0.523324 loss: 0.069923\n",
      "[172/300] 78.02 sec(s) Train Acc: 0.997719 Loss: 0.000115 | Val Acc: 0.536152 loss: 0.061392\n",
      "[173/300] 78.05 sec(s) Train Acc: 0.998885 Loss: 0.000073 | Val Acc: 0.541691 loss: 0.069616\n",
      "[174/300] 78.07 sec(s) Train Acc: 0.997517 Loss: 0.000169 | Val Acc: 0.521574 loss: 0.074578\n",
      "[175/300] 78.07 sec(s) Train Acc: 0.997162 Loss: 0.000187 | Val Acc: 0.536443 loss: 0.062448\n",
      "[176/300] 78.04 sec(s) Train Acc: 0.998024 Loss: 0.000119 | Val Acc: 0.529155 loss: 0.059983\n",
      "[177/300] 78.07 sec(s) Train Acc: 0.995794 Loss: 0.000253 | Val Acc: 0.524198 loss: 0.064203\n",
      "[178/300] 78.07 sec(s) Train Acc: 0.998277 Loss: 0.000115 | Val Acc: 0.524490 loss: 0.062969\n",
      "[179/300] 78.06 sec(s) Train Acc: 0.998682 Loss: 0.000075 | Val Acc: 0.526531 loss: 0.068932\n",
      "[180/300] 78.06 sec(s) Train Acc: 0.996250 Loss: 0.000233 | Val Acc: 0.522449 loss: 0.069534\n",
      "[181/300] 78.06 sec(s) Train Acc: 0.997922 Loss: 0.000151 | Val Acc: 0.542857 loss: 0.063129\n",
      "[182/300] 78.08 sec(s) Train Acc: 0.998429 Loss: 0.000104 | Val Acc: 0.529446 loss: 0.063238\n",
      "[183/300] 78.07 sec(s) Train Acc: 0.998530 Loss: 0.000102 | Val Acc: 0.539359 loss: 0.063126\n",
      "[184/300] 78.05 sec(s) Train Acc: 0.999240 Loss: 0.000044 | Val Acc: 0.540816 loss: 0.065075\n",
      "[185/300] 78.10 sec(s) Train Acc: 0.999240 Loss: 0.000045 | Val Acc: 0.540816 loss: 0.082318\n",
      "[186/300] 78.08 sec(s) Train Acc: 0.991739 Loss: 0.000636 | Val Acc: 0.524781 loss: 0.047485\n",
      "[187/300] 78.05 sec(s) Train Acc: 0.997719 Loss: 0.000154 | Val Acc: 0.539942 loss: 0.065044\n",
      "[188/300] 78.05 sec(s) Train Acc: 0.997973 Loss: 0.000145 | Val Acc: 0.537901 loss: 0.064002\n",
      "[189/300] 78.06 sec(s) Train Acc: 0.998328 Loss: 0.000083 | Val Acc: 0.531487 loss: 0.066784\n",
      "[190/300] 78.07 sec(s) Train Acc: 0.997567 Loss: 0.000162 | Val Acc: 0.523032 loss: 0.067573\n",
      "[191/300] 78.08 sec(s) Train Acc: 0.995996 Loss: 0.000252 | Val Acc: 0.537026 loss: 0.067867\n",
      "[192/300] 78.09 sec(s) Train Acc: 0.996402 Loss: 0.000237 | Val Acc: 0.530904 loss: 0.061275\n",
      "[193/300] 78.06 sec(s) Train Acc: 0.997770 Loss: 0.000131 | Val Acc: 0.524198 loss: 0.059963\n",
      "[194/300] 78.07 sec(s) Train Acc: 0.997010 Loss: 0.000268 | Val Acc: 0.528571 loss: 0.051347\n",
      "[195/300] 78.09 sec(s) Train Acc: 0.999240 Loss: 0.000053 | Val Acc: 0.539067 loss: 0.068328\n",
      "[196/300] 78.09 sec(s) Train Acc: 0.998784 Loss: 0.000101 | Val Acc: 0.528571 loss: 0.063833\n",
      "[197/300] 78.09 sec(s) Train Acc: 0.998429 Loss: 0.000090 | Val Acc: 0.534402 loss: 0.067579\n",
      "[198/300] 78.06 sec(s) Train Acc: 0.998480 Loss: 0.000103 | Val Acc: 0.540233 loss: 0.064722\n",
      "[199/300] 78.10 sec(s) Train Acc: 0.998682 Loss: 0.000089 | Val Acc: 0.530904 loss: 0.056548\n",
      "[200/300] 78.06 sec(s) Train Acc: 0.996909 Loss: 0.000195 | Val Acc: 0.543440 loss: 0.061882\n",
      "[201/300] 78.09 sec(s) Train Acc: 0.997517 Loss: 0.000155 | Val Acc: 0.530321 loss: 0.065051\n",
      "[202/300] 78.07 sec(s) Train Acc: 0.998277 Loss: 0.000106 | Val Acc: 0.530904 loss: 0.069324\n",
      "[203/300] 78.08 sec(s) Train Acc: 0.998682 Loss: 0.000075 | Val Acc: 0.535277 loss: 0.073911\n",
      "[204/300] 78.08 sec(s) Train Acc: 0.998480 Loss: 0.000109 | Val Acc: 0.519534 loss: 0.073508\n",
      "[205/300] 78.05 sec(s) Train Acc: 0.995439 Loss: 0.000340 | Val Acc: 0.529738 loss: 0.061217\n",
      "[206/300] 78.02 sec(s) Train Acc: 0.997973 Loss: 0.000173 | Val Acc: 0.526239 loss: 0.075922\n",
      "[207/300] 78.02 sec(s) Train Acc: 0.998581 Loss: 0.000100 | Val Acc: 0.524781 loss: 0.067561\n",
      "[208/300] 78.04 sec(s) Train Acc: 0.997567 Loss: 0.000185 | Val Acc: 0.524490 loss: 0.066472\n",
      "[209/300] 77.96 sec(s) Train Acc: 0.997871 Loss: 0.000135 | Val Acc: 0.531778 loss: 0.069737\n",
      "[210/300] 77.94 sec(s) Train Acc: 0.998429 Loss: 0.000088 | Val Acc: 0.520700 loss: 0.064023\n",
      "[211/300] 77.97 sec(s) Train Acc: 0.997567 Loss: 0.000170 | Val Acc: 0.532070 loss: 0.067837\n",
      "[212/300] 77.99 sec(s) Train Acc: 0.996554 Loss: 0.000192 | Val Acc: 0.507580 loss: 0.073772\n",
      "[213/300] 78.07 sec(s) Train Acc: 0.997567 Loss: 0.000154 | Val Acc: 0.531487 loss: 0.064686\n",
      "[214/300] 78.02 sec(s) Train Acc: 0.999138 Loss: 0.000057 | Val Acc: 0.531778 loss: 0.083733\n",
      "[215/300] 78.08 sec(s) Train Acc: 0.996605 Loss: 0.000244 | Val Acc: 0.512828 loss: 0.078663\n",
      "[216/300] 77.98 sec(s) Train Acc: 0.998429 Loss: 0.000092 | Val Acc: 0.525656 loss: 0.074553\n",
      "[217/300] 77.94 sec(s) Train Acc: 0.997618 Loss: 0.000185 | Val Acc: 0.525073 loss: 0.081375\n",
      "[218/300] 77.94 sec(s) Train Acc: 0.996706 Loss: 0.000279 | Val Acc: 0.516035 loss: 0.064121\n",
      "[219/300] 77.99 sec(s) Train Acc: 0.998632 Loss: 0.000101 | Val Acc: 0.533528 loss: 0.062725\n",
      "[220/300] 77.97 sec(s) Train Acc: 0.999240 Loss: 0.000043 | Val Acc: 0.514286 loss: 0.078717\n",
      "[221/300] 77.95 sec(s) Train Acc: 0.997871 Loss: 0.000179 | Val Acc: 0.525364 loss: 0.072939\n",
      "[222/300] 78.01 sec(s) Train Acc: 0.998784 Loss: 0.000147 | Val Acc: 0.534694 loss: 0.070030\n",
      "[223/300] 77.99 sec(s) Train Acc: 0.998328 Loss: 0.000125 | Val Acc: 0.514577 loss: 0.073695\n",
      "[224/300] 77.98 sec(s) Train Acc: 0.997010 Loss: 0.000230 | Val Acc: 0.499125 loss: 0.064394\n",
      "[225/300] 78.02 sec(s) Train Acc: 0.997618 Loss: 0.000178 | Val Acc: 0.519242 loss: 0.046701\n",
      "[226/300] 78.06 sec(s) Train Acc: 0.997111 Loss: 0.000192 | Val Acc: 0.520117 loss: 0.066947\n",
      "[227/300] 78.06 sec(s) Train Acc: 0.998429 Loss: 0.000091 | Val Acc: 0.534985 loss: 0.071612\n",
      "[228/300] 78.10 sec(s) Train Acc: 0.998936 Loss: 0.000059 | Val Acc: 0.532945 loss: 0.071953\n",
      "[229/300] 78.07 sec(s) Train Acc: 0.997162 Loss: 0.000278 | Val Acc: 0.530029 loss: 0.057982\n",
      "[230/300] 78.08 sec(s) Train Acc: 0.999341 Loss: 0.000047 | Val Acc: 0.533819 loss: 0.073662\n",
      "[231/300] 78.11 sec(s) Train Acc: 0.997517 Loss: 0.000196 | Val Acc: 0.534402 loss: 0.062895\n",
      "[232/300] 78.08 sec(s) Train Acc: 0.998226 Loss: 0.000133 | Val Acc: 0.529446 loss: 0.057346\n",
      "[233/300] 78.05 sec(s) Train Acc: 0.998632 Loss: 0.000084 | Val Acc: 0.528863 loss: 0.067937\n",
      "[234/300] 78.07 sec(s) Train Acc: 0.998936 Loss: 0.000053 | Val Acc: 0.539942 loss: 0.071475\n",
      "[235/300] 77.99 sec(s) Train Acc: 0.999595 Loss: 0.000026 | Val Acc: 0.534111 loss: 0.081444\n",
      "[236/300] 78.03 sec(s) Train Acc: 0.999088 Loss: 0.000063 | Val Acc: 0.498251 loss: 0.074996\n",
      "[237/300] 78.08 sec(s) Train Acc: 0.995692 Loss: 0.000363 | Val Acc: 0.529738 loss: 0.056713\n",
      "[238/300] 78.00 sec(s) Train Acc: 0.997973 Loss: 0.000164 | Val Acc: 0.525948 loss: 0.059702\n",
      "[239/300] 77.99 sec(s) Train Acc: 0.998125 Loss: 0.000107 | Val Acc: 0.530029 loss: 0.068125\n",
      "[240/300] 77.99 sec(s) Train Acc: 0.996858 Loss: 0.000211 | Val Acc: 0.531195 loss: 0.062437\n",
      "[241/300] 77.98 sec(s) Train Acc: 0.997314 Loss: 0.000216 | Val Acc: 0.527114 loss: 0.055957\n",
      "[242/300] 77.99 sec(s) Train Acc: 0.999138 Loss: 0.000038 | Val Acc: 0.527405 loss: 0.077420\n",
      "[243/300] 78.00 sec(s) Train Acc: 0.998378 Loss: 0.000105 | Val Acc: 0.530029 loss: 0.076698\n",
      "[244/300] 78.00 sec(s) Train Acc: 0.998378 Loss: 0.000106 | Val Acc: 0.520991 loss: 0.064056\n",
      "[245/300] 78.01 sec(s) Train Acc: 0.998784 Loss: 0.000109 | Val Acc: 0.540525 loss: 0.063312\n",
      "[246/300] 78.06 sec(s) Train Acc: 0.999392 Loss: 0.000056 | Val Acc: 0.541691 loss: 0.066400\n",
      "[247/300] 78.03 sec(s) Train Acc: 0.999747 Loss: 0.000013 | Val Acc: 0.534111 loss: 0.079316\n",
      "[248/300] 78.04 sec(s) Train Acc: 0.997669 Loss: 0.000178 | Val Acc: 0.523324 loss: 0.061398\n",
      "[249/300] 78.03 sec(s) Train Acc: 0.997669 Loss: 0.000190 | Val Acc: 0.527114 loss: 0.060429\n",
      "[250/300] 78.06 sec(s) Train Acc: 0.997922 Loss: 0.000174 | Val Acc: 0.510204 loss: 0.058108\n",
      "[251/300] 78.05 sec(s) Train Acc: 0.995946 Loss: 0.000291 | Val Acc: 0.540233 loss: 0.054104\n",
      "[252/300] 78.05 sec(s) Train Acc: 0.998176 Loss: 0.000179 | Val Acc: 0.536735 loss: 0.065883\n",
      "[253/300] 78.00 sec(s) Train Acc: 0.998986 Loss: 0.000083 | Val Acc: 0.533819 loss: 0.068941\n",
      "[254/300] 78.00 sec(s) Train Acc: 0.999189 Loss: 0.000054 | Val Acc: 0.530029 loss: 0.071795\n",
      "[255/300] 78.02 sec(s) Train Acc: 0.997669 Loss: 0.000191 | Val Acc: 0.520700 loss: 0.066699\n",
      "[256/300] 78.03 sec(s) Train Acc: 0.998682 Loss: 0.000118 | Val Acc: 0.525364 loss: 0.061273\n",
      "[257/300] 78.03 sec(s) Train Acc: 0.995338 Loss: 0.000372 | Val Acc: 0.531487 loss: 0.061046\n",
      "[258/300] 78.04 sec(s) Train Acc: 0.998480 Loss: 0.000129 | Val Acc: 0.529446 loss: 0.060313\n",
      "[259/300] 78.03 sec(s) Train Acc: 0.999189 Loss: 0.000085 | Val Acc: 0.530904 loss: 0.066896\n",
      "[260/300] 78.01 sec(s) Train Acc: 0.999037 Loss: 0.000071 | Val Acc: 0.522157 loss: 0.066721\n",
      "[261/300] 78.03 sec(s) Train Acc: 0.998733 Loss: 0.000097 | Val Acc: 0.539067 loss: 0.074702\n",
      "[262/300] 78.01 sec(s) Train Acc: 0.999138 Loss: 0.000054 | Val Acc: 0.523032 loss: 0.079322\n",
      "[263/300] 78.00 sec(s) Train Acc: 0.997517 Loss: 0.000187 | Val Acc: 0.520408 loss: 0.080011\n",
      "[264/300] 78.00 sec(s) Train Acc: 0.997922 Loss: 0.000124 | Val Acc: 0.518076 loss: 0.066955\n",
      "[265/300] 78.00 sec(s) Train Acc: 0.998378 Loss: 0.000094 | Val Acc: 0.529446 loss: 0.071417\n",
      "[266/300] 78.00 sec(s) Train Acc: 0.998682 Loss: 0.000168 | Val Acc: 0.538776 loss: 0.055659\n",
      "[267/300] 78.02 sec(s) Train Acc: 0.998378 Loss: 0.000120 | Val Acc: 0.531778 loss: 0.070473\n",
      "[268/300] 78.02 sec(s) Train Acc: 0.998784 Loss: 0.000126 | Val Acc: 0.530321 loss: 0.058269\n",
      "[269/300] 78.02 sec(s) Train Acc: 0.999443 Loss: 0.000027 | Val Acc: 0.536735 loss: 0.082845\n",
      "[270/300] 78.00 sec(s) Train Acc: 0.999138 Loss: 0.000050 | Val Acc: 0.535569 loss: 0.074924\n",
      "[271/300] 78.00 sec(s) Train Acc: 0.998986 Loss: 0.000064 | Val Acc: 0.535860 loss: 0.090043\n",
      "[272/300] 77.95 sec(s) Train Acc: 0.998277 Loss: 0.000131 | Val Acc: 0.541108 loss: 0.074867\n",
      "[273/300] 78.01 sec(s) Train Acc: 0.998277 Loss: 0.000103 | Val Acc: 0.527697 loss: 0.085515\n",
      "[274/300] 78.04 sec(s) Train Acc: 0.997922 Loss: 0.000155 | Val Acc: 0.534694 loss: 0.076964\n",
      "[275/300] 78.09 sec(s) Train Acc: 0.997415 Loss: 0.000182 | Val Acc: 0.527697 loss: 0.067180\n",
      "[276/300] 77.94 sec(s) Train Acc: 0.998834 Loss: 0.000098 | Val Acc: 0.528863 loss: 0.067734\n",
      "[277/300] 77.93 sec(s) Train Acc: 0.999138 Loss: 0.000071 | Val Acc: 0.531778 loss: 0.071579\n",
      "[278/300] 77.92 sec(s) Train Acc: 0.999544 Loss: 0.000058 | Val Acc: 0.537609 loss: 0.082225\n",
      "[279/300] 77.96 sec(s) Train Acc: 0.998784 Loss: 0.000112 | Val Acc: 0.518076 loss: 0.064351\n",
      "[280/300] 77.97 sec(s) Train Acc: 0.998378 Loss: 0.000122 | Val Acc: 0.537901 loss: 0.059738\n",
      "[281/300] 77.98 sec(s) Train Acc: 0.998328 Loss: 0.000143 | Val Acc: 0.534111 loss: 0.070103\n",
      "[282/300] 77.98 sec(s) Train Acc: 0.998834 Loss: 0.000084 | Val Acc: 0.531487 loss: 0.069040\n",
      "[283/300] 77.96 sec(s) Train Acc: 0.998378 Loss: 0.000129 | Val Acc: 0.531195 loss: 0.071307\n",
      "[284/300] 77.97 sec(s) Train Acc: 0.999189 Loss: 0.000068 | Val Acc: 0.532653 loss: 0.072935\n",
      "[285/300] 77.98 sec(s) Train Acc: 0.997821 Loss: 0.000173 | Val Acc: 0.528280 loss: 0.070841\n",
      "[286/300] 77.97 sec(s) Train Acc: 0.998378 Loss: 0.000120 | Val Acc: 0.540233 loss: 0.065213\n",
      "[287/300] 78.00 sec(s) Train Acc: 0.999544 Loss: 0.000040 | Val Acc: 0.538192 loss: 0.070344\n",
      "[288/300] 78.00 sec(s) Train Acc: 0.999595 Loss: 0.000019 | Val Acc: 0.523615 loss: 0.089055\n",
      "[289/300] 77.99 sec(s) Train Acc: 0.998682 Loss: 0.000077 | Val Acc: 0.526822 loss: 0.082554\n",
      "[290/300] 77.98 sec(s) Train Acc: 0.997314 Loss: 0.000197 | Val Acc: 0.526822 loss: 0.062065\n",
      "[291/300] 77.99 sec(s) Train Acc: 0.998226 Loss: 0.000105 | Val Acc: 0.528571 loss: 0.072431\n",
      "[292/300] 78.06 sec(s) Train Acc: 0.998480 Loss: 0.000107 | Val Acc: 0.539359 loss: 0.076660\n",
      "[293/300] 78.04 sec(s) Train Acc: 0.997466 Loss: 0.000226 | Val Acc: 0.531487 loss: 0.071838\n",
      "[294/300] 78.04 sec(s) Train Acc: 0.998530 Loss: 0.000092 | Val Acc: 0.531778 loss: 0.089655\n",
      "[295/300] 78.01 sec(s) Train Acc: 0.999392 Loss: 0.000034 | Val Acc: 0.527114 loss: 0.085329\n",
      "[296/300] 78.00 sec(s) Train Acc: 0.998024 Loss: 0.000145 | Val Acc: 0.526531 loss: 0.068776\n",
      "[297/300] 78.01 sec(s) Train Acc: 0.998834 Loss: 0.000100 | Val Acc: 0.517201 loss: 0.074274\n",
      "[298/300] 78.01 sec(s) Train Acc: 0.998328 Loss: 0.000182 | Val Acc: 0.527988 loss: 0.074171\n",
      "[299/300] 77.98 sec(s) Train Acc: 0.998834 Loss: 0.000094 | Val Acc: 0.525948 loss: 0.078890\n",
      "[300/300] 78.02 sec(s) Train Acc: 0.998226 Loss: 0.000168 | Val Acc: 0.524198 loss: 0.065145\n"
     ]
    }
   ],
   "source": [
    "model = Classifier().cuda()\n",
    "# model = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 300\n",
    "\n",
    "# # use apex to optimize\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        train_pred = model(data[0].cuda()) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "#         train_pred = model(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        batch_loss.backward() \n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "#             val_pred = model(data[0].cpu())\n",
    "#             batch_loss = loss(val_pred, data[1].cpu())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-ssSxXlsI_T"
   },
   "source": [
    "得到好的參數後，我們使用 training set 和 validation set 共同訓練（資料量變多，模型效果較好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKoUxLun8lFG",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.89311144813877, 102.3587941606983, 126.59376063616554]\n",
      "[72.80305392379675, 75.35438507973123, 79.31408066842762]\n"
     ]
    }
   ],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ConcatDataset([\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform1),\n",
    "    ImgDataset(train_val_x, train_val_y, train_transform2),\n",
    "])\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print([train_val_x[:,:,:,0].mean(), train_val_x[:,:,:,1].mean(), train_val_x[:,:,:,2].mean()])\n",
    "print([train_val_x[:,:,:,0].std(), train_val_x[:,:,:,1].std(), train_val_x[:,:,:,2].std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoAS5TtRsfOo",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/350] 98.20 sec(s) Train Acc: 0.271435 Loss: 0.032282\n",
      "[002/350] 98.32 sec(s) Train Acc: 0.348187 Loss: 0.029012\n",
      "[003/350] 98.35 sec(s) Train Acc: 0.419224 Loss: 0.026056\n",
      "[004/350] 98.33 sec(s) Train Acc: 0.480370 Loss: 0.023496\n",
      "[005/350] 98.20 sec(s) Train Acc: 0.534710 Loss: 0.021092\n",
      "[006/350] 98.23 sec(s) Train Acc: 0.584349 Loss: 0.019017\n",
      "[007/350] 98.21 sec(s) Train Acc: 0.626279 Loss: 0.017001\n",
      "[008/350] 98.20 sec(s) Train Acc: 0.678550 Loss: 0.014817\n",
      "[009/350] 98.26 sec(s) Train Acc: 0.722511 Loss: 0.012671\n",
      "[010/350] 98.25 sec(s) Train Acc: 0.772375 Loss: 0.010487\n",
      "[011/350] 98.24 sec(s) Train Acc: 0.813327 Loss: 0.008692\n",
      "[012/350] 98.25 sec(s) Train Acc: 0.853452 Loss: 0.006827\n",
      "[013/350] 98.25 sec(s) Train Acc: 0.885379 Loss: 0.005357\n",
      "[014/350] 98.22 sec(s) Train Acc: 0.909823 Loss: 0.004370\n",
      "[015/350] 98.23 sec(s) Train Acc: 0.924752 Loss: 0.003586\n",
      "[016/350] 98.24 sec(s) Train Acc: 0.946149 Loss: 0.002636\n",
      "[017/350] 98.21 sec(s) Train Acc: 0.952467 Loss: 0.002314\n",
      "[018/350] 98.27 sec(s) Train Acc: 0.958484 Loss: 0.002113\n",
      "[019/350] 98.28 sec(s) Train Acc: 0.965403 Loss: 0.001752\n",
      "[020/350] 98.20 sec(s) Train Acc: 0.971119 Loss: 0.001459\n",
      "[021/350] 98.17 sec(s) Train Acc: 0.969201 Loss: 0.001587\n",
      "[022/350] 98.19 sec(s) Train Acc: 0.974428 Loss: 0.001283\n",
      "[023/350] 98.19 sec(s) Train Acc: 0.976722 Loss: 0.001234\n",
      "[024/350] 98.19 sec(s) Train Acc: 0.979731 Loss: 0.001047\n",
      "[025/350] 98.16 sec(s) Train Acc: 0.977023 Loss: 0.001153\n",
      "[026/350] 98.22 sec(s) Train Acc: 0.978903 Loss: 0.001078\n",
      "[027/350] 98.26 sec(s) Train Acc: 0.980332 Loss: 0.001048\n",
      "[028/350] 98.22 sec(s) Train Acc: 0.984431 Loss: 0.000794\n",
      "[029/350] 98.19 sec(s) Train Acc: 0.982288 Loss: 0.000994\n",
      "[030/350] 98.17 sec(s) Train Acc: 0.987440 Loss: 0.000685\n",
      "[031/350] 98.18 sec(s) Train Acc: 0.985372 Loss: 0.000793\n",
      "[032/350] 98.15 sec(s) Train Acc: 0.985672 Loss: 0.000782\n",
      "[033/350] 98.16 sec(s) Train Acc: 0.987929 Loss: 0.000664\n",
      "[034/350] 98.17 sec(s) Train Acc: 0.983416 Loss: 0.000876\n",
      "[035/350] 98.16 sec(s) Train Acc: 0.985898 Loss: 0.000721\n",
      "[036/350] 98.15 sec(s) Train Acc: 0.986462 Loss: 0.000741\n",
      "[037/350] 98.18 sec(s) Train Acc: 0.990185 Loss: 0.000532\n",
      "[038/350] 98.17 sec(s) Train Acc: 0.989959 Loss: 0.000571\n",
      "[039/350] 98.17 sec(s) Train Acc: 0.991351 Loss: 0.000518\n",
      "[040/350] 98.19 sec(s) Train Acc: 0.986500 Loss: 0.000731\n",
      "[041/350] 98.20 sec(s) Train Acc: 0.990486 Loss: 0.000508\n",
      "[042/350] 98.23 sec(s) Train Acc: 0.989358 Loss: 0.000583\n",
      "[043/350] 98.26 sec(s) Train Acc: 0.992028 Loss: 0.000440\n",
      "[044/350] 98.24 sec(s) Train Acc: 0.988944 Loss: 0.000598\n",
      "[045/350] 98.27 sec(s) Train Acc: 0.991163 Loss: 0.000461\n",
      "[046/350] 98.28 sec(s) Train Acc: 0.989734 Loss: 0.000647\n",
      "[047/350] 98.28 sec(s) Train Acc: 0.992028 Loss: 0.000404\n",
      "[048/350] 98.30 sec(s) Train Acc: 0.990411 Loss: 0.000570\n",
      "[049/350] 98.30 sec(s) Train Acc: 0.991727 Loss: 0.000454\n",
      "[050/350] 98.27 sec(s) Train Acc: 0.991727 Loss: 0.000444\n",
      "[051/350] 98.31 sec(s) Train Acc: 0.991276 Loss: 0.000473\n",
      "[052/350] 98.29 sec(s) Train Acc: 0.991840 Loss: 0.000473\n",
      "[053/350] 98.31 sec(s) Train Acc: 0.991727 Loss: 0.000455\n",
      "[054/350] 98.34 sec(s) Train Acc: 0.991952 Loss: 0.000419\n",
      "[055/350] 98.36 sec(s) Train Acc: 0.993569 Loss: 0.000425\n",
      "[056/350] 98.29 sec(s) Train Acc: 0.993381 Loss: 0.000357\n",
      "[057/350] 98.30 sec(s) Train Acc: 0.992893 Loss: 0.000416\n",
      "[058/350] 98.28 sec(s) Train Acc: 0.993983 Loss: 0.000368\n",
      "[059/350] 98.31 sec(s) Train Acc: 0.992742 Loss: 0.000428\n",
      "[060/350] 98.32 sec(s) Train Acc: 0.993118 Loss: 0.000375\n",
      "[061/350] 98.35 sec(s) Train Acc: 0.993118 Loss: 0.000428\n",
      "[062/350] 98.30 sec(s) Train Acc: 0.995036 Loss: 0.000328\n",
      "[063/350] 98.30 sec(s) Train Acc: 0.993344 Loss: 0.000373\n",
      "[064/350] 98.33 sec(s) Train Acc: 0.994171 Loss: 0.000356\n",
      "[065/350] 98.35 sec(s) Train Acc: 0.994058 Loss: 0.000347\n",
      "[066/350] 98.31 sec(s) Train Acc: 0.994773 Loss: 0.000308\n",
      "[067/350] 98.36 sec(s) Train Acc: 0.993720 Loss: 0.000361\n",
      "[068/350] 98.29 sec(s) Train Acc: 0.993682 Loss: 0.000375\n",
      "[069/350] 98.29 sec(s) Train Acc: 0.996465 Loss: 0.000188\n",
      "[070/350] 98.31 sec(s) Train Acc: 0.991990 Loss: 0.000476\n",
      "[071/350] 98.29 sec(s) Train Acc: 0.993758 Loss: 0.000376\n",
      "[072/350] 98.22 sec(s) Train Acc: 0.995450 Loss: 0.000255\n",
      "[073/350] 98.19 sec(s) Train Acc: 0.994585 Loss: 0.000281\n",
      "[074/350] 98.28 sec(s) Train Acc: 0.994510 Loss: 0.000317\n",
      "[075/350] 98.15 sec(s) Train Acc: 0.995337 Loss: 0.000260\n",
      "[076/350] 98.16 sec(s) Train Acc: 0.995036 Loss: 0.000296\n",
      "[077/350] 98.20 sec(s) Train Acc: 0.993381 Loss: 0.000380\n",
      "[078/350] 98.22 sec(s) Train Acc: 0.993118 Loss: 0.000440\n",
      "[079/350] 98.22 sec(s) Train Acc: 0.995036 Loss: 0.000293\n",
      "[080/350] 98.22 sec(s) Train Acc: 0.996503 Loss: 0.000229\n",
      "[081/350] 98.20 sec(s) Train Acc: 0.997029 Loss: 0.000155\n",
      "[082/350] 98.19 sec(s) Train Acc: 0.994810 Loss: 0.000315\n",
      "[083/350] 98.19 sec(s) Train Acc: 0.992028 Loss: 0.000496\n",
      "[084/350] 98.18 sec(s) Train Acc: 0.995901 Loss: 0.000261\n",
      "[085/350] 98.21 sec(s) Train Acc: 0.994622 Loss: 0.000291\n",
      "[086/350] 98.21 sec(s) Train Acc: 0.995487 Loss: 0.000278\n",
      "[087/350] 98.20 sec(s) Train Acc: 0.997067 Loss: 0.000168\n",
      "[088/350] 98.20 sec(s) Train Acc: 0.994735 Loss: 0.000344\n",
      "[089/350] 98.20 sec(s) Train Acc: 0.996503 Loss: 0.000239\n",
      "[090/350] 98.22 sec(s) Train Acc: 0.993457 Loss: 0.000363\n",
      "[091/350] 98.22 sec(s) Train Acc: 0.996804 Loss: 0.000198\n",
      "[092/350] 98.26 sec(s) Train Acc: 0.994246 Loss: 0.000360\n",
      "[093/350] 98.24 sec(s) Train Acc: 0.995976 Loss: 0.000207\n",
      "[094/350] 98.22 sec(s) Train Acc: 0.995149 Loss: 0.000299\n",
      "[095/350] 98.19 sec(s) Train Acc: 0.997368 Loss: 0.000151\n",
      "[096/350] 98.24 sec(s) Train Acc: 0.995901 Loss: 0.000249\n",
      "[097/350] 98.24 sec(s) Train Acc: 0.994773 Loss: 0.000315\n",
      "[098/350] 98.24 sec(s) Train Acc: 0.996503 Loss: 0.000191\n",
      "[099/350] 98.23 sec(s) Train Acc: 0.994058 Loss: 0.000334\n",
      "[100/350] 98.23 sec(s) Train Acc: 0.996578 Loss: 0.000195\n",
      "[101/350] 98.25 sec(s) Train Acc: 0.994547 Loss: 0.000308\n",
      "[102/350] 98.25 sec(s) Train Acc: 0.997443 Loss: 0.000145\n",
      "[103/350] 98.26 sec(s) Train Acc: 0.997255 Loss: 0.000195\n",
      "[104/350] 98.25 sec(s) Train Acc: 0.995149 Loss: 0.000290\n",
      "[105/350] 98.26 sec(s) Train Acc: 0.995487 Loss: 0.000260\n",
      "[106/350] 98.25 sec(s) Train Acc: 0.995600 Loss: 0.000270\n",
      "[107/350] 98.25 sec(s) Train Acc: 0.995262 Loss: 0.000311\n",
      "[108/350] 98.23 sec(s) Train Acc: 0.997330 Loss: 0.000168\n",
      "[109/350] 98.20 sec(s) Train Acc: 0.996691 Loss: 0.000219\n",
      "[110/350] 98.23 sec(s) Train Acc: 0.997217 Loss: 0.000146\n",
      "[111/350] 98.24 sec(s) Train Acc: 0.996352 Loss: 0.000261\n",
      "[112/350] 98.23 sec(s) Train Acc: 0.995976 Loss: 0.000250\n",
      "[113/350] 98.24 sec(s) Train Acc: 0.995187 Loss: 0.000306\n",
      "[114/350] 98.28 sec(s) Train Acc: 0.996616 Loss: 0.000174\n",
      "[115/350] 98.28 sec(s) Train Acc: 0.997104 Loss: 0.000211\n",
      "[116/350] 98.24 sec(s) Train Acc: 0.996653 Loss: 0.000204\n",
      "[117/350] 98.24 sec(s) Train Acc: 0.995525 Loss: 0.000316\n",
      "[118/350] 98.27 sec(s) Train Acc: 0.996014 Loss: 0.000297\n",
      "[119/350] 98.24 sec(s) Train Acc: 0.997556 Loss: 0.000157\n",
      "[120/350] 98.28 sec(s) Train Acc: 0.998233 Loss: 0.000114\n",
      "[121/350] 98.25 sec(s) Train Acc: 0.998045 Loss: 0.000134\n",
      "[122/350] 98.22 sec(s) Train Acc: 0.995751 Loss: 0.000311\n",
      "[123/350] 98.23 sec(s) Train Acc: 0.996390 Loss: 0.000214\n",
      "[124/350] 98.24 sec(s) Train Acc: 0.993983 Loss: 0.000432\n",
      "[125/350] 98.19 sec(s) Train Acc: 0.997180 Loss: 0.000160\n",
      "[126/350] 98.25 sec(s) Train Acc: 0.998609 Loss: 0.000091\n",
      "[127/350] 98.28 sec(s) Train Acc: 0.997518 Loss: 0.000183\n",
      "[128/350] 98.20 sec(s) Train Acc: 0.995149 Loss: 0.000365\n",
      "[129/350] 98.19 sec(s) Train Acc: 0.996954 Loss: 0.000214\n",
      "[130/350] 98.19 sec(s) Train Acc: 0.995375 Loss: 0.000296\n",
      "[131/350] 98.16 sec(s) Train Acc: 0.996841 Loss: 0.000217\n",
      "[132/350] 98.16 sec(s) Train Acc: 0.997518 Loss: 0.000150\n",
      "[133/350] 98.17 sec(s) Train Acc: 0.996578 Loss: 0.000208\n",
      "[134/350] 98.19 sec(s) Train Acc: 0.998082 Loss: 0.000159\n",
      "[135/350] 98.20 sec(s) Train Acc: 0.997180 Loss: 0.000165\n",
      "[136/350] 98.21 sec(s) Train Acc: 0.997443 Loss: 0.000147\n",
      "[137/350] 98.22 sec(s) Train Acc: 0.997819 Loss: 0.000160\n",
      "[138/350] 98.21 sec(s) Train Acc: 0.994622 Loss: 0.000427\n",
      "[139/350] 98.26 sec(s) Train Acc: 0.997368 Loss: 0.000194\n",
      "[140/350] 98.24 sec(s) Train Acc: 0.996503 Loss: 0.000246\n",
      "[141/350] 98.28 sec(s) Train Acc: 0.998157 Loss: 0.000109\n",
      "[142/350] 98.22 sec(s) Train Acc: 0.997067 Loss: 0.000195\n",
      "[143/350] 98.24 sec(s) Train Acc: 0.997217 Loss: 0.000199\n",
      "[144/350] 98.26 sec(s) Train Acc: 0.997405 Loss: 0.000161\n",
      "[145/350] 98.25 sec(s) Train Acc: 0.998195 Loss: 0.000098\n",
      "[146/350] 98.26 sec(s) Train Acc: 0.996691 Loss: 0.000215\n",
      "[147/350] 98.26 sec(s) Train Acc: 0.998007 Loss: 0.000131\n",
      "[148/350] 98.26 sec(s) Train Acc: 0.996390 Loss: 0.000266\n",
      "[149/350] 98.27 sec(s) Train Acc: 0.997518 Loss: 0.000145\n",
      "[150/350] 98.24 sec(s) Train Acc: 0.996014 Loss: 0.000256\n",
      "[151/350] 98.26 sec(s) Train Acc: 0.995901 Loss: 0.000252\n",
      "[152/350] 98.28 sec(s) Train Acc: 0.996578 Loss: 0.000199\n",
      "[153/350] 98.30 sec(s) Train Acc: 0.996879 Loss: 0.000174\n",
      "[154/350] 98.28 sec(s) Train Acc: 0.998872 Loss: 0.000106\n",
      "[155/350] 98.25 sec(s) Train Acc: 0.995751 Loss: 0.000333\n",
      "[156/350] 98.27 sec(s) Train Acc: 0.995563 Loss: 0.000343\n",
      "[157/350] 98.27 sec(s) Train Acc: 0.998195 Loss: 0.000136\n",
      "[158/350] 98.25 sec(s) Train Acc: 0.998947 Loss: 0.000060\n",
      "[159/350] 98.26 sec(s) Train Acc: 0.998797 Loss: 0.000077\n",
      "[160/350] 98.27 sec(s) Train Acc: 0.996277 Loss: 0.000233\n",
      "[161/350] 98.31 sec(s) Train Acc: 0.996954 Loss: 0.000186\n",
      "[162/350] 99.08 sec(s) Train Acc: 0.997405 Loss: 0.000183\n",
      "[163/350] 98.43 sec(s) Train Acc: 0.996954 Loss: 0.000206\n",
      "[164/350] 98.87 sec(s) Train Acc: 0.998721 Loss: 0.000088\n",
      "[165/350] 98.66 sec(s) Train Acc: 0.998759 Loss: 0.000081\n",
      "[166/350] 98.61 sec(s) Train Acc: 0.996954 Loss: 0.000191\n",
      "[167/350] 99.43 sec(s) Train Acc: 0.998571 Loss: 0.000080\n",
      "[168/350] 99.16 sec(s) Train Acc: 0.996653 Loss: 0.000247\n",
      "[169/350] 98.83 sec(s) Train Acc: 0.996916 Loss: 0.000182\n",
      "[170/350] 98.95 sec(s) Train Acc: 0.997180 Loss: 0.000183\n",
      "[171/350] 99.29 sec(s) Train Acc: 0.996879 Loss: 0.000195\n",
      "[172/350] 98.82 sec(s) Train Acc: 0.998233 Loss: 0.000089\n",
      "[173/350] 98.54 sec(s) Train Acc: 0.996578 Loss: 0.000278\n",
      "[174/350] 98.58 sec(s) Train Acc: 0.997744 Loss: 0.000160\n",
      "[175/350] 98.52 sec(s) Train Acc: 0.998120 Loss: 0.000138\n",
      "[176/350] 98.53 sec(s) Train Acc: 0.998496 Loss: 0.000101\n",
      "[177/350] 98.51 sec(s) Train Acc: 0.996841 Loss: 0.000214\n",
      "[178/350] 98.49 sec(s) Train Acc: 0.997781 Loss: 0.000219\n",
      "[179/350] 98.49 sec(s) Train Acc: 0.998797 Loss: 0.000083\n",
      "[180/350] 98.56 sec(s) Train Acc: 0.997894 Loss: 0.000153\n",
      "[181/350] 98.20 sec(s) Train Acc: 0.998007 Loss: 0.000145\n",
      "[182/350] 98.23 sec(s) Train Acc: 0.996728 Loss: 0.000190\n",
      "[183/350] 98.27 sec(s) Train Acc: 0.996992 Loss: 0.000213\n",
      "[184/350] 98.29 sec(s) Train Acc: 0.997330 Loss: 0.000194\n",
      "[185/350] 98.18 sec(s) Train Acc: 0.998270 Loss: 0.000120\n",
      "[186/350] 98.14 sec(s) Train Acc: 0.998345 Loss: 0.000114\n",
      "[187/350] 98.17 sec(s) Train Acc: 0.998157 Loss: 0.000122\n",
      "[188/350] 98.16 sec(s) Train Acc: 0.997969 Loss: 0.000151\n",
      "[189/350] 98.16 sec(s) Train Acc: 0.996916 Loss: 0.000205\n",
      "[190/350] 98.13 sec(s) Train Acc: 0.998345 Loss: 0.000104\n",
      "[191/350] 98.19 sec(s) Train Acc: 0.997593 Loss: 0.000148\n",
      "[192/350] 98.16 sec(s) Train Acc: 0.997292 Loss: 0.000148\n",
      "[193/350] 98.20 sec(s) Train Acc: 0.997894 Loss: 0.000141\n",
      "[194/350] 98.20 sec(s) Train Acc: 0.997480 Loss: 0.000143\n",
      "[195/350] 98.18 sec(s) Train Acc: 0.999248 Loss: 0.000051\n",
      "[196/350] 98.19 sec(s) Train Acc: 0.998571 Loss: 0.000137\n",
      "[197/350] 98.19 sec(s) Train Acc: 0.995111 Loss: 0.000404\n",
      "[198/350] 98.20 sec(s) Train Acc: 0.998195 Loss: 0.000123\n",
      "[199/350] 98.21 sec(s) Train Acc: 0.998533 Loss: 0.000094\n",
      "[200/350] 98.19 sec(s) Train Acc: 0.998007 Loss: 0.000161\n",
      "[201/350] 98.16 sec(s) Train Acc: 0.998421 Loss: 0.000130\n",
      "[202/350] 98.23 sec(s) Train Acc: 0.996616 Loss: 0.000231\n",
      "[203/350] 98.14 sec(s) Train Acc: 0.998233 Loss: 0.000122\n",
      "[204/350] 98.14 sec(s) Train Acc: 0.996841 Loss: 0.000223\n",
      "[205/350] 98.20 sec(s) Train Acc: 0.997932 Loss: 0.000172\n",
      "[206/350] 98.21 sec(s) Train Acc: 0.998345 Loss: 0.000112\n",
      "[207/350] 98.16 sec(s) Train Acc: 0.998045 Loss: 0.000128\n",
      "[208/350] 98.19 sec(s) Train Acc: 0.999097 Loss: 0.000057\n",
      "[209/350] 98.20 sec(s) Train Acc: 0.997480 Loss: 0.000190\n",
      "[210/350] 98.21 sec(s) Train Acc: 0.998571 Loss: 0.000104\n",
      "[211/350] 98.20 sec(s) Train Acc: 0.997593 Loss: 0.000161\n",
      "[212/350] 98.24 sec(s) Train Acc: 0.996127 Loss: 0.000347\n",
      "[213/350] 98.21 sec(s) Train Acc: 0.997856 Loss: 0.000175\n",
      "[214/350] 98.21 sec(s) Train Acc: 0.998721 Loss: 0.000078\n",
      "[215/350] 98.18 sec(s) Train Acc: 0.998082 Loss: 0.000193\n",
      "[216/350] 98.17 sec(s) Train Acc: 0.998383 Loss: 0.000111\n",
      "[217/350] 98.18 sec(s) Train Acc: 0.999511 Loss: 0.000034\n",
      "[218/350] 98.27 sec(s) Train Acc: 0.997781 Loss: 0.000150\n",
      "[219/350] 98.21 sec(s) Train Acc: 0.997255 Loss: 0.000253\n",
      "[220/350] 98.30 sec(s) Train Acc: 0.998421 Loss: 0.000111\n",
      "[221/350] 98.24 sec(s) Train Acc: 0.998609 Loss: 0.000097\n",
      "[222/350] 98.15 sec(s) Train Acc: 0.997894 Loss: 0.000163\n",
      "[223/350] 98.17 sec(s) Train Acc: 0.998270 Loss: 0.000143\n",
      "[224/350] 98.15 sec(s) Train Acc: 0.997932 Loss: 0.000168\n",
      "[225/350] 98.19 sec(s) Train Acc: 0.998421 Loss: 0.000179\n",
      "[226/350] 98.17 sec(s) Train Acc: 0.998646 Loss: 0.000083\n",
      "[227/350] 98.15 sec(s) Train Acc: 0.997932 Loss: 0.000163\n",
      "[228/350] 98.17 sec(s) Train Acc: 0.998007 Loss: 0.000163\n",
      "[229/350] 98.16 sec(s) Train Acc: 0.998909 Loss: 0.000076\n",
      "[230/350] 98.19 sec(s) Train Acc: 0.997593 Loss: 0.000151\n",
      "[231/350] 98.21 sec(s) Train Acc: 0.998308 Loss: 0.000114\n",
      "[232/350] 98.22 sec(s) Train Acc: 0.997480 Loss: 0.000185\n",
      "[233/350] 98.20 sec(s) Train Acc: 0.998270 Loss: 0.000174\n",
      "[234/350] 98.23 sec(s) Train Acc: 0.997443 Loss: 0.000200\n",
      "[235/350] 98.21 sec(s) Train Acc: 0.998270 Loss: 0.000139\n",
      "[236/350] 98.23 sec(s) Train Acc: 0.998909 Loss: 0.000065\n",
      "[237/350] 98.21 sec(s) Train Acc: 0.998045 Loss: 0.000151\n",
      "[238/350] 98.24 sec(s) Train Acc: 0.997631 Loss: 0.000181\n",
      "[239/350] 98.23 sec(s) Train Acc: 0.997631 Loss: 0.000155\n",
      "[240/350] 98.23 sec(s) Train Acc: 0.998345 Loss: 0.000146\n",
      "[241/350] 98.24 sec(s) Train Acc: 0.999436 Loss: 0.000048\n",
      "[242/350] 98.22 sec(s) Train Acc: 0.997856 Loss: 0.000146\n",
      "[243/350] 98.21 sec(s) Train Acc: 0.997518 Loss: 0.000210\n",
      "[244/350] 98.23 sec(s) Train Acc: 0.998195 Loss: 0.000135\n",
      "[245/350] 98.21 sec(s) Train Acc: 0.999436 Loss: 0.000046\n",
      "[246/350] 98.22 sec(s) Train Acc: 0.997142 Loss: 0.000197\n",
      "[247/350] 98.25 sec(s) Train Acc: 0.999210 Loss: 0.000061\n",
      "[248/350] 98.24 sec(s) Train Acc: 0.996465 Loss: 0.000294\n",
      "[249/350] 98.20 sec(s) Train Acc: 0.997781 Loss: 0.000152\n",
      "[250/350] 98.18 sec(s) Train Acc: 0.998609 Loss: 0.000083\n",
      "[251/350] 98.21 sec(s) Train Acc: 0.998270 Loss: 0.000102\n",
      "[252/350] 98.24 sec(s) Train Acc: 0.998458 Loss: 0.000103\n",
      "[253/350] 98.23 sec(s) Train Acc: 0.998045 Loss: 0.000147\n",
      "[254/350] 98.26 sec(s) Train Acc: 0.998684 Loss: 0.000120\n",
      "[255/350] 98.25 sec(s) Train Acc: 0.999285 Loss: 0.000067\n",
      "[256/350] 98.26 sec(s) Train Acc: 0.997706 Loss: 0.000182\n",
      "[257/350] 98.29 sec(s) Train Acc: 0.998496 Loss: 0.000105\n",
      "[258/350] 98.26 sec(s) Train Acc: 0.996540 Loss: 0.000276\n",
      "[259/350] 98.27 sec(s) Train Acc: 0.998195 Loss: 0.000178\n",
      "[260/350] 98.30 sec(s) Train Acc: 0.998909 Loss: 0.000088\n",
      "[261/350] 98.26 sec(s) Train Acc: 0.998496 Loss: 0.000111\n",
      "[262/350] 98.28 sec(s) Train Acc: 0.998872 Loss: 0.000066\n",
      "[263/350] 98.26 sec(s) Train Acc: 0.998082 Loss: 0.000158\n",
      "[264/350] 98.28 sec(s) Train Acc: 0.998646 Loss: 0.000127\n",
      "[265/350] 98.26 sec(s) Train Acc: 0.997932 Loss: 0.000152\n",
      "[266/350] 98.26 sec(s) Train Acc: 0.996879 Loss: 0.000226\n",
      "[267/350] 98.23 sec(s) Train Acc: 0.998609 Loss: 0.000115\n",
      "[268/350] 98.21 sec(s) Train Acc: 0.997856 Loss: 0.000170\n",
      "[269/350] 98.24 sec(s) Train Acc: 0.998909 Loss: 0.000089\n",
      "[270/350] 98.22 sec(s) Train Acc: 0.999060 Loss: 0.000090\n",
      "[271/350] 98.24 sec(s) Train Acc: 0.998045 Loss: 0.000112\n",
      "[272/350] 98.27 sec(s) Train Acc: 0.998533 Loss: 0.000105\n",
      "[273/350] 98.25 sec(s) Train Acc: 0.998045 Loss: 0.000169\n",
      "[274/350] 98.19 sec(s) Train Acc: 0.998646 Loss: 0.000073\n",
      "[275/350] 98.17 sec(s) Train Acc: 0.999022 Loss: 0.000067\n",
      "[276/350] 98.17 sec(s) Train Acc: 0.999549 Loss: 0.000036\n",
      "[277/350] 98.17 sec(s) Train Acc: 0.997443 Loss: 0.000212\n",
      "[278/350] 98.21 sec(s) Train Acc: 0.998759 Loss: 0.000119\n",
      "[279/350] 98.24 sec(s) Train Acc: 0.998082 Loss: 0.000124\n",
      "[280/350] 98.24 sec(s) Train Acc: 0.998233 Loss: 0.000140\n",
      "[281/350] 98.25 sec(s) Train Acc: 0.997894 Loss: 0.000161\n",
      "[282/350] 98.25 sec(s) Train Acc: 0.996653 Loss: 0.000336\n",
      "[283/350] 98.21 sec(s) Train Acc: 0.999022 Loss: 0.000074\n",
      "[284/350] 98.23 sec(s) Train Acc: 0.998571 Loss: 0.000084\n",
      "[285/350] 98.22 sec(s) Train Acc: 0.999624 Loss: 0.000024\n",
      "[286/350] 98.20 sec(s) Train Acc: 0.998947 Loss: 0.000086\n",
      "[287/350] 98.20 sec(s) Train Acc: 0.998985 Loss: 0.000073\n",
      "[288/350] 98.20 sec(s) Train Acc: 0.997744 Loss: 0.000197\n",
      "[289/350] 98.24 sec(s) Train Acc: 0.999022 Loss: 0.000069\n",
      "[290/350] 98.20 sec(s) Train Acc: 0.999060 Loss: 0.000081\n",
      "[291/350] 98.22 sec(s) Train Acc: 0.999323 Loss: 0.000051\n",
      "[292/350] 98.27 sec(s) Train Acc: 0.998797 Loss: 0.000073\n",
      "[293/350] 98.26 sec(s) Train Acc: 0.996954 Loss: 0.000271\n",
      "[294/350] 98.24 sec(s) Train Acc: 0.996804 Loss: 0.000293\n",
      "[295/350] 98.25 sec(s) Train Acc: 0.996766 Loss: 0.000258\n",
      "[296/350] 98.22 sec(s) Train Acc: 0.998609 Loss: 0.000108\n",
      "[297/350] 98.19 sec(s) Train Acc: 0.999135 Loss: 0.000063\n",
      "[298/350] 98.20 sec(s) Train Acc: 0.998759 Loss: 0.000076\n",
      "[299/350] 98.21 sec(s) Train Acc: 0.998721 Loss: 0.000102\n",
      "[300/350] 98.21 sec(s) Train Acc: 0.998872 Loss: 0.000090\n",
      "[301/350] 98.22 sec(s) Train Acc: 0.998721 Loss: 0.000112\n",
      "[302/350] 98.18 sec(s) Train Acc: 0.999398 Loss: 0.000039\n",
      "[303/350] 98.21 sec(s) Train Acc: 0.999285 Loss: 0.000071\n",
      "[304/350] 98.22 sec(s) Train Acc: 0.998120 Loss: 0.000153\n",
      "[305/350] 98.23 sec(s) Train Acc: 0.997706 Loss: 0.000189\n",
      "[306/350] 98.23 sec(s) Train Acc: 0.998646 Loss: 0.000103\n",
      "[307/350] 98.24 sec(s) Train Acc: 0.999248 Loss: 0.000064\n",
      "[308/350] 98.23 sec(s) Train Acc: 0.998609 Loss: 0.000116\n",
      "[309/350] 98.20 sec(s) Train Acc: 0.999060 Loss: 0.000070\n",
      "[310/350] 98.21 sec(s) Train Acc: 0.998947 Loss: 0.000096\n",
      "[311/350] 98.26 sec(s) Train Acc: 0.999210 Loss: 0.000089\n",
      "[312/350] 98.28 sec(s) Train Acc: 0.998270 Loss: 0.000135\n",
      "[313/350] 98.22 sec(s) Train Acc: 0.998797 Loss: 0.000090\n",
      "[314/350] 98.22 sec(s) Train Acc: 0.997067 Loss: 0.000295\n",
      "[315/350] 98.24 sec(s) Train Acc: 0.998233 Loss: 0.000151\n",
      "[316/350] 98.23 sec(s) Train Acc: 0.999474 Loss: 0.000030\n",
      "[317/350] 98.23 sec(s) Train Acc: 0.998759 Loss: 0.000077\n",
      "[318/350] 98.23 sec(s) Train Acc: 0.998909 Loss: 0.000069\n",
      "[319/350] 98.23 sec(s) Train Acc: 0.999060 Loss: 0.000060\n",
      "[320/350] 98.23 sec(s) Train Acc: 0.998345 Loss: 0.000128\n",
      "[321/350] 98.28 sec(s) Train Acc: 0.996127 Loss: 0.000374\n",
      "[322/350] 98.31 sec(s) Train Acc: 0.998195 Loss: 0.000208\n",
      "[323/350] 98.28 sec(s) Train Acc: 0.998947 Loss: 0.000079\n",
      "[324/350] 98.27 sec(s) Train Acc: 0.998759 Loss: 0.000080\n",
      "[325/350] 98.27 sec(s) Train Acc: 0.999022 Loss: 0.000091\n",
      "[326/350] 98.26 sec(s) Train Acc: 0.999361 Loss: 0.000032\n",
      "[327/350] 98.29 sec(s) Train Acc: 0.998872 Loss: 0.000107\n",
      "[328/350] 98.28 sec(s) Train Acc: 0.996051 Loss: 0.000376\n",
      "[329/350] 98.28 sec(s) Train Acc: 0.997180 Loss: 0.000218\n",
      "[330/350] 98.25 sec(s) Train Acc: 0.999060 Loss: 0.000129\n",
      "[331/350] 98.25 sec(s) Train Acc: 0.999210 Loss: 0.000054\n",
      "[332/350] 98.28 sec(s) Train Acc: 0.998797 Loss: 0.000089\n",
      "[333/350] 98.24 sec(s) Train Acc: 0.999097 Loss: 0.000080\n",
      "[334/350] 98.27 sec(s) Train Acc: 0.999549 Loss: 0.000050\n",
      "[335/350] 98.28 sec(s) Train Acc: 0.999323 Loss: 0.000045\n",
      "[336/350] 98.31 sec(s) Train Acc: 0.999737 Loss: 0.000027\n",
      "[337/350] 98.25 sec(s) Train Acc: 0.998684 Loss: 0.000087\n",
      "[338/350] 98.26 sec(s) Train Acc: 0.999097 Loss: 0.000077\n",
      "[339/350] 98.28 sec(s) Train Acc: 0.998233 Loss: 0.000178\n",
      "[340/350] 98.32 sec(s) Train Acc: 0.997706 Loss: 0.000174\n",
      "[341/350] 98.28 sec(s) Train Acc: 0.998157 Loss: 0.000173\n",
      "[342/350] 98.30 sec(s) Train Acc: 0.998195 Loss: 0.000173\n",
      "[343/350] 98.33 sec(s) Train Acc: 0.998684 Loss: 0.000097\n",
      "[344/350] 98.32 sec(s) Train Acc: 0.999173 Loss: 0.000076\n",
      "[345/350] 98.35 sec(s) Train Acc: 0.998458 Loss: 0.000129\n",
      "[346/350] 98.31 sec(s) Train Acc: 0.998157 Loss: 0.000213\n",
      "[347/350] 98.31 sec(s) Train Acc: 0.999285 Loss: 0.000057\n",
      "[348/350] 98.32 sec(s) Train Acc: 0.999549 Loss: 0.000036\n",
      "[349/350] 98.30 sec(s) Train Acc: 0.999398 Loss: 0.000051\n",
      "[350/350] 98.30 sec(s) Train Acc: 0.998383 Loss: 0.000127\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "# model_best = Classifier().cpu()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 350\n",
    "\n",
    "# use apex to optimize\n",
    "# model_best, optimizer = amp.initialize(model_best, optimizer, opt_level=\"O3\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "#         train_pred = model_best(data[0].cpu())\n",
    "#         batch_loss = loss(train_pred, data[1].cpu())\n",
    "#         with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n",
    "#             scaled_loss.backward()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mingc\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(model_best, 'model_no_aug.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2o1oCMXy61_3"
   },
   "source": [
    "# Testing\n",
    "利用剛剛 train 好的 model 進行 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAR6sn8U661G"
   },
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HznI9_-ocrq"
   },
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "#         test_pred = model_best(data.cpu())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t2q2Th85ZUE"
   },
   "outputs": [],
   "source": [
    "#將結果寫入 csv 檔\n",
    "with open(\"predict_no_aug.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 釋放記憶體\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
